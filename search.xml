<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[CVPR 2017] Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields 리뷰]]></title>
    <url>%2F2019%2F08%2F02%2F2019-08-02-Openpose-paper-review%2F</url>
    <content type="text"><![CDATA[Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields을 읽고 정리한 글입니다. IntroductionHuman pose estimation 문제는 다음과 같은 어려움 점이 존재합니다. Challenges Unknown number of people that can occur in a frame. (한프레임에 사람에 몇명이 있는지 모른다.) Complex Spatial Interference - Contact, Occlusion between people. (사람들간의 접촉, 맞물림등 복잡한 공간적 간섭이 존재) Variance in person scales (사람들의 크기가 다양함) Run time Complexity. (실행시간이 길다) Human pose estimation 문제를 접근하는 방법을 크게 Top-down Approach, Bottom-up Approach 두가지 방법이 있습니다. Top-down Approach Fig1. Top-down Appraoch방식은 이미지에서 사람을 먼저 찾고 관절을 추정하는 방식입니다. Problem Human detector가 사람을 잘못찾으면 이를 해결할 방법이 없다. 사람의 수가 많아지면 computational cost가 증가한다. ( Challenges 4 ) Bottom-up Approach Fig2. Bottom-up Appraoch방식은 관절을 먼저 다찾고 이를 알맞게 이어주는 방식입니다. Problem 찾은 관절을 매칭할 수 있는 조합이 매우 많고 이를 적절하게 매칭하는데 시간이 많이 걸리고 Accuracy도 높이는 것이 힘듭니다. Architecture Fig3 Fig4 Input &amp; 2 BranchInput: RGB color image 병렬적으로 구성된 2개의 Branch Branch 1에서는 Confidnce maps를 찾아낸다.$$S^1 = \rho^1(F), S^t = \rho(F,S^(t-1),L^(t-1), \forall t \ge 2) - (1)$$ Branch 2에서는 PAF를 찾아낸다.$$S^1 = \rho^1(F), S^t = \rho(F,S^(t-1),L^(t-1), \forall t \ge 2) - (2)$$ $S^t$:$\ $ Stage $t$에서 만들어진 Confidence maps $F$:$\ $ VGG-19에서 추출된 Feature $L^t$:$\ $ Stage $t$에서 만들어진 PAF Fig5.stage가 지날수록 성능이 향상된다.(위에 주황색원: False negative, 빨간원: False positive이 해결되고 있음 Loss functiongroundtruths와 추정된 값의 $L_2$ loss를 사용하였다. $$f_{S}^t = \sum\limits_{j=1}^J\sum_{p}W(p) \dot \lVert S_{j}^t(p) - S_{j}^{\ast}(p)\rVert_{2}^2 - (3)$$ $$f_{L}^t = \sum\limits_{c=1}^C\sum_{p}W(p) \dot \lVert L_{c}^t(p) - L_{c}^{\ast}(p)\rVert_{2}^2 - (4)$$ $J$,$C$: Confidence map의 개수와 PAF의 개수 $f_{S}^t$: Stage $t$ 에서의 Confidence map loss $f_{L}^t$: Stage $t$ 에서의 PAF loss $p$: 이미지의 좌표 $W(p)$: binary mask, true-positive를 처벌하는것을 피하기위해서 사용됨, annotation이 없을때 $W(p) = 0$ Stage 마다 loss를 계산하여 Vanishing gradient를 해결( paper) overall objective$$f = \sum\limits_{t=1}^T(f_{S}^t + f_{L}^t)- (5)$$ Generation of confidence mapdataset은 keypoint만 주어지기에 groundtruths에 될 confidence maps를 만들어 줘야한다. $$S_{j,k}^{\ast}(p) = \exp (-\dfrac{\lVert p-x_{j,k}\rVert _{2}^2}{\sigma^2}) - (6)$$ $S_{j,k}^{\ast}(p)$:$ \ \ k$번째 사람의 $j$번째 관절의 confidence map $x_{j,k}$:$\ \ k$번째 사람의 $j$번째 관절의 keypoint $\sigma$:$ \ $ peak의 범위를 조절함 Fig6. $$S_{j}^{\ast}(p) = \max_{\rm k}S_{j,k}^{\ast}(p) - (7)$$ 이렇게 만든 가우시안 분포에 max를 취해준다.(average보다 peaks가 distinct하게 남아서 근접에 대한 정밀도를 가진다.) PAFs(Part Affinity Fields) Fig7. (a):$ \ $ keypoint의 모든 연결 후보, (b):$ \ $ 각 연결 쌍의 중간점을 추가하여 중간 점의 발생률을 가지고 그리기(초록색선:틀린선,검은선:맞는선),사람들이 몰려있으면 오판될 수 있다.(이러한 방법은 위치만 고려하고 방향은 고려 하지 않고 region of support of limb를 한점으로 줄여버린다.) (c):$ \ $PAF를 이용한 결과(위치와 방향문제 해결 및 region of support of limb의 문제를 해결) Fig8. 식(4)를 풀기위해서 train때 PAF의 groundtruth를 다음과 같이 정의합니다.$$L_{c,k}^{\ast}(p) = \begin{cases}v &amp; \ \ \text{ if } \ p \ \text{on limb} \ c,k \cr0 &amp; \ \ \ \text{otherwise.}\end{cases} - (8) $$ $c$:$\ $limb의 종류 $v$:$\ \ v = \frac{(x_{j_2,k}-x_{j_1,k})}{\lVert x_{j_2,k}-x_{j_1,k}\rVert_2}$, limb의 방향의 unit vector(위 그림의 초록색선) limb위에 있는 points는 distance threshold내로 정의합니다. $$0 \le v \dot (p-x_{j_1,k}) \le l_{c,k} , \text{and} , |v_{\perp} \dot (p-x_{j_1,k})| \le \sigma_l - (9)$$ $l_{c,k}$: $\ \ l_{c,k} = \lVert x_{j_2,k} - x_{j_1,k}\rVert_2$, limb의 길이 $\sigma_l$:$ \ $ limb의 폭 마지막으로 이미지에 모든 사람으로 평균화 합니다. $$L_{c}^{\ast}(p) = \dfrac 1 n_{c}(p) \sum_{k}L_{c,k}^*(p) - (10)$$ $n_c(p)$:$\ $p에서의 non-zero vectors의 수(서로다른 사람의 limbs의 overlap되는 픽셀을 평균화시킴) test때 에는 $d_{j_1}$으로부터 $d_{j_2}$로 일정간격으로 선적분을 수행하여 affinity field의 세기(E)를 구합니다. $$E = \int_{u=0}^{u=1} L_c(p(u)) \cdot \dfrac {d_{j_2}-d_{j_1}} {\lVert d_{j_2}-d_{j_1} \rVert_2} du - (11)$$ $L_c$: $ \ $예측된 PAF $d_{j_1}$: $ \ $ 시작part 위치 $d_{j_2}$: $ \ $ 끝 part 위치 $p(u)$: $ \ $ $p(u) = (1-u)d_{j_1} + ud_{j_2}$, 2 part의 위치 사이를 채웁니다. Multi-Person parsing using PAFs Fig9 (a): Original image (b): 가능한 모든 연결 (c): 사람의 관절구조(spanning tree형태) (d): 이웃한 관절끼리 두개씩의 매칭 non-maximum suppression(NMS)을 사용해서 모든 part의 confidence maps를 구하였습니다. 이로인해 다양한 후보가 발생하고 여러 사람이 있기에 false positive 문제가 발생할 수 있습니다. optimal한 parse를 찾는 문제는 K-dimensional matching problem을 가지고있다.(NP-Hard 이라고도 함) 이 논문에서는 계속해서 high-quality한 match를 찾는 greedy relaxation을 말하고 있습니다.(optimal association 문제를 maximum weight bipartite graphmatching problem으로 줄입니다.) 어떠한 2개의 edge도 1개의 node를 공유하지 않기에 목표는 주어진 edges에서 maximum weight를 찾는 것입니다.(weight는 식(10)을 이용하여 구합니다) $$\max_{Z_c}E_c = \max_{Z_c} \sum_{m \in D_{j_1}}\sum_{n \in D_{j_2}}E_{mn}\dot z_{j_1j_2}^{mn} - (12)$$ $z_{j_1j_2}^{mn}$:$\ \ z_{j_1j_2}^{mn} \in \lbrace 0,1\rbrace$, detection candidates $d_{j_1]}^m$와 $d_{j_2}^n$이 연결되 있는지 아닌지를 나타내는 변수 $Z$:$\ \ Z = \lbrace z_{j_1j_2}^{mn} \text{for} j_1,j_2 \in \lbrace 1…J\rbrace,m\in \lbrace 1…N_{j_1}\rbrace,n \in \lbrace 1…N_{j_2}\rbrace\rbrace$, $z_{j_1j_2}^{mn}$ 가능한 모든 연결의 집합 $D_J$:$\ \ D_J = \lbrace d_j^m : \text{for} j \in \lbrace 1…J\rbrace,m\in \lbrace 1…N_j\rbrace\rbrace$,body part detection candidates의 집합 결론적으로 optimization은 다음과 같이 표현된다 $$\max_{Z} E = \sum_{c=1}^C\max_{Z_c}E_c - (13)$$ ( ? adjacent tree nodes는 PAFs에의해서 explicitly 하게 모델되었고 nonadjacent tree nodes는 CNN에의해 implicitly 모델되었습니다. ) ( ? 이러한 특성은 CNN이 large receptive field를 가지고 train 되고 non-adjacent tree nodes의 PAFs 또한 predicted PAF에 영향 끼쳤기에 발생하였습니다. ) ResultsMPII Multi-person Dataset COCO Dataset Referencehttps://ml.starall.kr/1 https://cloudup.com/i_gPL3kASQg]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>deep_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openpose 설치 tutorial]]></title>
    <url>%2F2019%2F04%2F08%2F2019-04-08-Openpose-tutorial%2F</url>
    <content type="text"><![CDATA[Openpose installrequirement NVIDIA CUDA_(그래픽카드 메모리가 1.6GB이상이여한다.)NVIDIA cuDNN Clone OpenPose$ git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose openpose/3rdparty 폴더에 가보면 이렇게 되어있을텐데 openpose github페이지에 가서 3rdparty에 들어간다. 여기서 caffe,pybind11에 들어가 깔아둔 openpose/3rdparty에 복사한다. 12345# openpose/3rdparty에서 터미널 실행하고$ git clone https://github.com/CMU-Perceptual-Computing-Lab/caffe.git$ git clone https://github.com/pybind/pybind11.git install library12345678910111213$ sudo apt-get install wget vim cmake cmake-qt-gui$ sudo apt-get install python-dev python-pip python-numpy$ pip install --upgrade pip$ sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev$ sudo apt-get install libhdf5-serial-dev protobuf-compiler$ sudo apt-get install libboost-all-dev libgoogle-glog-dev$ sudo apt-get install liblmdb-dev libopenblas-dev libatlas-base-dev cmake터미널 창에 cmake-gui 입력 다음과같이 where is the source code: Openpose 주소 where to build the binaries: openpose/build 로 바꿔준다. 그리고 configure를 눌러준다. 이런창이 나오면 generate 버튼을 클릭하고 완료되면 build 폴더로 간다. cd openpose/build/ 그리고 make 해준다 make -j 8 Demo 실행make가 완료되면 ./build/examples/openpose/openpose.bin --video examples/media/video.avi 을 시키고 잘작동하는지 확인한다 TODOtf버전 Openpsoe 추가]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep learning 유용한 사이트]]></title>
    <url>%2F2019%2F04%2F03%2F2019-04-04-Deep-learning-site%2F</url>
    <content type="text"><![CDATA[공부하면서 도움이 되었던 사이트들을 모아두는 곳 입니다. deep learning 기법Weight Initializationhttps://flonelin.wordpress.com/2018/01/28/weight-initalizer-%EC%A2%85%EB%A5%98/ https://gomguard.tistory.com/184 Regularizationhttp://www.hellot.net/new_hellot/magazine/magazine_read.html?code=202&amp;idx=41074&amp;public_date=2018-06 논문 분석http://openresearch.ai/ Pose estimationhttps://github.com/wangzheallen/awesome-human-pose-estimation#3d-pose-estimation 영상처리https://laonple.blog.me/220463627091 Codinghttps://github.com/Hvass-Labs/TensorFlow-Tutorials/ https://github.com/aymericdamien/TensorFlow-Examples https://github.com/tensorflow/docs/blob/master/site/en/tutorials/estimators/cnn.ipynb]]></content>
      <categories>
        <category>Information</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker,XML,JSON,CSV 간단 공부]]></title>
    <url>%2F2019%2F04%2F02%2F2019-04-02-docker-DATA%2F</url>
    <content type="text"><![CDATA[머신러닝, 딥러닝 실전 개발 입문 강의를 보고 공부한 내용입니다. https://www.youtube.com/watch?v=l_XFlB1Wwz8&amp;list=PLBXuLgInP-5m_vn9ycXHRl7hlsd1huqmS&amp;index=1 Docker먼저 docker 이미지를 가져온다(miniconda: anaconda 패키지중 가장 기본적인것만 설치되어있음) 1docker pull continuumio/miniconda3 설치가 다되었으면 아래 명령어로 이미지를 실행한다. 1docker run -i -t continuumio/miniconda3 /bin/bash |![alt](/images/Deep-learningetc/docker.png)| 위 이미지처럼 환경이 바뀌었으면 잘 작동한 것이다. 1exit 를통해서 환경을 종료 할 수 있다. 아래 명령어를 입력하면 1docker ps -a |![alt](/images/Deep-learningetc/docker (2).png)| 위 그림과 같이 컨테이너 실행 기록을 확인할 수 있다. 123docker commit 컨테이너 ID 이름:태그ex) docker commit &lt;a4997a3ede6e&gt; mlearn:init 위 명령어로 통해 컨테이너 이미지를 저장할 수 있다. 1docker run -i -t mlearn:init 위 명령어로 위에서 사용했던 환경과 똑같은 환경을 이용할 수 있다. 123docker run -i -t -v 자신이 가진폴더:컨테이너의 폴더 이미지 이름:태그 이름ex) docker run -i -t -v /home/kist-student/docker_sample:/sample mlearn:init 위 명령어로 폴더 마운트해서 이미지를 실행시킨다. XML(Extensible Markup Language)XML 형태여는 태그와 닫는 태그 &lt;태그&gt;&lt;/태그&gt; #요소(element) &lt;태그 /&gt; 콘텐츠&lt;태그&gt;콘텐츠&lt;/태그&gt;&lt;태그&gt; &lt;태그&gt;콘텐츠&lt;/태그&gt; &lt;태그&gt;콘텐츠&lt;/태그&gt;&lt;/태그&gt; 속성: “” =&gt; 문자열&lt;태그 속성=”값” 속성=”값” 속성=”값” 속성=”값”&gt;콘텐츠&lt;/태그&gt;&lt;태그 속성=”값” 속성=”값” 속성=”값” 속성=”값” /&gt; |![alt](/images/Deep-learningetc/XML.png)| Root tag 항상 하나,CDATA 내부 글자가 클때 데이터 보호용,rss는 태그이름 참고: https://sjh836.tistory.com/118 JSON(JavaScript Object Notation)JSON 구조가능한 자료형 숫자: 10, 253, 52.3 문자열: “안녕하세요” bool: true false null: null 배열:[10, 273, “안녕하세요”, true] 객체: 1234567&#123; &quot;키A&quot;: &quot;값&quot;, &quot;키B&quot;: 273, &quot;키C&quot;: true, &quot;키D&quot;: [12, 52] &quot;키E&quot;: &#123; &quot;name&quot;: 52 &#125;&#125; |![alt](https://www.w3resource.com/w3r_images/json-introduction.png)| 처음에는 배열이나 객체가 먼저오는게 일반적 CSV(Comma-Seperated Values)CSV 특징 한 줄에 데이터 하나 첫 번쨰 줄은 헤더로 사용 가능 1234ID, 이름, 가격1000,비누,300 # 1번 데이터1001,장갑,150 # 2번 데이터1002,마스크,230 # 3번 데이터 SSV: 뛰어쓰기TSV: tabCSV &gt; TSV, SSV xml 글자 많음(데이터 많음) &gt; json &gt; csv 표현력 많음: xml &gt; json &gt; csv xml은 잘 쓰이지 않고 있다고함 참고https://www.youtube.com/watch?v=dmwBi_JiYMs&amp;list=PLBXuLgInP-5m_vn9ycXHRl7hlsd1huqmS&amp;index=12 https://sjh836.tistory.com/118 https://stophyun.tistory.com/162 ##TODO데이터형 parsing 코드추가 및 docker 설명 추가]]></content>
      <categories>
        <category>theory</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNN Family 간단 정리]]></title>
    <url>%2F2019%2F04%2F01%2F2019-04-01-RCNN-Family%2F</url>
    <content type="text"><![CDATA[R-CNNRegion Proposals 입력 영상에서 ‘물체가 있을 법한’ 영역을 빠른 속도로 찾아내는 알고리즘을 region proposal 알고리즘이라 합니다. ex) Selective search, Edge boxes Transfer Learning기존의 만들어진 모델을 사용하여 새로운 모델을 만들시 학습을 빠르게 하며, 예측을 더 높이는 방법입니다. 사용되는 이유: 실질적으로 Convolution network을 처음부터 학습시키는 일은 많지 않습니다. 대부분의 문제는 이미 학습된 모델을 사용해서 문제를 해결할 수 있습니다. 복잡한 모델일수록 학습시키기 어렵습니다. 어떤 모델은 2주정도 걸릴수 있으며, 비싼 GPU 여러대를 사용하기도 합니다. layers의 갯수, activation, hyper parameters등등 고려해야 할 사항들이 많으며, 실질적으로 처음부터 학습시키려면 많은 시도가 필요합니다. 결론적으로 이미 잘 훈련된 모델이 있고, 특히 해당 모델과 유사한 문제를 해결시 transfer learining을 사용합니다. 새로 훈련할 데이터가 적지만 original 데이터와 유사할 경우데이터의 양이 적어 fine-tune (전체 모델에 대해서 backpropagation을 진행하는 것) 은 over-fitting의 위험이 있기에 하지 않습니다.새로 학습할 데이터는 original 데이터와 유사하기 때문에 이 경우 최종 linear classfier 레이어만 학습을 합니다. 새로 훈련할 데이터가 매우 많으며 original 데이터와 유사할 경우새로 학습할 데이터의 양이 많다는 것은 over-fitting의 위험이 낮다는 뜻이므로, 전체 레이어에 대해서 fine-tune을 합니다. 새로 훈련할 데이터가 적으며 original 데이터와 다른 경우데이터의 양이 적기 때문에 최종 단계의 linear classifier 레이어를 학습하는 것이 좋을 것입니다. 반면서 데이터가 서로 다르기 때문에 거의 마지막부분 (the top of the network)만 학습하는 것은 좋지 않습니다. 서로 상충이 되는데.. 이 경우에는 네트워크 초기 부분 어딘가 activation 이후에 특정 레이어를 학습시키는게 좋습니다. 새로 훈련할 데이터가 많지만 original 데이터와와 다른 경우데이터가 많기 때문에 아예 새로운 ConvNet을 만들수도 있지만, 실적적으로 transfer learning이 더 효율이 좋습니다. 전체 네트워크에 대해서 fine-tune을 해도 됩니다. https://fabj.tistory.com/57 R-CNN의 구조 이미지를 입력으로 받음 Selective search를 이용해 이미지로부터 약 2000개 가량의 region proposal을 추출함 각 region proposal 영역을 이미지로부터 잘라내고(cropping) 동일한 크기로 만든 후(warping), CNN을 활용해 feature 추출 각 region proposal feature에 대한 classification을 수행 ImageNet classification 데이터로 ConvNet을 pre-train 시켜 모델 $M$을 얻습니다. $M$을 기반으로, object detection 데이터로 ConvNet을 fine-tune 시킨 모델 $M^’$을 얻습니다. object detection 데이터 각각의 이미지에 존재하는 모든 region proposal들에 대해 모델 $M^’$으로 feature vector $F$를 추출하여 저장합니다. a. 추출된 $F$를 기반으로 classifier (SVM)을 학습합니다.b. 추출된 $F$를 기반으로 linear bounding-box regressor를 학습합니다. 여기서 CNN은 Transfer Learning을 사용 4.:모델의 마지막 층에 SVM을 두어 간단하게 이 결과가 객체인지 아니지, 객체가 맞다면 어떤 객체인지를 분류하도록 하였다. 문제점: localization에 취약함 -&gt; 개선: linear regression model을 통해 tight하게 맞추도록함 모델이 Image feature를 생성하는 것(CNN), classifier가 class를 예측하는 것(SVM), regression model이 bouding box를 찾아낸 것 (linear regression) 총 3개가 필요합니다. R-CNN의 단점 Test 속도가 느림(CNN을 2000번 돌리기 때문) 학습과정이 복잡함(3단계 pipeline) Input image 크기를 강제로 224 x 224로 warp, crop Fast R-CNNSPP(Spatial Pyramid Pooling) 다양한 크기의 입력으로부터 일정한 크기의 feature를 추출해 낼 수 있는 방법 중 Bag-of-words (BoW)라는 방법이 있습니다. 하지만 BoW는 이미지가 지닌 특징들의 위치 정보를 모두 잃어버린다는 단점이 존재합니다. 이러한 단점을 보완하기 위한 Spatial Pyramid Pooling 은 이미지를 여러개의 일정 개수의 지역으로 나눈 뒤, 각 지역에 BoW를 적용하여 지역적인 정보를 어느정도 유지할 수 있게 됩니다. ROI Pooling SPP layer는 feature map 상의 특정 영역에 대해 일정한 고정된 개수의 bin으로 영역을 나눈 뒤, 각 bin에 대해 max pooling 또는 average pooling을 취함으로써 고정된 길이의 feature vector를 가져올 수 있습니다. Fast R-CNN에서는 이러한 SPP layer의 single level pyramid만을 사용하며, 이를 RoI Pooling layer라고 명칭하였습니다. RoIPool의 핵심은 한 이미지의 subregion에 대한 forward pass값을 서로 공유하는 것이다. 위의 그림을 통해 어떻게 각 region에 대한 CNN feature가 feature map의 동일한 영역으로 부터 선택되어 값을 얻어내는지 확인할 수 있습니다. Fast R-CNN의 구조 pretrained된 모델에 이미지를 1개만 입력으로 받음 CNN을 통과한 feature map을 selective search로 2000개 가량의 region proposal을 추출함 region proposal(feature map)을 Roi pooling을 한다. Softmax classifier와 linear bounding-box regressor의 loss를 더하여 학습시킨다. 특징: CNN을 2000번 돌리는 것이 아닌 1번만 돌리면 된다.(속도 향상), 모델이 3개에서 1개의 네트워크로 통일됨 Fast R-CNN의 장단점장점 R-CNN에 비해 detection/localization 정확성 및 속도 개선 단점 region proposal 시간을 포함 시 real-time X (region proposal 에서 병목 현상) Faster R-CNNRPN(Region Proposal Network)Fast R-CNN 중 Selective Search(Region proposal)부분을 딥러닝으로 바꾼 것을 RPN이라 한다. 즉, 이 CNN기반의 미니 CNN인 RPN이 이미지 -&gt; CNN -&gt; output feature(feature map)를 잘라준다. RPN의 Covolution NN이 output feature를 sliding window방식으로 돌면서 연산후 classification 과 Regression 연산까지 한다. forward/ backward propagation -&gt; weight 업데이트 과정을 거치면 -&gt; Selective search를 대체하여 이미지를 2000개로 조각낸다. 즉, CNN기반의 RPN이 sliding window방식으로 box를 찾는 역활을 한다. RPN 구현 방법 이미지를 CNN으로 연산한다. 연산 결과를 n x n(보통 3x3) Convolutional Layer로 연산하고, 이 연산 결과가 맞는지를 보유하고 있는 bounding box 데이터와 Loss Function으로 비교한다. Loss function 결과로 backpropagation을 시키면 Region Proposal Network가 학습된다. 이 때, box를 찾는 과정에서, 어떤 object는 가로가 길고, 어떤 object는 세로가 길어서, sliding window가 꼭 정사각형이 아니라 직사각형 형태로 도는 것이 유리할 수 있다. 이러한 여러 형태의 sliding window를 anchor box라 한다. 그래서 RPN에서는 output feature인 feature map을 도는 여러개의 anchor box를 운영한다. 즉 CNN의 필터 대신, RPN은 anchor box를 사용하여, 따로 foward/backward하면서 training하여, 2000조각 낼 부분을 predict한다. Faster R-CNN구조 image를 CNN에 집어넣는다. CNN에서 나온 output feature(feature map)을 RPN에 집어넣어 classification 과 box를 얼마나 쳐야하는지를 따로 return받는다. Roi pooling을 이용하여 box크기를 fully-connected에 넣을 수 있게 resizing해준다. fast R-CNN과 동일하게 해준다. \ 1개의 모델에 끝에만 classification / regression을 따로 만들어 loss2개, weight업데이트도 2개로 따로하여 classification / regression(box위치)를 predict한다. ##TODO Mask R-CNN정리 참고http://incredible.ai/artificial-intelligence/2017/05/13/Transfer-Learning/ https://blog.lunit.io/2017/06/01/r-cnns-tutorial/ https://junn.in/archives/2517 https://nittaku.tistory.com/273]]></content>
      <categories>
        <category>theory</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mask RCNN-tensorflow tutorial]]></title>
    <url>%2F2019%2F03%2F27%2F2019-03-27-Mask-RCNN-tensorflow-tutorial%2F</url>
    <content type="text"><![CDATA[이 내용은 유튜버 Augmented Startups와 Mark Jay을 공부한 내용을 정리한 글입니다. Augmented Startups: https://www.youtube.com/watch?v=GSDbfGsxruA&amp;t=561s Mark Jay: https://www.youtube.com/watch?v=lLM8oAsi32g 1. Mask RCNN-tensorflow버전 설치Mask-RCNN-Tensorflow을 git clone 해준다1$ git clone https://github.com/matterport/Mask_RCNN.git Dependencies을 설치해주자먼저 필요한 python package들은 설치한다.12$ cd Mask_RCNN$ pip install -r requirements.txt 그리고 pretrained model을 다운받고 Mask-RCNN 폴더에 옮겨주자https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5 이제 cocoapi를 설치하자1234$ git clone https://github.com/philferriere/cocoapi.git$ cd cocoapi$ cd PythonAPI$ python setup.py build_ext install 설치가 잘되었는지를 확인하기터미널 창에 jupyter-notebook 입력해 쥬피터 노트북을 실행하고 samples 폴더안에 demo.jpynb를 실행시켜본다. 잘 설치된 경우 아래 그림과 같이 랜덤한 사진의 결과가 나온다. Failed to get convolution algorithm 문제 발생시: 12345import tensorflow as tfsess_config = tf.ConfigProto()sess_config.gpu_options.allow_growth = Truesess = tf.Session(config=sess_config) 와 같이 텐서플로우를 import하고 sess를 적절한위치에 추가해주자 2. webcam과 video으로 테스트 해보기webcam과 video 코드는 Mark Jay가 만든 코드를 사용할 것이다.https://github.com/markjay4k/Mask-RCNN-series 에서 visualize_cv2.py process_video.py 이 두개의 파일을 다운받는다. 그리고 visualize_cv2.py을 열어 다음과 같이 수정해준다. 12345678910111213import utils -&gt; from mrcnn import utilsimport model as modellib -&gt; from mrcnn import model as modellibROOT_DIR = os.getcwd() -&gt; ROOT_DIR = os.path.abspath("./")import coco -&gt; sys.path.append(os.path.join(ROOT_DIR,"samples/coco/"))import coco (위에 있던 import coco를 sys.path.append(os.path.join(ROOT_DIR,"samples/coco/")) 추가하고 아래에 옮겨준다. ) Failed to get convolution algorithm 문제 발생하면 위에 해결법을 이용하면 된다. 비디오 테스트에 경우 process_video.py에서 capture = cv2.VideoCapture(&#39;비디오 주소&#39;) 만 자신의 비디오 주소로 변경하면 된다. 3. 간단한 코드 분석123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133import cv2import numpy as npimport osimport sysfrom mrcnn import utilsfrom mrcnn import model as modellibimport tensorflow as tfROOT_DIR = os.path.abspath("./")MODEL_DIR = os.path.join(ROOT_DIR, "logs")sys.path.append(os.path.join(ROOT_DIR,"samples/coco/"))import cocoCOCO_MODEL_PATH = os.path.join(ROOT_DIR, "mask_rcnn_coco.h5")if not os.path.exists(COCO_MODEL_PATH): utils.download_trained_weights(COCO_MODEL_PATH)############# 여기까지 필요한 module importclass InferenceConfig(coco.CocoConfig): GPU_COUNT = 1 IMAGES_PER_GPU = 1config = InferenceConfig()config.display()############## cudnn 문제 해결 부분sess_config = tf.ConfigProto()sess_config.gpu_options.allow_growth = Truesess = tf.Session(config=sess_config)##############model = modellib.MaskRCNN( mode="inference", model_dir=MODEL_DIR, config=config ############# 모델 불러오기)model.load_weights(COCO_MODEL_PATH, by_name=True) ############# 모델 weight 불러오기class_names = [ 'BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']############## class이미지 이름 지정 (자리순서마다 이미어떤 클래스가 정해져 있고 이를 text로 표현하는 부분이다)def random_colors(N): np.random.seed(1) colors = [tuple(255 * np.random.rand(3)) for _ in range(N)] return colors############## class마다 다른 색깔로 segmentation 하기위한 부분colors = random_colors(len(class_names))class_dict = &#123; name: color for name, color in zip(class_names, colors)&#125;def apply_mask(image, mask, color, alpha=0.5): """apply mask to image""" for n, c in enumerate(color): image[:, :, n] = np.where( mask == 1, image[:, :, n] * (1 - alpha) + alpha * c, image[:, :, n] ) return image############## segemetation mask를 이미지에 표시def display_instances(image, boxes, masks, ids, names, scores): """ take the image and results and apply the mask, box, and Label """ n_instances = boxes.shape[0] if not n_instances: print('NO INSTANCES TO DISPLAY') else: assert boxes.shape[0] == masks.shape[-1] == ids.shape[0] for i in range(n_instances): if not np.any(boxes[i]): continue y1, x1, y2, x2 = boxes[i] label = names[ids[i]] color = class_dict[label] score = scores[i] if scores is not None else None caption = '&#123;&#125; &#123;:.2f&#125;'.format(label, score) if score else label mask = masks[:, :, i] image = apply_mask(image, mask, color) image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2) image = cv2.putText( image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2 ) return image############## mask, box, class_name을 모두 이미지에 표시if __name__ == '__main__': test everything capture = cv2.VideoCapture(0) # these 2 lines can be removed if you dont have a 1080p camera. capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640) capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 640) while True: ret, frame = capture.read() results = model.detect([frame], verbose=0) r = results[0] ############## 여기서 roi,masks, class_id가 나온다. frame = display_instances( frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'] ) cv2.imshow('frame', frame) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break capture.release()cv2.destroyAllWindows() 여기서 이용할 수 있는 부분은 results에서 나온 roi, masks, class_id로 어떤물체의 위치와 크기 이다. Mask-RCNN Train먼저 https://supervise.ly/이 사이트에 가입하고 사이트에 로그인한다. 그 후 Import 탭에 들어간 후 Import Plugin을 Images로 바꾸고 자신의 데이터셋 폴더를 위 그림의 상자안에 드래그 드랍 합니다. 그러면 위 사진처럼 창이 바뀌면 칸에 Project이름을 작성한후 Start Import 버튼을 누르면 Import가 됩니다. Project 탭에 들어가면 그림과 같이 자신이 Import한 이미지의 Project가 생성된 것을 확인할 수 있습니다. 이제 프로젝트를 클릭하고 이미지 폴더에 들어가면 다음과 같은 창이 열린다. 여기서 빨간색 박스가 쳐진 버튼을 클릭합니다. 그러면 이런 창이 뜨는데 여기서 Title에 물체의 label을 지정합니다. 그 후 그림처럼 labeling을 진행 합니다. 그리고 class를 하나더 추가하고 싶으면 위 그림처럼 Create Class를 눌러 추가 해주면됩니다. labeling을 다 했으면 다시 프로젝트 탭으로 돌아온후 위 그림처럼 Instance segmentation 버튼을 클릭해준다. Cluster 탭에 들어간 후 Instructions을 클릭합니다. 그러면 위 같은 창이 뜨는데 먼저 nvidia docker를 설치합니다. Neural Networks 탭에 들어간 후 ADD 버튼을 클릭합니다. 그 후 아래로 내려보면 Mask-RCNN이 있고 ADD버튼을 눌러준다. Neural Networks 탭에 다시 들어간 후 Train 버튼을 누른다. 자신의 데이터셋을 input project에 입력해주고 결과의 project이름을 정해준다. Train이 끝나면 다음과 같이 새로운 Neural Network가 생기고 여기서 Download를 해주거나 test버튼을 눌러 test 데이터를 test할 수 있다. 만약 Download한다면 .tar파일 안에 model.h5라는 파일 있는데 이 파일을 Mask-RCNN 폴더에 옮겨준다. 그리고 위 코드처럼 바꿔준다. 여기서 NUM_CLASSES 에서 뒤에 있는 2는 자신이 train 시킨 클래스의 수를 적어준다. https://deepmi.me/linux/18791/ nvidia docker를 설치 했으면 그림에 있는 명령어를 터미널에 입력합니다. TODO Mask-RCNN을 자신만의 데이터로 training 하기]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VNect-tensorflow]]></title>
    <url>%2F2019%2F03%2F22%2F2019-03-22-vnect-tensorflow-tutorial%2F</url>
    <content type="text"><![CDATA[VNect Tensorflow버전 github: https://github.com/timctho/VNect-tensorflow 1. 먼저 VNect-Tensorflow를 git clone 해준다.1$ git clone https://github.com/timctho/VNect-tensorflow.git 2. caffe python버전과 opengl을 설치한다.caffe는 여러가지 설치법이 있지만 anaconda에서 Python3로 쉽게 설치하는 방법은 아래 사이트를 참고한다 https://yangcha.github.io/Caffe-Conda3/ opengl의 경우는 1$ pip install PyOpenGL PyOpenGL_accelerate 으로 설치해주면 된다. 3. Vnect-Tensorflow가 설치된 폴더로 이동하고 caffe_weights_to_pickle.py를 실행한다.먼저 Vnect-Tensorflow가 설치된 폴더로 이동한다. 1$ cd VNect-tensorflow 그다음 환경을 자신이 caffe를 설치한 환경으로 바꿔준다. 1$ source activate testcaffe 이제 caffe로 만들어진 모델을 pickle 형식으로 바꿔준다. 123$ python caffe_weights_to_pickle.py --prototxt=../Documents/VNECT/mpii_vnect_model_code/mpii_vnect_model_demo/models/vnect_net.prototxt --caffemodel=../Documents/VNECT/mpii_vnect_model_code/mpii_vnect_model_demo/models/vnect_model.caffemodel 주소가 복잡하면 VNect-Tensorflow에 있는 models 폴더안에 vnect_net.prototxt 와 vnect_model.caffemodel 을 복사한후 아래와 같이 실행시키면 1$ python caffe_weights_to_pickle.py vnect.pkl 이라는 파일이 만들어졌을것이다. 4. models 폴더안에 vnect_model.py 수정하고 실행에 필요한 모델파일 만들기이유는 모르겠지만 직접 실행할때 필요한 모델을 만드는 파일이 없기에 models 폴더안에 vnect_model.py를 조금 수정해야한다.코드를 보시면 맨아래 if __name__ == &#39;name&#39;: 아래를 1234567model_file = '../vnect.pkl'model = VNect(368)with tf.Session() as sess: saver = tf.train.Saver() model.load_weights(sess, model_file) save_path = saver.save(sess, "./vnect_tf") 으로 바꿔주고 실행시키면 123vnect_tf.data-00000-of-00001vnect_tf.indexvnect_tf.meta 세가지 파일이 만들어 졌을것이다.이제 이 파일을 models/weights 안에 복사한다. (weights폴더가 없으니 만들어주자) 5. demo_tf_gl.py로 테스트 해보자.cudnn 오류때문에 1sess_config = tf.ConfigProto(device_count=gpu_count) 아래에 1sess_config.gpu_options.allow_growth = True 을 추가해준다. 그리고--demo_type&#39;, default=&#39;image&#39;를 --demo_type&#39;, default=&#39;webcam&#39; 으로 바꿔주면 웹캠으로 테스트가 가능하다. TODOcaffe 설치 오류 확인해보기]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
</search>
