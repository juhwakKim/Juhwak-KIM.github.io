<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 2주차 2-3 행렬과 벡터의 연산(2)]]></title>
    <url>%2F2020%2F10%2F22%2F2019-03-29-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-2-Weak-2-3%2F</url>
    <content type="text"><![CDATA[서울과학기술대학교 김진현교수님의 K-MOOC강좌 Robot Manipulator and Underwater Robot Application을 공부하고 내용을 정리한 것 입니다. K-MOOC 강좌 주소: http://www.kmooc.kr/courses/course-v1:SEOULTECHk+SMOOC03k+2019_T3/about 행렬과 벡터의 연산 - 2Vector Norms|벡터의 크기나 측정 비교을 가능하게 해주는 요소이다.| Norm의 조건 놈의 크기는 항상 0보다 크다, 0보다 크거나 같다. ($\text {x} = 0$인 경우만 등호가 성립한다.) $\text {x}+y$의 크기는 절대 $\text {x}$와 $y$를 각각 구한 후, 그 둘을 더한 것보다 작거나 같아야 한다. (삼각부등식) 상수 $α$을 곱했을 때, 상수만큼 늘어난다. 위 3 가지 성질을 만족하는 것을 놈으로 정의할 수 있습니다. P-norm p는 1보다 크거나 같고 딱히 자연수일 필요는 없습니다. p-norm의 특수한 예로써, 1-norm(맨해튼 놈), 2-norm(유클리드 놈)이 있다. 그리고 전체 원소 중에서 가장 큰 값을 골라내는 것을 인피니티 놈이라고 합니다. 스칼라곱(Scalar Multiplication) 벡터곱(Vector Product) 벡터곱 같은 경우에는 어떤 평면상에 있는 $A$라는 벡터와 $B$라는 벡터로 2개의 벡터곱을 취했을 경우에는, 2개의 벡터들의 공통된 수직방향, 즉 전체 면에 대한 수직방향으로 결과가 얻어지게 됩니다. 이 수직방향의 결과 때문에, 앞서 말씀드렸던 것처럼 내적을 취하게 되면 어떠한 벡터와 수직되는 벡터를 곱했을 때는, 0이 된다는 성질 때문에, 한마디로 $a\cdot(a$x$b)=0$은 $a$와 $b$ 모두에 수직되는 방향으로 결과가 도출된다는 것을 확인할 수 있습니다. 쌍선형 형식(bilinear form)|어떤 매트릭스를 기준으로 앞에 벡터를 곱하고, 다시 뒤에도 벡터를 곱하는 이러한 연산을 쌍선형 형식이라고 합니다.| 이런 쌍선형 형식 중에서, 특수한 형태가 존재하는데, 만약에 $A$라는 매트릭스가 정방행렬이고, 앞에 $\text {x}^T$를 곱하고, 뒤에 $x$를 곱하는, 즉 앞에서 $x$하고 $y$가 같은 형태를, 쌍선형 형식의 특수한 형태라 해서, quadratic form(이차형식)이라고 합니다. 만약에 이 이차형식에서, 주어진 매트릭스 $A$가 존재하고, $A\text{x}$ = 0일때 모든 x에 대해서, 어떤 걸 넣더라도 0이 된다. 만약에 주어진 매트릭스 $S$가 반대칭행렬이라면, $x^TSx = 0$이 되는 성질을 갖고 있습니다. 만약에 어떤 반대칭행렬 $C$가 존재할 때, 그 $C$가 이차형식의 형태로 $x^TCx$라고 했을 때, 그게 0이 된다고 하면, $x$가 0이거나, $C$가 0인 경우인 상황을 제외하고는 존재하지 않습니다. 만약에 $A$라는 대칭행렬이 존재할 때, 이 $A$라는 대칭행렬이의 경우에는 이 이차형식은 $x$가 0이 아닌 경우를 제외하고는, 항상 0보다 큽니다. (이것을 positive definite라고 한다.) 그리고, 0까지 포함해서 모든 $x$의 경우에, 0보다 항상 크거나 같다면, positive semi-definite 이라고 합니다. 이 둘의 차이는 positive definite은 $x$가 0일 경우에만 결과가 0이 되고, positive semi-definite은 $x$가 0이 아니더라도 0은 될 수 있다는 차이가 있습니다. 0보다 작아서는 안되지만, 0은 될 수 있다는 라는 것을 positive semi-definite이라고 정의할 수 있습니다. 그 외의 동일하게, 0이 아닌 모든 $x$에 대해서, 항상 0보다 작은 결과가 나올 경우가 바로 negative definite입니다. 그리고, negative semi-definite은, 앞서 말씀 드렸던 positive semi-definite과 동일하게, 모든 $x$에 대해서, 0보다 작거나 같은 이러한 성질들을 가질 때, negative semi-definite이라고 얘기합니다. 경우에 따라서는, 0보다 크고, 0보다 작은 경우가 존재할 수 있는데, 이 경우에는 결정할 수 없다, 라고 해서 indefinite이라고 합니다. 어떤 이차형식을 미분했을 때, 앞뒤로 대칭되는 성질을 갖고 있기 때문에, $2x^TAx+x^TAx$라는 형태로, 이차형식이 주어지게 됩니다. 벡터와 행렬의 미분(Derivative of Vectors and Matrices)|벡터, 행렬에 미분이 필요한 이유는, 앞으로 연산을 진행할 때, 기구학을 구하고, 동력학을 구할 때, 속도, 가속도 등이 필요할 때, 표현하는 방법 중 하나가 주어진 매트릭스와 벡터를 미분하는 것이기 때문입니다| 만약에 어떤 매트릭스의 함수, 즉 매트릭스로 이루어진 함수인 $u$,$v$가 존재하고, 모든 원소들이 전부 실수인 상수를 갖는 매트릭스 $A$가 존재할 때, 이 매트릭스 $A$를 미분한다고 할 때, 실수 상수를 가지고 있기 때문에, 항상 미분하면 0이 됩니다. 그리고, $u$,$v$라는 벡터의 경우, 흔히 알고 있는 미분의 성질을 그대로 따르게 됩니다. 주어진 매트릭스와 벡터를 각각 스칼라값, 즉 일정한 상수 값으로 미분을 했을 때는, 그 주어진 디멘션을 그대로 가지면서, 각각의 원소들을 스칼라 값인 t로 미분하는, 이러한 성질을 갖습니다. 역시 마찬가지로 벡터에 대해서 미분을 하는 경우에도, 벡터의 한 원소인 $x_1$으로 미분했을 때, 원래의 벡터의 디멘션을 그대로 유지하면서 각각의 스칼라값으로 미분하는 특성을 갖습니다. 이제부터 중요하고, 주의해야 하는 법칙들이 나타나게 되는데, 만약에 옆으로 긴, 즉 트랜스포즈가 되어있는 벡터로서, 스칼라값을 미분한다면 어떨까요? 스칼라값을 벡터로 미분한다고 하면, 벡터가 옆으로 긴 것의 경우에는, 각각의 원소들을 이용해서 각각의 스칼라값을 미분하는 것으로 정의할 수 있게 됩니다. 마찬가지로, 아래로 긴 벡터의 형태를 이용해서, 어떤 스칼라값 $a$를 미분하게 되었을 때, 이를 흔히 벡터에서 ‘그레디언트’라고 표현하기도 합니다. 마찬가지로 각각의 원소로 $x_1$~$x_n$행까지, 벡터의 성분으로 미분하게 되는 효과를 갖게 됩니다. 매트릭스로 확장하게 되면, 마찬가지로 스칼라 값을 매트릭스의 원소로 미분하게 되면, 각각의 값을 갖고 미분을 한다고 이해하면, 큰 문제는 없을 겁니다. 어떠한 벡터를 벡터로 미분하는 경우 $\partial v$, $\partial x$라는, 어떤 둘 다 $v$라는 벡터와 $x$라는 벡터가 존재하게 되고, $x$라는 벡터로 $v$라는 벡터를 미분한다고 한다면, $v$도 아래로 길고, $x$도 아래로 길다면, 이 두 벡터를 이용해서 미분을 할 때, 이에 대한 정의를 위한 새로운 디멘션이 필요할 것입니다. 그래서, 보편적으로, 벡터를 벡터로 미분할 경우에는 항상 미분당하는 벡터는 아래로 길게, 미분하는 벡터는 옆으로 길게 배치를 해서 이 규칙을 통해서 벡터를 미분하는 것이 일반적인 방법입니다. 규칙을 깨지 않고, 일반적인 성질들을 그대로 유지할 수 있는 방법이 이와 같은 정의를 사용하는 것입니다. 좀 더 확장하게 되면, 만약에 $u^Tv$라고 하는, 이런 벡터 2개를 내적을 취하게 되면, 이것은 스칼라 값이 될 것입니다. 어떤 스칼라 값인, $α$가 될 테니까, 앞서서 정의한 것처럼, 옆으로 긴 벡터 $\partial x_1$~$\partial α$까지, 라운드 xn의 라운드 알파 형태로 결과값을 얻게 됩니다. 2개의 서로 다른 벡터가 있을 때, 앞의 것을 미분하고, 뒤의 것을 미분하는 방식을 그대로 사용하게 되면, 결국에는 결과값은 스칼라값, 이 형식을 유지해야 하기 때문에, 그럴 수 있도록 $v^Tu$에 대한 미분, 즉 원래 아까 정의한 것처럼, 어떤 벡터를 미분할 때에는 옆으로 긴 벡터에서 세로로 긴 벡터를 미분해야 하기 때문에, 이러한 노테이션을 활용해서 $v^Tu$를 곱하게 되면, 아래와 같은 디멘션을 얻게 됩니다. 마찬가지로 뒤의 것을 미분할 경우에는, 이번에는 다행히도 $v$가 아래로 긴 벡터이기 때문에, 옆으로 긴 벡터에 아래로 긴 벡터를 미분하여 얻은 결과에 트랜스포즈를 하고, 동일한 결과값을 얻어, 두 결과를 더하는 방식으로, 이러한 규칙성을 따질 필요가 생깁니다. 더 나아가서, 만약에 이차형식이나, 쌍선형 형식이 주어지게 되면, 이 결과값은 결과적으로 $α$, 즉 상수값이 됩니다.그렇기 때문에 옆으로 긴 벡터로 미분할 수 있게 됩니다. 각각의 요소들을 살펴보면, $u$에 대한 미분, $v$에 대한 미분은 큰 어려움 없이 수행할 수 있게 되고, 대신에, 가운데, $Q$에 대한 미분이 필요하게 되는데, 이 매트릭스에 대한 미분은 $x^T$로 미분하는 것까지는 좋은데, Q라는 것은 매트릭스입니다. 그렇기 때문에 미분을 하게 되면 3차원의 형태로 확장해야 합니다. 이런 어려움이 있기 때문에, 이런 부분들을, $v$하고 같이 연결지어서 이와 같이 $x^T$와 $Q$를 미분한 것에 $v$를 곱한 것을, 각기 $x_1$~$x_n$에 대한 것을, 벡터와 연결지어서 스칼라값과 하는 방법으로, 전체적인 연산방법을 깨지 않으면서, 쌍선형 형식까지 어떻게 미분할 수 있는지를 살펴보았습니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 2주차 2-2 행렬과 벡터의 연산(1)]]></title>
    <url>%2F2020%2F10%2F22%2F2019-03-28-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-2-Weak-2-2%2F</url>
    <content type="text"><![CDATA[서울과학기술대학교 김진현교수님의 K-MOOC강좌 Robot Manipulator and Underwater Robot Application을 공부하고 내용을 정리한 것 입니다. K-MOOC 강좌 주소: http://www.kmooc.kr/courses/course-v1:SEOULTECHk+SMOOC03k+2019_T3/about 행렬과 벡터의 연산행렬의 덧셈과 곱셈 대각합(trace)|정방행렬의 주대각선 성분들의 합이다.| 행렬식(Determinant)|${detA_i}_j$는 minor 행렬 (소행렬식)이다.| 선형 독립, Span, basis 만약에 k개의 벡터가 존재한다고 했을 때, k개의 벡터들을 각각, 앞에 상수를 전부 다 곱해서, 0으로 만들 수 있는 그런 조합이 존재 한다면 선형종속이라고 한다. 만약 $c_1$~$c_k$까지 전부, 0을 제외하고는 어떤 수를 가지고도 이 전체를 0벡터를 만들 수 없다, 이런 경우를 ‘선형독립’이라고 말합니다. |선형독립이 되는 벡터들이 있다면, 어떤 벡터 공간을 만들 수 있게 됩니다.| 즉, $v_1$~$v_k$까지라고 하는 k개의 어떤 선형독립된 벡터들이 있다면, 그 벡터들을 통해서, 새로운 벡터를 만들어낼 수 있게 됩니다.이 경우, 이렇게 만드는 과정을 공간을 ‘스팬’한다고 말하게 된다. |이렇게 3개의 (1, 0, 0), (0, 1, 0), (0, 0, 1)이라고하는 선형독립 벡터를 통해서 공간을 만들어냈을 때, 그 공간을 스팬한다고 말할 수 있다.| 3차원 벡터를 온전히 스팬하기 위해서, 필요한 최소한의 선형독립 벡터의 수가 3개임을 쉽게 알 수 있습니다. 이 3이라는 숫자가, 바로 차원, 즉 3차원이라고 부르는 차원, 디멘션이라고 말할 수 있습니다. 그리고, 어떤 공간을 만들기 위해서, 필요한 최소한의 수의 벡터를, ‘basis’라고 합니다. 이 basis를 통해서, 전체 벡터 공간을 스팬할 수 있다고 말할 수 있고, 반대로 그 스팬을 하기 위해 필요한 벡터의 수를 basis, 다른 말로 기저라고 할 수 있습니다. basis는 기저벡터들은 독립(Independent)이다. 기적벡터들은 공간(space)을 “span” 한다. span자체에서는 column vector가 독립인지 종속인지 상관없다. 행렬의 계수(Rank of a Matrix)|이러한 선형독립에 대한 이야기를 한 이유는, 바로 행렬의 계수, 즉 랭크라고 부르는 행렬의 계수에 대한 설명을 하기 위해서다.| 만약에 m by n 매트릭스인 $A$가 있다고 할 때, 굳이 정방행렬일 필요는 없습니다. 이 행렬의 계수는 방금 설명했던 것처럼, 선형 독립적인 칼럼의 수, 선형독립인 로우의 수, 즉 (선형 독립적인) 행과 열의 수가 바로 랭크가 됩니다 Range space of $A$, 즉 columns으로 만들 수 있는 공간을 뜻하는 $R(A)$의 차원과 같은 개념이 됩니다. 마찬가지로 어떤 매트릭스의 Row Space의 디멘션하고도 같게 됩니다. 만약에 어떤 매트릭스 $A$의 랭크를 구했을 때, 만일 $A$가 옆으로 긴 행렬이면, m by n에서 m이 n보다 작게 됩니다. 이 경우 최대한으로 가질 수 있는 랭크의 수는 이 행의 개수만큼을 초과할 수가 없게 됩니다. 역행렬(Inverse Matrix) |$A$가 nonsingular, 즉 정칙행렬일 경우, 행렬식은 0이 아니고, $A$의 역행렬, $A$의 inverse는 존재합니다.| $A$의 inverse 존재할 때, 만약에 $det(A^{-1})$, 즉 역행렬에 대한 Determinant는 $1 \above 1pt detA$의 형태로 구할 수 있습니다. 반대로, Determinant 0이면, 즉 어떠한 매트릭스 $A$의 Determinant이 0일 경우에는 singular 이라고 하고 역행렬의 Determinant은 정의 할 수 없습니다. 역행렬은 어떠한 방법으로 구했건, 한번 구했다면 그 역행렬은 유일합니다. 여인수(Cofactor) |역행렬을 범용적으로 구하기위한 수식이다.|]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 2주차 2-1 행렬과 벡터]]></title>
    <url>%2F2020%2F10%2F22%2F2019-03-27-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-2-Weak-2-1%2F</url>
    <content type="text"><![CDATA[서울과학기술대학교 김진현교수님의 K-MOOC강좌 Robot Manipulator and Underwater Robot Application을 공부하고 내용을 정리한 것 입니다. K-MOOC 강좌 주소: http://www.kmooc.kr/courses/course-v1:SEOULTECHk+SMOOC03k+2019_T3/about 벡터와 행렬의 정의와 특징벡터(vector)|벡터는 크기와 방향을 가지고 있다.| |한 개의 행이나 열로 구성된 행렬이라고 볼 수 있다.| 행렬(matrix)|행렬은 2차원 평면상의 숫자들의 위치를 잡아서 표현하는 방법이다.| 행렬과 벡터의 전치 특수한 행렬들대칭행렬과 반대칭행렬 |어떤 주어진 행렬에 대해서 Transpose를 더한 다음에 반으로 나누거나 Transpose 취한 것을 빼서, 바로 대칭행렬과 반대칭행렬로, 모든 정방행렬들은 이렇게 반으로 분리해낼 수 있는 성질을 갖고 있다.||ex)https://www.teachoo.com/3182/685/Example-22---Express-matrix-B-as-sum-of-symmetric-and-skew/category/Examples/| 삼각행렬 그림변경대각행렬과 단위행렬]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 6주차 6-1 Jacobian matrix]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-18-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-6-Weak-6-1%2F</url>
    <content type="text"><![CDATA[Jacobian matrix 그렇다면 먼저 Jacobian matrix를 유도하기 앞서서 먼저 Jacobian matrix를 유도하기 위한 속도에 대한 Rotational, 회전하는 속도에 대한 관계식들을 먼저 정의해볼 필요가 있습니다. 첫번째, object가 rotational velocity, 즉 회전속도에 의해서 이동하고 있었을 때 그랬을 때 어떠한 각도로 표현되는 것을 이 각도의 속도, 즉 각도의 변화량으로써 어떻게 표현할 수 있는지는 보통 두 가지 정도로 요약해서 나타낼 수가 있습니다.첫번째 앞서서, 기구학 부분에 있어서 Euler angle 하고 roll/pitch/yaw라는 각도를 이용을 해서 각도표현을 하는 법을 배웠습니다.Euler angle하고 roll/pitch/yaw 입장에서 만약에 이런 Euler angle하고 roll/pitch/yaw 를 $\phi$라고 정의를 했을 때, 어떠한 두 좌표계사이의 관계식이 됩니다.두 좌표사이에 roll/pitch/yaw 또는 Euler angle로서 transformation이 가능할텐데요.이랬을때 roll/pitch/yaw 가 각각 속도로써 나타내게 될 때 roll/pitch/yaw 를 아니면 Euler angle을 미분하게 됐을 때, 즉, 시간에 대해서 미분했을 때, 그 미분한 항을 각각 각도에 대한 속도로써 표현이 가능 할 겁니다. Rotational Velocity of a Object 그랬을 때 이렇게 표현을 하게 되면 roll/pitch/yaw 에 대한 아니면 Euler angle에 대한 속도를 적분을 하게됩니다.적분을 하게되면, 그 적분한 값이 가지는 의미는 결국에는 정의했던 roll/pitch/yaw 또는 Euler angle의 파라미터 앵글로써 표현이 될 수 있겠습니다. 그런데 문제는 이렇게 roll/pitch/yaw 라던지 Euler angle 같은 것들을 속도관계식을 가지고 미분하고 적분해서 얻어지는 관계식을 설명하는 것은 굉장히 쉽습니다. 문제는 이러한 roll/pitch/yaw 라고 하는 것들 아니면 Euler angle이라고 하는 각도들의 변화량이 직접적으로 이 각도로써 표현이 될 수 있는 부분은 아닙니다. 어떤 의미냐 하면, 비행기를 상상해보면, 앞서서 기구학을 설명을 할 때, 기구학에서의 각도표현을 설명할 때 비행기를 가지고 roll/pitch/yaw 라던지 Euler angle을 설명드린적이 있었습니다.비행기를 생각했을 때 이 비행기가 가지고 있는 예를 들어서 X방향에 대한 roll 또는 pitch 그다음에 yaw를 봤을 때, 만약에 yaw가 단독으로 나타나거나 아니면 pitch가 단독으로 나타날때는 큰 문제가 없지만,yaw하고 pitch가 동시에 나타나는 운동, 동시에 나타나는 움직임이 있었을 때, 시시각각 계속해서 이 속도가 변할 때 각각에 대한 값들을 정확하게 표현하는 것은 이러한 Euler angle, roll/pitch/yaw 각도들을 rotation matrix로부터 뽑아낸 각도이기 때문에 1대1 mapping이 되지 않습니다. 따라서 사실상 어떤 각도를 표현하고 그 각도에 대한 rate, 즉 각속도를 표현할 때는 그 표현법이 조금은 달라져야 될텐데요일단은 가장 기본적인 물리 법칙에서는 $\omega$로써 이 각도에 대한 변화량 즉, 속도변화를 보통 표현합니다.그래서 원하는 roll/pitch/yaw, Euler angle 같은 변화량은(속도는) 이러한 $\omega$의 값으로, 일반적인 좌표계를 설명하는 오메가의 값을 이용해 표현을 해야 할 필요가 있습니다.그래서 뽑아내는 roll/pitch/yaw 각도 또는 Euler angle 값에 따라서 각각 뽑아내는 방법이 다르기 때문에역시 그 둘 사이에 매칭을 가져가게 되는 표현법은 또 역시 달라지게 될 것 입니다.옆에 보이는 수식은 대표적으로 ZYZ앵글로써 뽑은 Euler angle값을, 그 Euler angle값과 $\omega$의 어떤 관계식을 뽑아낸 수식이 되겠습니다.이 수식이 어떻게 이렇게 유도되었는지에 대해서는 차차 진행을 하면서 설명을 드리기로 하겠습니다.이런식으로 수식을 유도를 하게되면 우리는 roll/pitch/yaw에 대한 각각의 변화량, 또는 Euler angle에 대한 변화량을 오메가로써 표현하는 이러한 방법을 얻게 되는 것 입니다. 이것들은 아무래도 roll/pitch/yaw 라는 값들을 직관적으로 알 수가 있습니다.그 알 수 있는 값들을 속도로써 기술함으로써 우리에게 전달되면 physical meaning, 물리적인 의미가 명확합니다. 이와 반대로 사실상 각도표현법이라고 하는 것들은 앞서서 말씀드렸지만 rotation matrix로 가지고 온 것들입니다.각도표현법에 있어서 rotation matrix를 설명 드릴 때 이러한 rotation matrix 같은 경우에는 어떤 임의의 축, 임의의 축에 대해서 회전하는 Euler’s theorem으로 표현이 가능하다고 얘기했습니다.그러면 두 개의 좌표계가 있었고, 그 두 개의 좌표계 사이의 각도가 Euler angle 또는 다른 방법이 아닌 그냥 그 자체로써 이 rotation matrix가 회전하는 것으로 표현할 수 있는 방법은 없을까라는 그러한 표현법을 생각해 볼 수가 있겠습니다.앞서서 말씀드렸던 Euler axis라고 불리는 특정한 그 각도, 그 각도에 대해서 어떤 일정한 각도만큼 돌리면 두 개의 좌표계를 일치시킬 수 있다고 앞서서 말씀드린 적이 있었습니다.그것들을 이용해서 임의의 축을 찾을 수 있고, 그 축에 대해서 어떠한 rate을 찾을 수 있다면, 회전하는 rate을 찾을 수 있다면, 두 개의 좌표계를 어떤식으로 회전이 일어나는지에 대한 설명이 가능해지는 것입니다.이렇게 두번째 방법으로 $\omega$라는 값을 이용해서 어떤 축과 그 축에 대해서 회전하는 값, 회전하는 양에 대해서 표현하는 방법도 역시 있겠습니다. 그래서 이 두번째 방법, 이 두번째 방법을 두 좌표계 상에서의 수학적인 표현법으로서 명확하게 표현하는 것은 매우 쉽습니다.또 하나 예를 들어서 두 좌표계사이의 rotation matrix가 바뀌어가는 그 rate을 계측을 해서 써먹어야 된다라는 측면에서 봤을 때 계측을 하기 위해서는 일반적으로는 만약에 비행기라고 하면 비행기에 장착되어 있는 IMU센서센서라고 불리는 이러한 자체의 gyroscope하고 accelerometer 같은 것들을 활용해서이 자체의 회전 각속도를 측정하게 됩니다. 이러한 회전 각속도를 측정할 때는 각각은 겉에서 보기에는 다른 좌표계에서 보기에는 정확하게 의미가 뭔지를 모르겠지만이 $\omega$라는 값 같은 것들은 결국에는 어떠한 비행기라던지 움직이는 물체가 이렇게 자세를 취하고 있을 때, 이 자세, 취한 자세에서 이 자세를 똑바로 놨다라고 봤을 때그 자세에서 이루어지는 각각의 축에 대한 속도를 gyroscope 같은 것들로 얻게되는 것입니다.그래서 이러한 Angular velocity vector같은 경우들은 센서로부터 얻고 그리고 두 좌표계를 명확하게 설명할 수 있다는 큰 장점이 있는 반면에문제는 이러한 $\omega$를 직접적으로 접근했을 때, $\omega$를 직접적으로 접근했을 때 얻게 되는 건 무엇이지? 라고 했을 때 명확한 물리적 의미 같은 것들은 파악하기가 어렵다는 단점을 가지고 있습니다. 그래서 첫번째 방법은 physical 의미가 명확하고 yaw방향으로 얼마나 변했는지, roll방향으로 얼마나 변했는지를 정확하게 설명하는 속도표현법이었습니다. 두번째 방법은 수학적으로 딱맞게 떨어지고 수학적으로 유도가 편한 그런 장점, 그리고 실질적인 센서를 활용해서 속도를 측정하는 그런 측면에서 유리한 그런 성질을 가지고 있다라고 할 수 있겠습니다. 이렇게 속도를 표현하는 방법에 대해서 살펴보았습니다.이러한 속도표현법을 바탕으로 이제는 Jacobian matrix라는 것을 먼저 정의를 하고 다시또 이어서 속도기구학에 대한 이야기를 해보도록 하겠습니다.만약에 옆에 보이는 식처럼 $\eta$라고 하는 식이 함수$f$로 주어져있고 그 function들은, 함수들은 $\xi_1$ 부터 $\xi_k$까지의 변수들의 함수라고 생각을 해 보면,이랬을 때 $\eta$ 같은 경우에는 $\eta_1$부터 $\eta_l$까지 l개의 파라미터를 가지고 있고, $\xi$ 같은 경우에는 $\xi_1$ 부터 $\xi_k$까지 $k$개의 파라미터를 가지고 있다고 한번 생각해보시죠.그랬을 때 만약에 $\eta$하고 $\xi$ 사이에 어떠한 속도, 즉 미분관계식을 가지는지를 한번 살펴보도록 하겠습니다. $\dot \eta$을 미분을 하고 $\dot \xi$에 미분된 식과 비교하기 위해서 $J$라고 하는 이러한 매트릭스를 통해서 둘 사이를 표현할 수 있다면,$\dot \eta$과 $J \dot \eta$이라고 하는 옆에 보이는 이러한 식처럼 선형관계식을 찾아낼 수 있을 것입니다.이랬을 때 이 $J$라고 불리는 매트릭스가 어떻게하면 정의가 될 수 있는지를 앞서 수학적 기초에서 설명을 드렸던 체인룰에 의해서 설명하는 방법이 있습니다. 벡터를 벡터로 미분하는 관계식에 대해서, 아랫변에는 벡터의 Transpose를 넣어주고 윗쪽 분자에는 벡터를 아래로 긴 벡터 형태로 넣어주고두 관계식을 Chain rule을 쓰기위한 매트릭스 형태로 이렇게 만들어 주게 되면, $J$를 이와 같이 정의한다면 앞서서 말씀드렸던 $\dot \eta$ = $J \dot \xi$ 이라고 하는 선형관계식이 유도가 되게 되는 것입니다.그리고 그 중간에 $J$라고 하는 것은 각각 두 벡터들 사이에서의 미분관계식을 표현하고 있습니다. 그래서 이 미분관계식을 Jacobian matrix라고 이야기 합니다. 이 Jacobian matrix라고 하는 것은 로봇쪽에서만 쓰이는 것은 아니고 일반적으로 어떤 수식들이 두개의 좌표계, 두개의 어떤 공간이 있을 때 두개의 공간의 어떤 관계식을 설명할 때 그 관계식을 미분의 형태로 설명을 해보자 라고 할때는그 중간의 매개체가 전부 다 Jacobian matrix가 되는 것 입니다. 따라서 이 Jacobian matrix는 여러분들이 다른 분야에서도 널리 들을 수 있는 그런 내용입니다.로봇쪽에서도 역시 마찬가지로 이러한 Jacobian matrix를 또 살펴보시게 될텐데, 각각에서 쓰이는 의미들은 조금씩 다를 수 있지만 내용, 그 자체, 이 Jacobian matrix 그 자체가 포함하고 있는 어떤 의미들은 동일하다라는 것을 여러분들은 기억해두시면 되겠습니다. 그렇다면 우리는 이러한 Jacobian matrix를 가지고앞서서 말씀드린 것 처럼 end-effector velocity, 끝점의 운동을 바로 joint velocity, 이 둘 사이의 관계식을 속도관계식으로써 한번 표현해보실 수 있을텐데요.앞서서 보여드렸던 표를, 앞서서 보여드렸던 $\dot \eta$, $\dot \xi$을 Jacobian에 연결한 것과 마찬가지로, 앞서서 살펴봤었던 기구학, 즉 $r=f(\theta)$, 여기서 $r$은 끝점, 작업공간에서의 움직임이 되겠습니다.$\theta$는 관절공간에서 움직임으로 기술 했었습니다. 이랬을 때 $r$ 하고 $f(\theta)$ 많이 보신 느낌이 들죠. $r$은 예를 들면 $r_1$ 부터 해서 $r_n$까지, 만약에 3차원 공간에서의 6개의 공간에서의 좌표를 표현한다면 6개가 되겠고 $\theta$는 거기에 따라서 만약에 6자유도 manipulator면 6개,7자유도 manipulator면 7개 이런 형태의 dimension을 가지게 되겠습니다.이것들을 미분을 하게되면, $r$이라고 하는 것은 결국에는 위치하고 Euler angle 또는 parameterized된 각도들이 될텐데그 각도들을 가지고 각각 미분을 취하게 되면, 옆에 보이는 식처럼 $\dot r = J_r(\theta) \dot \theta$. 즉, 이 $r$을 가지고 바로 $\theta$를 표현한 Jacobian matrix $J_r$을 유도할 수 있겠고이 때 이 $J_r$은 $\partial r \above 1pt \partial \theta^T$ 즉, $r$을 $\theta^T$의 벡터로써 미분한 형태가 Jacobian의 형태가 되겠죠. 자 이게 method1의 표현법이었습니다. method 2에서 아까 말씀드렸던 $\omega$와 그다음에 $\omega$와 각도의 그 관계식, 각각의 조인트 각도의 관계식으로서 표현을 한번 해 볼텐데 속도라고 하는 $v$라고 하는 것을 $j_v(\theta) \dot \theta$이라고 $J_v$를 찾고 싶습니다.즉 $v$하고 $\dot \theta$. 여기서 $v$는 generalized 되어있는 velocity, 즉 위치에 대한 $\dot P$과 그 다음에 아까 각도표현법을 $\omega$로 한번 표현했습니다.두 번째 방법으로는, 그래서 그 $\omega$하고 $\dot P$으로써 기술되어 있는 벡터 $v$는 결과적으로 $\dot P$ 같은 경우에는 그대로 연결이 되고 그 속도에 대한 velocity는 그대로 연결이 되는데$\omega$ 같은 경우는 이 $\omega$는 직접적으로 계측은 되지만 의미가 없다라고 해서 중간에 어떠한 $Q$라던지 이런 매트릭스 형태로 $\dot \Phi$과 함께 정의를 했었습니다.그래서 $Q \dot \Phi$ 이라고 가정을 한다면 $\omega$는 $Q \dot \Phi$ 이 될거고, 그것들을 쭉 풀어서 앞에있는 각각 $\dot P$과 $\dot \Phi$의 형태를 그대로 유지하게 하기위해서 새로운 매트릭스를 만들어주면$\dot P$은 그대로 연결이 되니까, 윗쪽 3by3 매트릭스는 Identity 매트릭스, 그리고 $\dot P$은 $\dot \Phi$과 그다음에 역시 마찬가지로 $\dot \omega$도 $omega$나 이런 함수들도 $\dot P$하고 관련이 없기 때문에대각텀들은 3by3 0매트릭스가 배치가 되고 그 다음에 대각선 아랫쪽은 $Q_r$로 이렇게 매트릭스 옆에 보이시는 이러한 형태로 새로운 식을 만들 수 있겠고이렇게 만들어지면 조금전에 method 1에서 정리했었던 $J \dot \theta$ 형태로 $\dot P$과 $\dot \Phi$에 의한 식을 유도할 수 있게 되는 것 입니다. 그렇게해서 최종적으로는 $J_v = T_rJ_r$형태로 이렇게 용어들이나 기호들은 조금씩 바뀔 수 있습니다. 지금 전달드리고 싶은 내용들은 정의한 parameterized 된 velocity, 즉 roll/pitch/yaw 라던지 Euler angle이라던지이런 값들로써 얻어진것들의 속도관계식과 조인트 공간, 관절공간에서의 속도관계식을 연결하는 Jacobian과, $\omega$,공간상에서의 좌표계의 움직임을 직접적으로 표현할 수 있는 속도와 $\omega$로 표현되어있는 그 공간과 관절공간의 관계식, 이 두가지를 다 우리는 Jacobian이라는 것으로써 얘기할 수 있는데,두 가지 의미 차이는 조금 있다라는 것. 이것을 조금 기억해두시는게 좋을 것 같습니다. 그러면 이러한 Jacobian을 3링크 manipulator에 대해서 먼저 한번 연습을 진행 해 보도록 하겠습니다.이러한 planar, 이 평면상에서 구동되고 있는 3링크 manipulator를 그림과 같이 가정을 해 보시죠.이렇게 $\theta_1$ $\theta_2$ $\theta_3$로 이뤄진 이 3링크 manipulator에 대해서 연습을 진행해보도록 하겠습니다.각각의 $x y \phi$ 그 끝점에서의 위치값을 $x y$ 그다음에 $\phi$라고 표현을 한다면아마 앞에서 2링크 3링크 manipulator의 예를 들면서 식들을 유도한 기억이 있을텐데요.이렇게 $x y \phi$ 를 각각 미분을 하게 되면 $\dot x \dot y \dot \phi$ 이렇게 주어지고 식들은 옆에 보이는 식처럼 각각의 $x y \phi$를 미분한 형태로써 나타낼수가 있을 것 입니다.이렇게 미분한 형태를 나타난 식들을 바탕으로 여기에서 보시면 앞에는 분명히 $\dot x \dot y \dot \phi$ 이라고 하는것은 정의했던 작업공간상에서의 속도가 되겠고 그 각각의 식들을 $\dot \theta$, $\dot \theta_1 \dot \theta_2 \dot \theta_3$ 이렇게 세개의 관절공간에서의 속도하고 매칭을 시켜보았습니다. 이렇게 매칭을 시켜놓고 이것들을 다음과 같이 정리를 해 보면 $J_r$ 이라고 얘기를 하고이 $J_r$에 각각의 텀들을 이렇게 표현, 그 각각의 인자들을 전부다 따서 이렇게 매트릭스 형태로 표현을 한다면 최종적으로 앞서서 보여드렸던 것 처럼 세타닷을 이용해서 $J$라는 것을 곱해줌으로써,$J \dot \theta$을 곱해줌으로써 원하는 $\dot x \dot y \dot \phi$을 정의하는 식, 즉, Jacobian matrix를 정의할 수 있게 되는 것 입니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 5주차 5-4 D-H approach]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-16-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-5-Weak-5-4%2F</url>
    <content type="text"><![CDATA[D-H approach 순기구학, Forward Kinematics에서 살펴봤던 stanford arm, 즉 다섯개의 revolute joint와 한 개의 prismatic joint로 이뤄진 Stanford arm에 대해서 Forward Kinematics를 어떻게 구하는지를 살펴봤습니다.그렇게해서 최종적으로 말단장치에서부터 베이스까지 오는 Transformation Matrix $T$를 구해봤습니다. 이렇게 $T$를 구했을 때 이렇게 주어진 상태에서 만약의 끝점의 위치와 각도를 이렇게 갔으면 좋겠다 라고 주어졌을 때 어떻게 $\theta_1$ 부터 $\theta_6$까지를 구할 수 있을지를 생각할 수 있습니다. 이랬을 때 이 $T$를 활용하는 방법을 가지고 옆에 보이는 식 처럼 원래는 $A_0$에서 1까지, 1에서 2까지, 2에서 3까지 각각 옆으로 쭉 곱해져서 마지막으로 5에서 6까지 곱해진 형태였는데 거기서 한단계 더 나아가서 만약에 end effctor라고 하는 것을 별도의 하나의 프레임으로 분할을 한다면6부터 end effector 까지 또는 6부터 어떤 특정하게 원하는 좌표까지 이렇게 하나의 변환을 더 추가한다고 가정 했을 때, 총 $A_0$에서부터 1까지부터 시작해서 7개의 매트릭스로 구성이 되는 $T$매트릭스를 앞뒤에다가 inverse transformation을 표현하는 $(^0A_1)^{-1}$를앞쪽에다가 곱해주게 되면 결과적으로는 우측변에 있는 7개의 매트릭스 중에 하나를 제거 해 줄 수가 있고 마찬가지로 맨 뒷쪽에 $A$의 5에서부터 6, 6에서부터 end effector까지 가는 변환에 대한 inverse를 곱해주게 된다면 마찬가지로 우측변에서 5에서6, 6에서 end effector까지 가는 그러한 변환을 제거해 줄 수가 있게됩니다. 이렇게 제거해주는 이유는 처음 base point부터 끝점까지 가는 변환에 있어서 6개의 좌표계를 전부다 한번에 고려를 해서 6개의 조인트값을 동시에 고려를 해서 구한다는 것은 사실상 쉬운일이 아닙니다. 그래서 이 부분을 조금 간단하게 접근하기 위한 방법을 쓰기 위해서 이렇게 하는 것입니다. 이렇게 일반적인 공간상에서 구동되는, 공간상에서의 manipulator의 경우에는 대부분 사람의 손목에 해당하는 wrist 조인트, 손목조인트가 존재하게 됩니다.이 조인트는 앞서서 forward kinematics에서 D-H parameter를 구할 때도 역시 그러한 문제를 본적이 있었는데요.세개의 축이 교차하는 경우가 생깁니다. 세개의 축이 교차한다는 의미는 그 세개의 축의 원점이 같다는 얘기입니다. 이 점을 주로 이용할 것입니다.그래서 대부분의 3차원상에서 구동되고 있는 여러 다축 manipulator의 경우에는 항상 이 손목을 잘 활용하게 됩니다.그것을 활용하기 위해서 이와 같이 먼저 주어져있는 전체적인 Transformation Matrix를 이와 같이 간단하게 바꾸기 위한 노력을 했습니다. 이렇게 바꿔졌을 경우에 앞서서 말씀드렸던 것 처럼 $W$, wrist의 포인트 위치는 생각해보시면 base frame에서 1번 프레임, 1번프레임에서 2번 프레임, 2번프레임에서 3번 프레임즉, $\theta_1$ $\theta_2$ 그리고 2번프레임에서 3번프레임까지 오는 $d_3$ 이렇게 세 개의 값으로만 $W$를 표현할 수 있다는 것을 알 수 있습니다. 이렇게 $T$를 주어진 값을 활용할 건데 이렇게 주어진식을 앞서서 정의했던 것 처럼 $T$값은 정해졌구요, $A$의 1에서부터 2까지 2에서부터 3까지 3에서부터 4까지 4에서부터 5까지 즉 이 리스트에 의한 식,이 네가지 식이 위에서 살펴본 것 처럼 이렇게 주어졌기 때문에 여기에서 A의 0에서 1까지 5에서부터 6까지를 어떻게 활용을 해서 하나 하나 솔루션을 찾아나갈지를 한번 살펴보도록 하겠습니다. 이 식을 진행하기에 앞서서 전부 구했었던 D-H parameter를 이용해서 바로 Transformation Matrix를 먼저 정의할 필요가 있겠죠. 그래서 이렇게 주어져있는 D-H parameter를 활용을 해서 이와 같이 5에서부터 6까지 오는 변환, 그리고 5에서부터 6까지 오는 변환의 역변환이 필요합니다.그래서 5에서부터 6까지 오는 변환과 역변환을 옆에있는 이러한 수식처럼 구해낼수가 있습니다. 마찬가지로 0에서부터 1번까지 오는 변환, 0에서부터 1번까지 오는 변환에 대한 역변환도 필요합니다.이와 같이 이 변환식들을 활용을 하면 앞서서 보여드렸던 수식에 있는 두가지 inverse 파트를 채워넣을 수 있게 되는 것입니다. 그래서 이 두식을 활용을 해서 이와 같이 주어진 $T$에다가 앞뒤에 이 수식을 집어넣어서 계산을 하게 되면, 여기 … 이라고 표시되어 있는 수식에서의 부분들은 로테이션, 즉 각도를 표현하는것을 의미합니다.우리는 앞서 나왔던 식에서 $W$의 위치만을 활용해서 구해내려고 하는 것이기 때문에 앞에있는 …은 일단은 잊어버리시고, 위치의 의미를 가지고 있는 오른쪽 위쪽에 있는 위치벡터를 뽑아서이러한 위치벡터를 각각 $P_x$ $P_y$ $P_z$라는 표현을 써서 뽑아보도록 하겠습니다. 이렇게 뽑아놓은 것 중에서 각각 $P_x^{\ast} = –a_xl_2 + P_x$이고 $P_y^{\ast} = –a_yl_2 + P_y$ 그리고 $P_z^{\ast} = –a_zl_2 +P_z$가 되겠습니다. 이렇게 주어진 식을 이용해서 앞서서 $W$를 구했을 때 $W$에서도 마찬가지로 포지션만 사용을 하고 오리엔테이션은 사용하지 않았습니다.이렇게 두 개로 얻어진 $d_3s_2$는 금방 구했던 수식에 의해서 즉, $a$의 Transformation 0에서부터 1까지, 5에서부터 6까지에 대한 식과 주어진 $T$로 주어진 그 식과 앞서서 $W$를 바로 구했었던이 식을 활용을 해서 이와 같이 (1) (2) (3)과 같은 세 개의 수식을 만들어 낼 수 있겠습니다.여기에서 보시면 이 주어진 수식에서는 결과적으로 이 세개의 수식과 세 개의 미지수가 정해지게 되는데,그 세개의 수식과 세개의 미지수를 바탕으로 즉, $\theta_1$ $\theta_2$ 그리고 $d_3$라고 하는 이 세개의 미지수를 어떻게 결정하는지를 살펴보도록 하겠습니다. 먼저 $t$를 $tan(\theta_1 \above 1pt 2)$ 이라고 이렇게 치환을 한번 해보도록 하겠습니다. 그렇게되면 $cos\theta_1$은 $1-t^2 \above 1pt 1+t^2$ 이라고 쓸 수가 있구요.그 다음에 $sin\theta_1 = 2t \above 1pt 1+t^2$로써 쓰게되고, 이것을 보통 반각공식이라고 얘기를 합니다. 그래서 이렇게 반각공식으로 정의하게 됩니다. 이렇게 정의한 것들을 바탕으로 앞서서 있었던 세번째 수식에다가 $sin$ $cos$에 대해서 각각 집어넣게 됩니다. 이렇게 집어넣게 되면 여러분들 익숙하신 이차방정식입니다.이차방정식이 이렇게 주어지게 됩니다. 이렇게 이차방정식이 주어지게 되면 우리는 근의공식을 활용할 수 있게 됩니다. 근의 공식을 활용하면 $t$는 위에 보이는 식처럼 $-P_x^{\ast} \pm \sqrt{P_x^{\ast ^2}+P_y^{\ast ^2}-l_1^2} \above 1pt l_1 + P_y^{\ast}$입니다.여러분들 이제 이 시점에서 $\pm$가 왜 나오는지 감을 잡으실 수가 있겠습니다. 바로 두개, 복수의 솔루션을 얻게 된다는 그런 의미를 갖게되는 것입니다.$\pm \sqrt{P_x^{\ast ^2}+P_y^{\ast ^2}-;_1^2}$ 이렇게 얻어지게 됩니다. 이렇게되면 여기서 $P_x^{\ast}$, $P_y^{\ast}$, $l_1$은 전부다 알고있는 값입니다.주어진 값이죠. 이러한 주어진값을 바탕으로 결과적으로는 $t= tan(\theta_1 \above 1pt 2)$ 을 결정할 수 있게 되는 것 입니다. 이렇게 결정이되면, 이 값을 통해서 $\theta_1$은 $2arctan$ 그리고 앞서서 구한 $tan$값을 활용을 해서 $\pm$값을 포함하는 이러한 $\theta_1$을 계산할 수 있게 되는 것 입니다.이렇게 $\theta_1$을 구하면서 복수의 솔루션을 찾을 수 있다는 것을 염두에 두셔야 합니다. 그 다음으로 이용할 식은, 바로 두 번째 식과 첫 번째 식을 나누는 식 입니다. 앞서서 사용했던 세번째 식을 제외한 첫번째 식과 두번째 식을 활용해서 $\theta_2$를 한번 구해보면 $\theta_2$는 첫번째 식과 두번째 식에서 보시면 $s_2$, $c_2$만 이렇게 포함이 되고 $d_3$는 아직 모르는 값입니다.모르는 값이지만 $d_3$는 같은 값이니까 $d_3$를 모르더라도 활용을 할 수 있겠구요. 이 두 식을 나눠주게 되면 결과적으로는 $tan$값을 얻게됩니다. 그래서 첫번째 식을 두번째 식으로 나눠주게 되면 이와 같이 $tan$식을 얻게되고, $tan$식을 활용을 해서 $\theta_2$는 $arctan$라는 역변환을 활용해서 이와 같이 구할 수 있게 됩니다.여기서도 마찬가지로 $\theta_1$에 따라서 $c_1$과 $s_1$의 값이 결정되는데, $\theta_1$은 앞서서 살펴봤던 것 처럼 두 개의 솔루션을 얻는다는 것을 확인할 수 있었습니다.이렇게 두개의 솔루션을 활용해서 $\theta_2$도 역시 두 개의 솔루션을 갖는다는 것을 확인 할 수가 있습니다. 이렇게 구한 상태에서 첫번째 수식과 두 번째 수식을 제곱을 해서 더해주게 된다면, $d_3$라는 값을 이 두개의 더해진 수식에 의해서 구할 수 있게 됩니다.위에 보이는 식처럼 $d_3$는 알고있는 값, $P_x^{\ast}$ $P_y^{\ast}$ $P_z^{\ast}$ $l_0$ 값을 활용합니다.거기에다가 추가적으로 앞서서 구했던 $sin$값과 $cos$값, $s_1$ $c_1$의 값을 대입함으로써 $d_3$의 값을 결정할 수 있게 되는 것 입니다.세 개의 수식을 이용해서 $\theta_1$ $\theta_2$ 그리고 $d_3$까지 세 개의 수식을 결정할 수가 있게 됩니다. 그러면 $\theta_1$ $\theta_2$ $d_3$는 앞서서의 방법을 통해서 wrist position을 잘 이용해서 그 포지션의 어떤 관계의 식을 활용을 해서 구했습니다. 그럼 다시 수식을 이와 같이 정리해보겠습니다.0번째부터 1번째, 1번째부터 2번째, 2번째부터 3번째 까지의 변환은 각각 $\theta_1$ $\theta_2$ $\theta_3$를 구했기 때문에 우리가 알고 있는 값, known값이라고 가정을 할 수가 있겠습니다.이러한 Known값을 inverse를 취해서 역행렬을 취해서 왼쪽에다가 곱해주게 되면, $T$역시 주어진 값이라고 앞서 말씀드렸던 것 처럼결국에는 앞쪽은 알고 있는 값들 그리고 뒤쪽에 3번부터 6번까지 가는 이 세 개의 변환은 모르는 값으로 이렇게 배치를 할 수 있게 됩니다.이렇게 배치를 하고 추가적으로 3번 4번 5번의 값들을 구하는지, $\theta_4$ $\theta_5$ $\theta_6$의 값을 어떻게 구하는지 살펴보도록 하겠습니다. $T’$이라고 하는 것을 새롭게 정의하도록 하겠습니다. $T’$은 우리가 알고 있었던 0에서부터 6번까지 가는 그 변환에다가 0에서부터 3번까지 가는 변환의 역수, 역변환을 이렇게 새롭게 정의를 하게 되면 일반적으로 $T$는 $n$ $o$ $a$ $p$ 이렇게 네 개의 벡터로서 정의가 되고, 물론 이 $n$ $o$ $a$는 Rotation Matrix를 의미합니다. 그랬을 때 그 앞에다가 우리가 알고있는 0에서부터 3번까지가는 변환의 inverse를 곱해주었기 때문에 그것들에 대한 값이 좀 바뀌었습니다. 그렇지만 우리가 알고 있는 값입니다.주어진 값에다가 구한값을 넣었기 때문입니다. 그래서 이렇게 새롭게 구한 이 값을 앞의식과 구분하기 위해서 프라임이라는 표현으로 새롭게 정의를 해 보도록 하겠습니다. 그렇지만 그 프라임값은 모두 우리가 알고 있는 값입니다. 만약에 우리가 프라임 앞에 3번에서부터 4번까지 가는 inverse를 곱해준다라고 했을 때,$T’$이라고 되어 있는 것에 3번, 4번에 Transformation Matrix에 inverse를 곱해주게되면 위에 식처럼 {\ast}는 굳이 식을 전개하는데 꼭 필요한 부분이 아니기 때문에 이 값들이 없다는 얘기는 아니고 이 값들은 있지만 표기는 그냥 {\ast}로써 지금 당장 쓰지는 않을 것이라는 의미로써 이렇게 별표로 표시를 했습니다. 그렇게 곱해주게 되면 옆에 보이는 식처럼 각각의 $n_x’$ $n_y’$ $n_z’$ $o_x’$ $o_y’$ $o_z’$ $a_x’$ $a_y’$ $a_z’$ 이라고 되어있는 값들과 $sin$과 $cos$의 네번째 값, $s_4$ $c_4$ 이러한 값들로써 식이 주어지게 됩니다. 이렇게 주어지는 식의 앞에 곱해주게 되었을 경우에는 우측편에는 네번째에서 다섯번째 다섯번째에서 여섯번째로 가는 변환만 남게 됩니다. 이렇게 남게 됐을 때 네번째와 다섯번째 수식, 다섯번째와 여섯번째 수식을 Transformation matrix로 정의하면옆에보이는 식에서의 우측변처럼, $c_5c_6 – c_5s_6 s_5$ 이런식으로 식이 주어지게 됩니다. 이렇게 두 개의 변환 매트릭스를 활용해서 각각을 비교해 볼 텐데요. 이 변환매트릭스에서 활용할 수 있는 값들이 무엇이 있는지 한번 살펴보도록 하겠습니다. 즉 $s_4$하고 $sin$ 네번째 조인트의 값인 $sin\theta_4$ 와 $cos\theta_4$를 포함한 식이 0이라는 수식으로 이용하기 쉬운 수식이 주어지게 됩니다.그렇기 때문에 그 수식을 활용해서 $arctan4$ 값을 구할 수가 있겠죠. 그래서 $a_{y’} \above 1pt a_{x’}$을 하게 되면 $\theta_2$는 $arctan$를 활용해서 구할 수 있게 됩니다.역시 $arctan$ 같은 경우에는 $\theta_4$와 $\theta_4 + \pi$, 즉 $tan$ 같은 경우에는, $\pi$마다, 즉 180도 마다 반복된다는 것을 알 수 있습니다.따라서 $\theta_4$에 대해서도 두 개의 솔루션을 얻을 수 있다는 것을 확인할 수 있습니다. 다음으로는 (1,3), (2,3) element를 활용하도록 하겠습니다. (1,3)element, (2,3) element를 활용하게 되면, 결국에는 $arctan$ 값, $\theta_5$의 $tan\theta_5$가 주어지게 됩니다.$tan\theta_5$가 주어지게 됨으로써 $arctan$를 이용해서 $a_{y’}c_4 + a_{y’}s_4 \above 1pt a_{z’}$로 이렇게 식이 주어지게 됩니다.이렇게 식이 주어지게 됨으로써 $a_{y’}$,$a_{z’}$은 다 알고 있는 값이고 $\theta_4$는 조금전에 구했기 때문에 역시 알고있는 값이 됩니다.이 값들을 대입함으로써 $\theta_5$를 얻을 수 있게 되는데요. 역시 $\theta_5$ 형태도 두 개의 솔루션을 얻게 되는데, 이 때의 값들은 $arctan$ 값에서 앞서 구했던 것 처럼 $+\pi$의 형태로 쓸 수도 있고 또는 $-\theta\pi$?로 즉 어떤 $tan$값의 경우에는 +값과 -값이 앞의 $\theta_4$의 값에 따라서 바뀔수 있기 때문에 굳이 $+\pi$를 넣어주지 않고$\theta_4$에 의해서 $\theta_5$가 각각 두 개의 솔루션을 얻게 되는, 부호가 반대로 되는 그런 효과를 얻게 되기 때문에 굳이 $+\pi$를 해서 진행하지는 않습니다.그래서 $\theta_5$는 앞서 구했던 $\theta_4$에 따라서 부호가 -가 되기도 하고 +가 되기도 하는 것 입니다. 이제 다섯개의 값을 구했습니다. 마지막으로 여섯번째 값을 구해보도록 하겠습니다. 여섯번째 값은 이번에는 (3,1), (3,2) element를 활용하도록 하겠습니다.(3,1), (3,2) element를 활용했을 때 $\theta_6$는 결국에는 $tan\theta_6$가 위에 보이는 식 처럼 $n_{x’}s_4 + n_{y’}c_4 \above 1pt o_{x’}s_4 + o_{y’}c_4$로 이렇게 주어지게 됩니다.역시 마찬가지로 이렇게 주어짐으로써 $\theta_6$값을 결정할 수가 있게 되고 이렇게 $\theta_6$를 구했는데 역시 마찬가지로 $tan$값은 항상 복수의 값이 얻어지게 됩니다.여기서도 마찬가지로 $\theta_4$와 $\theta_4$의 값에 의해서, $\theta_4$가 $+\pi$만큼의 값이 들어감으로써 앞서서 $\theta_5$를 결정할 때 처럼 마찬가지로 $\theta_5$에 대해서 결정을 할 수도 있습니다.여기서는 $\theta_6$에 대해서 각각 그 값을 활용하지 않고 분자와 분모에 동일하게 부호의 값도 동일한 형태로 반영이 되기 때문에 두개의 값에 의해서 자동적으로 $tan$ 값을 결정하는 것이 아니라 $\theta_6$에다가 $+\pi$의 형태의 솔루션을 가지고 두 개의 솔루션을 찾을 수 있다라는 식으로 접근을 하겠습니다. 마지막으로 구했던 $\theta_4$ $\theta_5$ $\theta_6$의 경우에는, $\theta_4$가 만약에 $\theta_4$와 $\theta_4 + \pi$두개의 값을 갖게 된다면, $\theta_4$는 $\theta_5$로 연결이 되구요 $\theta_5$는 다시 두 개의 값으로 나눠지게 되구요.그래서 $\theta_6$와 $\theta_6 + \pi$로 두개의 값이 나눠지게 되고, 마찬가지로 $\theta_4 + \pi$는 $-\theta_5$로 가게되고, 그 값에 의해서 다시 $\theta_6$는 $\theta_6$와 $\theta_6 + \pi$로 총 네 가지 케이스에 대해서 얻어지게 됩니다.그런데 여기에서 만약에 $\theta_5$가 0으로 주어지게 된다면, 여러분들 앞서서 기억하실 지 모르겠지만 stanford arm 같은 경우에는 마지막 손목 조인트들이 ZYZ의 형태로 구성이 되어 있습니다.제가 앞서서 Euler angle을 설명드릴 때 마찬가지로 설명 했습니다.ZYZ의 형태에서 Y가 0, 즉 Y가 움직이지 않는 다섯번째 축이 움직이지 않는 형태가 되면 네번째 축과 여섯번째 축은 같은 각도에 대해서 회전하게 됩니다.즉, $\theta_4$를 돌리는 것과 $\theta_6$를 돌리는 것이 두 가지의 값들이 결과적으로는 끝점, end effector의 입장에서는 같은값을 반영하게 되는 것입니다.그래서 $\theta_5$가 0이 되는 경우에는 보통 degenerate한다. 즉 네번째 조인트값과 여섯번째 조인트값이 독립적으로 어떠한 각도를 만들어 낼 수 있는게 아니라$\theta_5$가 0이 됨으로써 만들어낼 수 있는 각도의 어떠한 전체적인 형상이라던지 이러한 기능들이 떨어지게 된다는 얘기가 됩니다.그래서 일반적으로 이러한 현상은 공간상에서 구동을 하고 있는 3차원 manipulator, 흔히 기구학 해석하는데 많이 활용되는 퓨마560이라고 하는 manipulator또 Stanford arm 등등 사람의 손목 조인트를 모방한 그러한 manipulator와 같은 경우에는 항상 발생할 수 있는 문제 입니다. 그래서 이러한 부분들을 접근할 때 $\theta_5$에 부분에 있어서는 항상 0의 값이 되는지 이렇게 degenerate되는지를 주의깊게 살펴볼 필요가 있겠습니다.이렇게해서 total 세가지 방법으로의 역기구학 해석하는 법을 살펴보았구요. 앞선주차에서 Forward Kinematics 더 앞선 주차에서는 Kinematics를 전체적으로 설명하기 위한 위치표현법, 각도표현법 그리고 이것들을 통한 homogeneous transformation에 대해서 살펴보았습니다.그러한 방법들을 통해서 Forward Kinematics를 구했고 Forward Kinematics에 반대되는 Inverse Kinematics를 구해봤습니다.여러분들이 어떤 느낌이실지 모르겠지만, 제가 느끼기에는 Inverse Kinematics는 굉장히 manually, 한마디로 수작업으로 모든 것들을 하나하나 계산을 해서 각각의 케이스마다 정해져 있는 룰이 아닌, 그 케이스에 따라서 내가 어떻게 계산을 해서 어떻게 구할 것이라는 어떤 전략을 세워야하는 조금 어렵고 귀찮은 작업이 될 수 있습니다.그래서 이런것들은 시간이 많이 소요되고 중간에 실수가 많이 발생할 수 있는 단점을 가지고 있습니다. 늘 이러한 방법을 써야하느냐? 그렇진 않고 또 하나의 새로운 방법을 통해서 이러한 문제점을 극복할 수 있는 방법이 있습니다.예를 들어서 끝점이 어떻게 움직이느냐에 대한 움직임은 결과적으로는 어떤 그 끝점이 속도를 적분을 해서 그 위치를 정한다고 볼 수 있습니다.따라서 속도측면에서 이 해석을 진행을 하고 그 속도를 계속해서 적분해나간다면, 이 속도에 따라서 마지막 끝점이 결정되는 것을 굳이 이렇게 어렵고 time consuming 한 이런 작업을 거치지 않더라도어쩌면 결정 할 수도 있을 것입니다. 따라서 이러한 부분들에 대해서는 이어지는 주차에서 속도기구학이라는 내용을 학습함으로써 이러한 부분을 어떻게 활용할 수 있는지 살펴보도록 하겠습니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 5주차 5-3 Algebraic approach]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-15-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-5-Weak-5-3%2F</url>
    <content type="text"><![CDATA[Algebraic approach Algebraic solution에서는 기하학적인 방법에 의존하는 것이 아니라 각각의 값들을 해석적으로 수학적인 식들만 가지고 풀이하는 방법을 의미합니다.수식을 좀 간단하게 표현하기 위해서 $\cos(\theta_1)$은 줄여서 $c_1$, $\cos(\theta_1+\theta_2)$는 $c_{12}$라는 notation을 사용해서 활용을 하도록 하겠습니다.그러면 앞서서 설명드렸던 $X$점, $Y$점, 그리고 각도를 표시하는 $\theta$는 각각 이렇게 위에서처럼 $l_1c_1 + l_2c_{12}$, $l_1s_1+l_2s_{12}$ 이렇게 표현이 가능하겠죠.이렇게 표현한 상태에서 이렇게 $X$점하고 $Y$점이 주어졌을 때 이 안에 들어가있는 수식들은 전부다 $\sin$과 $\cos$의 값을 포함하고 있습니다. $\sin$하고 $\cos$을 활용을 해서 어떤 수식을 풀어나갈 때 가장 좋은 방법중의 하나는 바로 제곱을 통해서 $\cos^2 + \sin^2 = 1 $이라는 성질을 활용하는 것입니다.조금전에 보여드렸던 수식에서 $x$와 $y$를 제곱해서 더하게 되면바로 이와 같이 $l_1^2 c_1^2 + l_2^2 c_{12}^2 + 2l_1l_2 c_1 c_{12}$ 이런 한 묶음, 즉 $x$에 대한 제곱과 $y$에 대한 제곱으로 이 두가지 식을 더할 수가 있겠됩니다.이렇게 더해주게되면 여기에서 아까 말씀드렸던 것 처럼 $\cos^2$과 $\sin^2$ 같은 경우에는 더하게 되면 1이되죠, 그래서 두 식이 더해져서 $l_1^2$만 남게되구요,역시 마찬가지로 l2제곱에 c12의제곱을 전부다 더하게 되면, 마찬가지로 c12의제곱과 s12의제곱을 더함으로써 l2의 제곱만 남게됩니다.결과적으로는 $l_1^2+ l_2^2 + 2l_1l_2(c_1 c_{12} + s_1 s_{12})$라는 이렇게 제곱이 되지 않는 항들만 남게되는 것 입니다.여기서 $\cos$과 $\sin$의 덧셈법칙을 활용하게 되면 $\cos a \cos b + \sin a \sin b$ 형태이기 때문에 이 두개를 더하면 \cos의 뺼셈법칙이 적용되서 결과적으로는 $2l_1l_2c_2$만 남게됩니다.이렇게 주어지게 되면 앞서 마찬가지로 $(1)^2 + (2)^2$이라고 되어 있는 부분은 각각 $x$하고 $y$구요, $x$하고 $y$는 주어진 값 끝점에 위치했으면 좋겠다라고 하는 그 값이 되겠고 $l_1 l_2$ 역시 주어진 값이 되겠습니다.결과적으로는 모르는 값은 $c_2$만 되겠고, 그래서 $c_2$를 arc \co\sine을 이용하면 이와 같이 주어지게 되는데조금전에 Geometric solution에서 발견했던 그 식과 정확하게 일치하는 수식이 됩니다. 결과적으로는 해석적 접근법은 그림에 의존하는 것이 아니라, 단지 $\cos$과 $\sin$의 제곱, 그리고 그 관계식을 활용한 것 뿐인데 같은 결과를 얻을 수 있다는 것을 확인할 수 있습니다. 2자유도 manipulator 에서 한걸음 더 나아가서 똑 같은 평면이지만 3자유도 manipulator 에 대해서는 어떻게 해석할 수 있는지 한번 살펴보도록 하겠습니다.이번에는 3자유도이고, 그리고 마지막 manipulator 의 각도, End-effector의 각도를 의미하는 $\phi$값까지 총 세개, $x$,$y$, $\phi$값을 내가 이렇게 움직였으면 좋겠다, 이 manipulator가 이렇게 움직였으면 좋겠다라고 하는 방식으로 입력을 넣어주게 됩니다. Geometric 한게아니라 algebraic하게 풀이합니다. 그렇다면 역시 $x$ $y$ $\phi$가 주어지구요, $\theta_1$, $\theta_2$, $\theta_3$를 구하기 위해서 $x$ $y$ $\phi$에 대한 수식을 위에서 보이는 식들과 같이 정리할 수가 있는데요.이 중에서 $\phi$라고 하는 값은 $\theta_1 + \theta_2 + \theta_3$로 주어지게 되고, $\phi$는 갔으면 좋겠다고 하는 주어진 기본값입니다.그렇기 때문에 알고 있는 값이라서 위에 있는 $x$ $y$의 각각 맨 마지막 항인 $l_3c_{123}$ , $l_3s_{123}$의 코싸인과 싸인값은 기본값이라고 생각해도 무방합니다.즉 $\phi$값이 주어짐으로써 $\theta_1 + \theta_2 + \theta_3$의 최종적인 합은 얼마다라는 것을 알 수 있기 때문입니다. 그렇다면 앞서와 마찬가지로 $l_3c_{123}$ , $l_3s_{123}$를 알고 있는 값이 앞쪽으로 빼보게되면 우리는 식은 두 개구요, 모르는 미지수는 구하고자 하는 $\theta_1$과 $\theta_2$ 두 개 이기 때문에 역시 비슷한 방법으로 구할 수 있을 것입니다. 그러면 역시 앞서와 마찬가지로 이 두식을 한번 제곱을 해서 더해보도록 하겠습니다. 그러면 알고있는 값으로 $x$를 다시한번 바꿔서 정의를 해보겠습니다.주어진 $x$, 가고자하는 $x’ = x – l_3c{\phi}$ 전부다 아는 값들이죠.이 아는 값들로 구성된 $x’$과 마찬가지로 $y – l_3s{_\phi}$ 를 포함하는 $y’$. 그래서 $x’$과 $y’$으로 다시한번 식을 정리하면위에보이는 식 처럼, $x’ – l_1c_1 = l_2c{12}$ 이렇게 하나의 식과, $y’ – l_1s_1 = l_2s_{12}$ 이렇게 두 개의 식으로 식을 다시한번 정리해볼 수 있겠구요. 역시 여기에서도 양변을 한번 제곱해보도록 하겠습니다. 양변을 제곱을 해서 더해주게 되면, 이와 같은 식으로 주어지게 되겠구요.이렇게 주어지는 이유는 앞서 말씀드린 것 처럼 각각의 $\sin^2$과 $\cos^2$을 가지고 있는 그러한 성분들은 전부다 더해서 1이되기 때문이죠. 이렇게 최종적으로 식이 주어지게 되는데요.이렇게되면 이 안에 식을 들여다보면 $l_1x’$ , $l_1y’$ , $x’$ , $y’$ , $l_1$, $l_2$는 전부 아는 값이죠. 오로지 모르는 값은 $\theta_1$만 모르게 되겠죠.그렇다면 식은 하나로 주어지구요, 구해야 되는 값이 $\theta_1$으로 결정이 되기 때문에 이 식은 풀 수가 있는데, 식이 좀 복잡하죠.하나는 $\cos(\theta_1)$ $\sin(\theta_1)$ 서로 선형적이지 않은 이러한 식들이 들어가있기 때문에, 이런 식을 풀기 위해서 새로운 방법을 한번 도입해 보도록 하겠습니다. $\theta_1$과 하나의 식이 주어져 있는데, $\sin$만 들어가 있거나 $\cos$만 들어가 있을 경우에는 보통은 $\arcsin$, $\arccos$이라는 함수를 이용해서 구하면 쉽게 구할 수 있는데요. 이와 같이 싸인과 $\cos$이 동시에 들어가 있을 경우에는, 하나의 새로운 변수를 정의를 해서 그 변수를 통해서 새롭게 식을 만들어 줄 수 가 있습니다. 먼저, 앞서서 있는 식을 이렇게 다시한번 써보도록 하겠습니다. $P \cos \alpha + Q \sin \alpha + R = 0$ 이런식으로 식을 다시 이렇게 다시 쓴 상태에서 $\gamma$라는 각도를 새롭게 정의를 할 텐데요. 바로 이 $\gamma$라는 각도는 $Q \above 1pt \sqrt {P^2 + Q^2}$와 그다음에 $P \above 1pt \sqrt {P^2 + Q^2}$라는 두 개의 값을, $arctan$으로 하는 $\gamma$를 정의하게 되겠습니다. 이렇게 정의를 해서 다시 조금전에 새롭게 정의한 식에다가 대입을 하게 되면, ${\cos\gamma \cos\alpha + \sin\gamma \sin\alpha} + {R} \above 1pt \sqrt {P^2 + Q^2}$ 의 형태로 식을 이렇게 쓸 수가 있습니다. 여기서 보시면 여러분들 좀 낯익은 수식이 보여요. $\cos\gamma \cos\alpha + \sin\gamma \sin\alpha$ 바로 코싸인의 뺄셈법칙을 활용 할 수 있습니다. 그래서 $\cos(\alpha - \gamma)$는 결과적으로 $-R \above 1pt \sqrt {P^2 + Q^2}$이 됩니다. 그러면 이렇게 주어지게 되면 $\sin$이 없어지고 $\cos$만 남게되기 때문에 역시 $\arccos$이라는 걸 이용해서 $\alpha = \gamma + \sigma \arccos(-{R} \above 1pt \sqrt {P^2 + Q^2})$이 되겠습니다.여기서 $\sigma$는 옆에 보이시는 것처럼 $\pm 1$이라고 말씀을 드렸는데요.앞서서 Algebraic solution에서 그리고 또 Geometric solution에서 살펴봤던 것 처럼 $\cos$ 같은 경우에는 $\arccos$의 경우에 +,- 두 개의 값을 복수의 솔루션으로 갖는다고 말씀을 드렸습니다.그래서 그 값을 표시하기 위해서 이렇게 $\pm 1$이라는 값을 $\sigma$에 대입을 하게 되면 이렇게 두 개의 솔루션을 다 포함하게 되는 그런 식을 얻을 수가 있게됩니다. 원래 식에서 $-2l_1x’$을$P$라고 정의를 했었고, 그리고 $-2l_1y’$을 $Q$ 그리고 나머지 뒤에 있는 식들을 $R$이라고 표시 했습니다. 이렇게 $P$, $Q$, $R$을 활용해서 $\gamma$를 정의 하겠습니다. $\gamma$를 정의해보면 즉, 위에 있는 수식으로 $\arccos$값을 정의할 수가 있습니다 결과적으로 이렇게 $\gamma$값을 앞서서 정의를 했었고, 그리고 조금전처럼 $\arccos$에 들어가는 인자들은 모두 찾을 수가 있었습니다. 앞서서 구했던 그러한 방법들을 통해서 $\theta_1$을 정의할 수가 있습니다.이렇게 구한 $\theta_1$을 이제는 앞에서 사용했었던 $x - l_3c\phi = l_1c_1 + l_2c_{12}$ 라는 $x$에 대한 식과 그리고 마찬가지로 $y$에 대한 식에다가 각각 $\theta_1$의 값을 집어넣도록 하겠습니다.각각의 값을 집어넣게 되면 남는 식들은 결과적으로는 $\theta_2$에 대한 식만 주어지게 됩니다.그래서 $\theta_2$에 대한 식, $\theta_1$,$\theta_2$에 대한 식이죠 엄밀히 말하면, $\theta_1$,$\theta_2$에 대한 식이 주어지면 그 $\theta_1$,$\theta_2$에 대한 식을 이용해서 $arctan$라는 명령어를 활용을 해서 $\theta_1$,$\theta_2$를 구하게 되고,$\theta_1$,$\theta_2$에서 $\theta_1$을 빼주게되면 최종적으로 $\theta_2$를 구할 수 있게 되는 것이죠. 이렇게 해서 $\theta_2$를 구했구요, 다시 $\theta_2$을 앞서서 구했기 때문에 $\theta_1$과 $\theta_2$를 모두 구한 상태에서 우리는 이 식들을 활용을 해서 $\phi – (\theta_1 + \theta_2)$라는 값을 활용을 하게 된다면 이 식들을 통해서 $\theta_3$는 쉽게 결정을 할 수가 있겠죠. 앞서 살펴본 것 처럼, $\theta_1$,$\theta_2$,$\theta_3$를 순차적으로 하나씩 Algebraic solution을 이용해서 구할 수가 있었고요. 앞서서 $\theta_1$을 구했을 때 $\sigma$는 $\pm 1$이 된다즉 두 개의 솔루션을 얻을 수 있다. 그래서 이 두 개의 식을 어떻게 그림으로 표현을 할 수가 있냐하면 위에 보이시는 것 처럼 $\sigma = -1$이라면 lower arm, 즉 암의 형태가 아랫쪽으로 이렇게 구부러지는 형태가 되는 이러한 형상을 띄게되는 것이고 반대로 $\sigma = +1$ 이라는 값을 나타내면 upper arm 이렇게 위로 구부러지는 형태의 복수개의 솔루션을 얻게됩니다. 이랬을 때 $theta_1$과 $\theta_2$는 각각의 값들에 따라서 부호가 같이 결정되는 그러한 형태가 됩니다.$\theta_3$같은 경우 복수로 얻어지지 않는 이유는 $\theta_1$과 $\theta_2$가 결정이 되면 결국에는 마지막 $\phi$값을 결정해주기 위해서,즉, 마지막 End Effector가 사용하는 각도를 결정해 주기 위해서 마지막 축은 항상 같은 위치를 차지하고 있어야 하기 때문에 $\theta_3$에 대해서는 단일 솔루션을 얻게 되는 것 입니다.여기에서 만약에 구해야 하는 점이 있는데, 그 점이 범위를 벗어난다거나 또는 $\theta_1$, $\theta_2$, $\theta_3$를 결정할 수 없거나,그 솔루션을 찾을 수 없을 경우에, 우리는 그 지점을 \singular point라고 이야기 합니다.\singular point라고 하는 것은 즉, 가고싶은 $x$, $y$, $\phi$가 있을 때 그 $x$ $y$ $\phi$에 해당하는 $\theta$값을 정할 수 없는 경우를 \singular point, 즉 특이점이라고 이야기 하고 이 특이점에 대해서는 보다 자세하게는 나중에 속도기구학을 설명할 때 조금 더 자세하게 설명 드리도록 하겠습니다. 그리고 조금전에 3자유도 manipulator에 대해서 평면상에서의 3자유도 모션, 즉, $x$,$y$ $\phi$가 전부 주어졌을 때 1:1로 $\theta$값들을 결정을 해 봤습니다.그것이 결정되지 않았을 경우를 \singular point라고 얘기했습니다.만약에 끝점의 위치는 중요하지만 끝점의 각도는 중요하지 않을 경우, 즉 “끝점의 각도는 상관없으니까 니맘대로해, 나는 $x$,$y$만 잘 쫓아갔으면 좋겠어” 라고 사용자가 입력을 넣어주게 된다면 $\phi$값에 대해서는 값이 없기 때문에, 어찌보면 이 끝점의 위치를 고정하고 많은 자세들을 만들 수 있습니다. 즉, orientation이 specify 되지 않은 orientation을 특정하지 않았을 경우에는 무한히 많은 솔루션들, 나중에 이것을 null motion 이라고 얘기할 건데요.Redundant manipulator의 개념에서 잠시 다시한번 자세하게 설명을 드릴텐데 무한대의 솔루션, 즉 이런 무한대의 솔루션이 나오는 경우는 일반적으로 관절공간에서의 자유도 공간이 작업공간의 자유도 보다 많을 경우작업공간은 원래는 $x$,$y$,$\phi$였는데 $\phi$는 관여하지 않는다고 얘기 했기 때문에, $x$,$y$ 2차원으로 표현이 되고관절공간은 $q_1$, $q_2$, $q_3$ $\theta_1$, $\theta_2$, $\theta_3$ 처럼 세 개의 자유도로 구성이 되어 있기 때문에 관절자유도가 작업공간자유도보다 높아서 발생하는 현상 입니다.이럴경우를 Redundant case라고 얘기하고, 이 케이스에 대해서는 역시 속도기구학에서 조금 더 자세히 설명 드리도록 하겠습니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 5주차 5-2 Geometric approach]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-14-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-5-Weak-5-2%2F</url>
    <content type="text"><![CDATA[Geometric approach 여기에서 주어진 2자유도 Manipulator의 X, Y점이 주어졌을 경우에 각각 구해야 되는 것은 이 $\theta_1$의 각도와 $\theta_2$의 각도입니다이 Two Link Manipulator의 경우에는 $\theta_2$부터 어떻게 해석할지 먼저 접근을 시작하지만 경우에 따라서는 여러 가지 주어지는 상황에 따라서 여러분들이 어떤걸 먼저 결정해야 되는지 그때그때 바뀌어야 합니다. 그래서 여기에서 주어진 Two Link Manipulator의 방법을 그대로 적용하는 것은 아니고 그때그때마다 주어지는 Manipulator의 상황에 맞게 바뀌어야 된다는 점을 먼저 염두에 두시기 바랍니다.기하학적인 방법으로 하기 위해서 우리는 먼저 $cos$법칙을 활용해서 $\theta_2$를 결정해볼 텐데요. $cos$ 제2법칙을 이용하면 $c^2 = a^2 + b^2 – 2ab cosC$ 입니다.$a,b,c$ 라고 하는 세 변의 길이가 주어지고 각각 마주보는 각도를 $ C, B, A$ 라고 했을 때 이 각 변의 길이와 끼인각들의 관계의 식을 설명하는 것이 바로 $cos$ 제2법칙이 되겠는데요.그래서 $c^2$이 가장 긴 변의 제곱이고 이 값은 $a^2 + b^2$, 즉 나머지 두 변의 제곱을 더하고, 그리고 $-2ab$에 $cos$ 끼인각 $C$의 값을 이용해서 성립하는 법칙입니다.이것을 2 Link Manipulator에 적용을 하게 된다면 $\sqrt {x^2+y^2}$을 하는 것은 결국 이 길이를 의미하고 이게 C의 역할을 합니다.그리고 각각 $l_1$ 과 $l_2$의 길이는 주어져있기 때문에 $l_1^2$ $l_2^2$을 집어 넣고 $2l_1l_2$을 끼인각은 결국에는 이 $\theta_2$, $\theta_2$에서 180도에서 $\theta_2$를 뺀 각 만큼 즉, $cos( 180 - \theta_2)$ 만큼의 값을 갖게 됩니다. 여기에서 우리는 $cos$의 성질에 의해서 $cos(180 - \theta_2$는 결국 $-cos(\theta_2)$가 된다는 것을 아실 수 있을 겁니다.이 성질을 이용해서 결과적으로 $cos(\theta_2)$는 $x^2 + y^2 -l_1^2-l_2^2 \above 1pt 2l_1l_2$이 되겠습니다. 여기에서 좌변의 경우에는 $cos(\theta_2)$ 우변의 경우에는 $l_1 l_2 x y$ 들만의 함수로써 주어져 있습니다.따라서 모두다 아는 값들이죠. 그래서 이 값을 통해서 $cos(\theta_2)$의 값을 구하고 결과적으로는 $arccos$이라는 것을 이용해서 $\theta_2$를 결정할 수 있게 되는 것입니다. 그러면 이어서, $arccos$을 통해서 $\theta_2$의 값을 얻게 되는데요, 여러분들이 코싸인의 그래프를 생각해보시면, 코싸인 그래프는 이렇게 주어지죠.그러면 주어지는 어떠한 값들은 그때에 복수의 솔루션을 갖게 되는 것이죠. 그렇기 때문에 세타2의 값은 각각의 코싸인의 값에 따라서 두개의 복수의 값이 주어지게 되겠습니다이 두 개 값에 의해서 $\theta_2$가 결과적으로는 이렇게 작은각과 큰각으로 주어질 수 있는데 그렇게 주어졌을 때 이렇게 upper arm과 lower arm의 형태로써 $\theta_2$가 결정되게 되는 것 입니다.그래서 이렇게 $cos$제2법칙을 이용해서 $\theta_2$는 $arccos$ 이라는 식을 이용해서 결과적으로 다 주어진 $X,Y,l_1,l_2$를 이용해서 구할 수가 있습니다.이 때 주의해야할 점이 아까 살펴본 바로는 이렇게 Manipulator의 형태가 upper arm과 lower arm의 형태로 배치가 될 수가 있고 두 가지 솔루션을 구할 수가 있다라고 말씀을 드렸는데요. 이렇게 주어지는 이유가 $cos$의 그래프를 그려보면 $cos$의 그래프는 이와 같이 그려지는 것을 여러분들이 확인하실수가 있죠.그래서 같은 값이더라도 이러한 값과 이러한 값으로 두 가지 솔루션을 얻게 되는 것이죠. 그렇기 때문에 또는 이 값은 마이너스값이라고 볼 수도 있는 것이고요.그래서 이렇게 $cos$제2법칙을 이용해서 $\theta_2$는 $arccos$ 이라는 식을 이용해서 결과적으로 다 주어진 $x,y,l_1,l_2$를 이용해서 구할 수가 있습니다.이럴 때 주의해야할 점이 아까 살펴본 바로는 이렇게 Manipulator의 형태가 upper arm과 lower arm의 형태로 배치가 될 수가 있고 이렇게 $\theta_2$를 먼저 결정하고 나면 그 다음으로는 $\theta_1$을 찾아야 합니다. $\theta_1$의 값을 구할 때 우리는 $sin$법칙을 활용합니다.$sin$법칙은 삼각형이 이렇게 주어졌을 경우에 마주보는 각과 그리고 그 마주보는 길이, 마주보는 변의 길이를 싸인을 통해서 서로 같은 비율을 가진다는 것을 이용하는 것 입니다.즉, 이 끼인각이 B고 이 길이가 b 였을 경우에, 이렇게 주어졌을 때 이것들간의 관계는 여기에 주어진 것 처럼 $sin$법칙으로, 변의 길이 분의 그 변을 마주보는 각도에 대한 $sin$값은 서로 모든 세 각과 세 변에 대해서 성립한다는 것을 활용합니다. 조금전에 $\theta_2$를 구했으면 이 삼각형 내에서 이 각도를 구한것과 같은 개념이죠. 그래서 이 각도는 $sin(180 - \theta_2)$라는 값으로 사용할 수가 있습니다.이 각의 마주보는 이 변의 길이를 $\sqrt {x^2+y^2}$ 즉, 이 변의 길이를 이렇게 정의할 수가 있게 됩니다.이렇게 된 상태에서 필요로 하는 $\theta_1$을 얻고 싶은건데, $\theta_1$값은 이 $\alpha$, 즉 $x$하고 $y$라는 값이 주어지면 $arctan$에 의해서 구할 수 있는 이 $\alpha$값과 그리고 이 사이에 끼인각을 $\theta_1$에 바라고 했을 경우에 이 두개의 더하기로 이 $\theta_1$을 결정할 수 있게 됩니다.그러면 이 $\theta_1$을 조금전에 말씀드렸던 이 각의 비율을 활용해서 구한다면 이 마주보는, 이 $\theta_1$에 대한 마주보는 이 길이를 $c$ 또는 $l_2$라고 했을 경우에 이 $sin(\theta_1) \above 1pt l_2$의 값을 이용해서 이러한 방법을 이용해서 이 $\theta_1$에 대한 값을 $\bar \theta_1$에 대한 값을 구할 수 있게 되는 것이죠. 이렇게 $\bar \theta_1$와 $\alpha$에 대한 수식을 정리를 했습니다. 그렇게 얻어진 것을 바탕으로 $\theta_1$은 $arcsin$, 이 수식 그리고 $arctan2$라는 값을 이용을 해서 역시 이 안에서도 보시면 $x,y,l_2$ 전부다 알고 있는 값들 이었고 $\theta_2$라는 값은 바로 이전에 $cos$제2법칙을 이용해서 구한 값이 되겠죠. 그래서 $\theta_2$의 값에 따라서 역시 마찬가지로 $\theta_1$의 값도 두 개가 나올 수 있는 것이죠.아까 전에 그림을 통해서 설명드렸던 것처럼, $\theta_2$의 값이 이렇게 주어지게 되고 $\theta_2$의 값이 이렇게 두 가지의 솔루션이 나온것과 마찬가지로 만약에 $\theta_2$의 값이 이렇게 +의 값으로 주어지게 되면 이게 $\theta_1$의 값이 되겠고요. 만약에 $\theta_2$의 값이 이와같이 -값으로 주어지게 되면 $\theta_1$의 값은 이렇게 주어지게 되겠죠. 즉 $\theta_2$에 따라서 마찬가지로 $\theta_1$도 두 개의 솔루션을 얻게되는 결과를 볼 수 있습니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 4주차 4-3 D-H notation]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-11-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-4-Weak-4-3%2F</url>
    <content type="text"><![CDATA[D-H notation|![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3.jpg)| D-H parameter라고 하는 것은 1955년도에 Denavit교수님과 Hartenberg교수님께서 만드신 `notation’으로지금까지도 로봇기구학에서는 golden standard로 표현될 만큼 계속해서 사용되고 있는 표현입니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(2).jpg)| Denavit-Hartenberg notation이라고하는 것은 systematic하게, 즉, 어떤 로봇이 주어졌을때 그러한 로봇이 주어진 상태에서 어떻게 좌표축을 설정하고그 좌표축 간의 관계는 어떻게 표현을 하고, 그랬을때 그 표현법에 의해서 원하는 ‘Homogeneous Transformation’을 어떻게 생성하는지를자동화하는, systematic하게 표현하는 기법을 의미합니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(3).jpg)| D-H notation은 몇가지 과정과 그리고 설정이 필요합니다. 첫째, 만들어 놓은, 또는, 만들고 싶은, 해석하고 싶은 manipulator가 있다면 이러한 manipulator는 n+1 link로 구성이 된다.앞서서 3자유도 로봇 같은 경우에는 3개의 joint와 3개의 link로써 구성이 되어있다고 했는데 여기서는 n+1 link로 표현을 하는데요.지구상에 보통은 로봇들이 발을 디디고 있어야 됩니다. 그래서 고정되어 있는 바닥을 하나의 link라고 표현을 해서 총 n+1개의 link를 가지고 있다. 라고 얘기를 합니다.그리고 그 link와 link 사이 base link와 로봇, 그 중간에 하나의 joint가 들어가고 총 n개의 joint들을 가지고 n+1개의 link들을 연결하고 있다. 라고 하는 그러한 과정을 사용을 하고 있습니다.그리고 각각의 링크에는 좌표계를 하나씩 붙일 것입니다. 그리고 그 좌표계가 각각의 링크마다 좌표계가 하나씩 붙어있다라는 이러한 과정을 통해서 그 좌표계 상에서의 변환을 통해서 끝점의 위치라던지 아니면 또 로봇상의 중간에 어떠한 점이라도 상관없습니다. 그러한 점의 위치를 찾아내기 위해서 활용할 예정입니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(4).jpg)| 그러면 이러한 과정을 바탕으로 framework을 한번 설명을 드릴 필요가 있을것 같은데요.기본적인 D-H parameter를 잡기 위해서는 가장 중요한 것은 Z축입니다.일단은 Z축을 잡는 가장 기본적인 방법은 joint i+1 번째에 위치시키는 것을 원칙으로 합니다.즉, 여러분들은 i 번째의 joint에 i번째 frame을 잡는 것이 아니라 i+1 번째의 joint에 i번째의 frame이 되는 것입니다.예를들면, 첫 번째 joint가 있다면 0번째 frame이 붙게 되는 것입니다. 이 부분을 혼동을 하면 안됩니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(5).jpg)| 이렇게 i번째 frame에다가 $Z_1$ 축을 잡았다고 했을 때 $Z_i$번째 원점이 존재했을 때 그 다음으로 필요한 것은 $Z$축과 관계되는 $X$축과 $Y$축을 잡아줘야 되는데이 $X$축을 결정하기 위해서 필요한 것은 이전 축입니다. 즉, $i$번째 축을 가지고 있는데 이전에 i-1번째 축은 어떻게든지 만들었다라고 가정을 한다면, 그 만들어진 축에서 이 i축, 즉 i-1번째 축에서 i축을 연결하는, i축의 Z축을 연결하는 공통의, 공통으로 수직되게 연결될 수 있는 하나의 선을 정할 수 있게 됩니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(6).jpg)| 그렇게 선을 정하게 되고 그렇게 선을 정한 상태에서 이 공통 수직선을 보통 common normal방향이라고 하는데이 common normal방향을 $X_i$축으로 설정하게 되는 것 입니다.즉, joint i + 1 번째의 $Z_i$축을 설정하게 되는 것 입니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(7).jpg)| Z축을 잡았고요 또 Z축과 Z축이 존재했을 때 두 개의 common normal 방향으로 X축을 잡습니다.이렇게 잡고나면 두개의 축이 결정되면 나머지 축은 오른손 법칙에 따라서 Y축은 자동적으로 결정되게 되는것이죠. 그래서 여러분들은 Y축에 대한 고민을 하실 필요가 없으시고요.Z축을 어떻게 잡는지 그리고 그 Z축에 대해서 X축을 어떻게 결정하는지만 잘 기억해 두시면 되겠습니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(8).jpg)| 그렇다면 ‘D-H notation’에서 어떻게 Z 축을 잡는지에 대해서 한번 그림을 통해서 살펴보도록 하겠습니다.로봇을 활용할 때 주로 활용하는 로봇의 조인트는 전부 다 Revolute joint 아니면 Prismatic joint가 거의 대부분입니다.물론 이 외의 joint들에 대해서는 어떻게 기술할 것이냐 물론 그런 것들에 대한 방안들도 있지만 일반적으로는 Rotational하는 Revolute joint와translational하는 Prismatic joint 이 두가지 joint가 거의 모든 로봇, 90%, 아니면 99% 이상 로봇들에서 다 활용이 되고 있습니다.그렇기 때문에 D-H parameter, 특히 1955년도에 이 개념을 만들 당시에는 이 두가지만 가지고도 충분히 설명이 가능하지 않겠냐 라고 해서 이 개념을 만들기 시작했던 것이고그 개념이 사실상 지금까지도 그렇게 크게 위배되지 않게 사용되고 있기 때문에 D-H parameter가 아직까지도 널리 이용되고 있다고 보면 되겠습니다.그러면 이 Rotational한 Revolute joint축에 대해서 Z축을 회전축에 대해서 잡아주자.회전하는 축을 기준으로해서 Z축을 잡아주자 라고 하는 것이 기본적인 rotation하는 joint에 대한 Z축 방향이 됩니다.그리고 그 다음에 translational하는 경우에 대해서는 바로 이동하는 방향에 대해서 Z축을 잡아줍니다.그러면 두 가지의 개념을 두 가지의 joint에 대해 살펴보자면 각각 Z축은 이동하는, 회전을 한다고 하면 결국은 회전하는 방향에 대해서 Z축을 잡아주는 것입니다,그리고 Prismatic joint, translational하는 이러한 Prismatic joint에 대해서도 역시 마찬가지로 이동하는 방향에 대해서 역시 Z축을 잡아준다.이렇게 Z축을 잡아주는 방법은 두가지 방법으로 나눌 수 있다는 것을 기억해두시면 되겠습니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(9).jpg)| 축을 어떻게 잡는지에 대해 설명을 드렸는데요. 그 다음에 필요한 것이 X축을 정하는 것이 되겠습니다. 그래서 X축을 정하는 방법은 조금전에 말씀드렸던 것처럼$Z_{i-1}$번째 축에서 $Z_i$축까지를 연결하는 공통으로 수직하는 line. common perpendicular상으로 결정하기만 하면 X축을 잡아주게 되는 것입니다.그래서 이 위에 있는 그림처럼 $Z_i$축과 $Z_{i-1}$번째 축이 있으면 공통으로 수직되는 그 line만 찾아주면 되는 것이죠. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(10).jpg)| 그런데 항상 이 X축을 찾을 수 있는 것은 아닙니다. 예를 들어서 0번째 frame에 대해서 Z축을 결정할 수 있습니다.X축을 결정하기 위해서는 $Z_{n-1}$번째 축이 존재해야 합니다. 그렇지만 우리는 0번째 frame부터 시작하기 때문에 $Z_{n-1}$번째 축이 존재하지 않습니다.그렇기 때문에 X축을 결정할 수 없게 됩니다.그렇지만 0번째 frame은 보통 기준 좌표계를 의미하기 때문에 0번째 좌표계에서의 규칙은 여러분들이 임의로 여러분들이 편한 방법으로 X축을 결정해주면 되는것입니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(11).jpg)| 또 이와 마찬가지로 또 이와 비슷한 개념으로 마지막축일 경우에는 $Z_n$축 같은 경우,마지막 축에서는 joint를 가지고 joint를 Z축을 잡아주게 되는게 마지막 $Z_n$축 같은 경우에는 joint가 존재하지 않습니다.즉, link의 끝점이 될텐데. 끝점일 경우에는 조인트가 존재하지 않기 때문에 Z축을 결정할 수가 없습니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(12).jpg)| 그래서 일반적으로는 두가지 방법을 많이 쓰고 있는습니다. 한가지 방법으로는 바로 이전 축의 Z축. 즉, $Z_{n-1}$번째 축과 같은 방향으로 $Z_n$축을 잡아주는 경우 두번째 방법으로는 보통 마지막 joint 같은 경우는 일반적으로 end-effector, 마지막에 있는 손가락이 될 수도 있겠고 집게가 될 수 있겠고 이러한 포인트에서 point out하는 방향,즉, 밖으로 향하는 방향으로 잡아주는 경우도 있습니다. 그것들도 역시 마찬가지로 여러분들이 필요에 따라서 결정을 하면 되는 것입니다. 그래서 앞서서 X, 첫번째 $X_0$축을 결정할 때는 즉 $Z_{n-1}$번째 축에 있기 때문에 결정할 수 없게 되는 것입니다 마지막축인 경우에는 역시 마찬가지로 마지막 축에는 joint가 존재하지 않기 때문에 축을 결정할 수가 없으니 arbitrary하게 여러분들의 입맛대로 결정하면 되는 것입니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(13).jpg)| 그 다음에는 만약에 Prismatic joint라고 생각을 해보셨을 때,Prismatic joint라고 하면 translational하는 joint가 되는데 이동하는 방향이 양쪽 방향 다 이동할 수가 있습니다.그렇기 때문에 이 경우에 Prismatic joint같은 경우에도 Z축은 방향은 위쪽 아니면 아래쪽인데, 벡터는 결정이 되지만 벡터의 방향은 위쪽과 아래쪽 이 두개로 나눠질 수 있게 때문에그 경우는 일반적으로는 로봇이 링크로 연결되다 보면 Prismatic joint가 중간에 들어가면 Prismatic joint가 늘어나는 방향으로 Z축의 방향을 보통 설정 하는데반대 방향에 경우 필요에 따라서 그렇게 설정을 하더라도 무리가 없습니다. 여러분들이 여러분들 입맛대로 여러분들이 원하는대로 그렇게 설정해주시면 되겠습니다 |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(14).jpg)| 그 다음으로 D-H parameter를 구하기 위해서 하나씩 rotation을 전개하다 보면 Z축과 $Z_{i-1}$의 Z축이 이렇게 두개가 교차하는 경우와 평면상에서 교차하는 경우.대부분 다 수직으로 교차를 많이 할텐데 수직으로 교차하는 경우가 생깁니다.이럴 경우에는 이 두 축에 대해서 수직인 방향은 바로 이 두개의 축이 형성하는 평면에 대해서 나오는 이 평면의 법선 방향으로 나오는 이 축이 됩니다.그래서 Z축과 $Z_{i-1}$ 번째 축이 교차할 경우에는 이 X축은 평면에 대해서 수직으로 나오는 방향. 그런데 이 수직으로 나오는 방향은 위로 나올 수도 있고 아래로 나올 수도 있습니다.그렇기 때문에 역시 마찬가지로 위든 아래든 여러분이 원하시는 방향으로 X축을 정해주면 됩니다.그렇지만 일반적인 방법으로는 $Z_{i-1}$ 번째 축에서 $Z_i$축으로 이렇게 손가락을 감쌌을때 엄지손가락이 향하는 방향쪽으로 일반적으로 많이 잡아줍니다. 그렇지만 반대로 잡는다고해서 틀린 방법은 아닙니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(15).jpg)| 그 다음으로 만약에 Z축이 서로 수평으로 놓여있을 때, 즉 $Z_{i-1}$번째와 $Z_i$축이 수평으로 이루어질 경우에는 두개를 연결하는 수직선, 두개를 공통으로 연결하는 common perpendicular line이 무수히 많이 존재하게 됩니다.무수히 많이 존재하게 되기 때문에 우리는 그 중에서 임의로 하나를 설정한 다음에 일반적으로는 원점을 기준으로해서 교차할 수 있도록 보통은 설정해주게 됩니다.역시 이 위치도 여러분들의 필요에 따라서 바꿀수는 있습니다. 그래서 정확하게 결정되지는 않는다는 그런 단점을 가지고 있습니다.그리고 마지막 case의 경우 $Z_{i-1}$번째 축과 $Z_i$축이 같은 이런 평행하지만 평행하면서도 아예 같은 방향으로 같이 일치하는 경우가 있습니다.이경우에는 X축을 어떻게 결정할 수 있을까요? X축은 두개의 common perpendicular인데 X축은 무수히 많이 존재할 수 있습니다.어떻게 되냐하면 이 축을 감싸고 있는 평면전체가 바로 $Z_{i-1}$번째 축과 $Z_i$축에 모두 수직한 그런 vector가 되기 때문에 이 경우에도 여러분들이 임의로 X축의 방향을 결정할 수 있게 됩니다.그리고 이 경우에는 조금 후에도 다시 말씀드리겠지만$Z_{i-1}$번째 축과 $Z_i$축을 두개의 떨어진 거리를 이제 필요로 하게 되는데 그 떨어진 거리도 임의로 원점을 잡아줌으로써 그 거리를 결정하게 되는데이 원점도 마찬가지로 여러분들이 임의로 여러분들이 편의에 따라서 잡을 수 있게 됩니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(16).jpg)| 어떻게 보면 이렇게 모호성을 많이 가지고 있는 방법이 조금 복잡하다 느끼실 수도 있겠는데 어떻게 보면 이러한 모호성이 여러분들한테 많은 자유도를 줄 수 있다. 라고 생각해 볼 수 있는게 좋을 것 같습니다.앞서서 Z축하고 X축을 어떻게 잡는지 살펴보았습니다. X축과 Z축을 잡으면 Y축까지 자연스럽게 결정된다고 말씀을 드렸고요,그렇다면 각각의 joint에 어떻게 좌표계가 설정 되는지가 다 결정이 되었으니깐 그러한 좌표계를 이용해서 D-H parameter를 어떻게 정의하는지도 한번 살펴보도록 하겠습니다.D-H parameter라고 하는 것은 총 4개로 구성되는데 $\theta$, $d$, $a$, $\alpha$가 되겠고요, 이 중에서 여러분들은 $\theta$하고 $d$를 묶어서, $a$하고 $\alpha$하고 묶어서 한번 생각해 보시면 좋을 것 같습니다.보통 $\theta$하고 $d$같은 경우는 모두 $Z_{i-1}$축을 이용을 해서 정의를 하기 때문입니다. 그리고 마찬가지로 $a$하고 $\alpha$는 $X_i$축을 기준으로 정의를 하기 때문에 두개씩 묶어서 정의하시는게 이해하시기 좋습니다.$\theta$ 값 같은 경우에는 $Z_{i-1}$ 축을 기준으로 $X_{i-1}$축으로 부터 그다음 Xi축으로 얼마만큼 틀어져 있느냐, 얼마만큼 각도가 rotation되어 있느냐? 를 설명하는 각도가 되겠습니다.$Z_{i-1}$축에 당연히 수직인 $X_{i-1}$축과 그라고 $X_i$축을 $Z_{i-1}$축에서 수직되게 결정을 했기 때문에 두 개는 결국엔 $Z_{i-1}$축을 기준으로 회전됐다라고 생각을 할 수가 있는 것이죠.그래서 그 두 축 사이의 회전을 $\theta$i라고 설정을 하게 됩니다. 그 다음에 $d_i$라고 하는 것은 $Z_{i-1}$ 방향으로 이 $Z_{i-1}$축을 포함하고 있는 i-1프레임의 원점에서부터 그리고 이 $X_i$ 축이랑 만나게 되는 점이 존재하게 될텐데 이 두 점 사이의 거리를 $d_i$라고 결정을 하게 됩니다.이렇게 $Z_{i-1}$축을 기준으로 두개의 parameter $\theta$하고 $d$를 살펴 보았습니다. 이어서 $X_i$축을 기준으로 $a$하고 $\alpha$에 대해서 살펴보도록 하겠습니다. $a$라고 하는것은 $Z_i$번째 축을 포함하고 있는 i번째 프레임에서 이번에는 $Z_{i-1}$번축에서부터 i번째 프레임의 원점까지 떨어진 거리를 결국에는 이 떨어진 거리를 $X_i$ 축 방향으로 떨어진 거리를 정의할 수 있는데두개 사이의 축이 즉, $Z_{i-1}$과 $Z_i$가 수직하기 때문에 그 수직하는 두개 사이의 거리를 결정할 수 있게 되는 것 입니다. 그래서 이 떨어진 거리를 $a_i$라고 설정하게 됩니다.그리고 마지막으로 $X_i$ 축은 아까전에 정의할때 $Z_{i-1}$과 $Z_i$에 대해서 둘다 수직한 그러한 방향이라고 설정을 했기 때문에 이 $X_i$에 대해서 얼마만큼 회전이 되느냐?그 각도 회전각도를 $\alpha_i$, $X_i$축에 대해서 $Z_{i-1}$번째 방향에서 $Z_i$방향으로 얼마만큼, $\alpha$만큼 회전이 일어났느냐?그래서 이 네가지 parameter로서 D-H parameter를 설정할 수 있게 되는 것입니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(17).jpg)| 이어서 D-H parameter를 이용을 해서 어떻게 joint간에 그리고 어떻게 frame간에 이 Transformation과 rotation을 만들어 내는지를 한번 살펴보도록 하겠습니다.네 가지 parameter정의를 했습니다. 여러분들이 먼저 거꾸로 온다고 생각을 해봅시다. 거꾸로 즉 $Z_i$축에서부터 $Z_{i-1}$축. 즉, i프레임에서 i-1번째 frame으로 가지고 오는 것입니다. 즉 i-1번째 프레임에서의 표현되는 이 i번째의 frame에 있는 위치를 설명하기 위해서는 첫 번째로 이 $Z_i$축은 $Z_{i-1}$축에 대해서 $\alpha$만큼 rotation이 되어 있는 것이죠.그렇기 때문에 먼저 X축 방향으로 $\alpha$를 X축에 대해 정의를 했었습니다. X축 방향으로 $\alpha$만큼 rotation을 시키고 $X_i$ 축에 대해서 역시 $a_i$ 만큼의 translation했습니다.그렇기 때문에 translation시키는 Matrix를 이용해서 coordinate(좌표)을 통일시켜줍니다. 그리고 그다음에 한 것이 바로 $d_i$만큼을 $Z_{i-1}$축으로 이동을 시켰습니다.이 거리만큼을 그래서 이 이동하는 Transformation Matrix를 계산을 해주고 그리고 마지막으로 Z축을 서로 통일시켜주기 위해서 $\theta_i$만큼을 회전을 시켰습니다.그래서 마지막으로 rotation을 $\theta_i$만큼 회전을 시켜준다면 이 순서대로 거꾸로 rotation translation translation rotation.이 두개 사이의 위치는 바뀌어도 됩니다. 즉, rotation을 먼저하고 translation을 해줘도 됩니다.거꾸로 translation을 해주고 rotation을 먼저 해줘도 관계는 없습니다.이렇게 네 개의 ‘Homogeneous Transformation’을 활용을 해서 전부다 연산을 취하면 우리는 바로 D-H Matrix라고 하는 이 A Matrix를 찾을 수 있게 되는 것입니다.그래서 이 각각의 Rotation Matrix와 translation Matrix를 전부 다 이 값들 $\alpha$, $a$, $d$, $\theta$를 집어 넣어서 계산을 하게 되면아래와 같이 이런 ‘Homogeneous Transformation’을 얻을 수 있는데 이렇게 생긴 ‘Homogeneous Transformation’를1Denavit-Hartenberg transformation 이라고 하게 되는 것 입니다.이렇게 되면 요 앞에 있는 ‘Homogeneous Transformation’에서 상위, 이 위에 있는 3 by 3 Matrix는 Rotation을 의미한다라고 했는데요.즉 $Z_{i-1}$번째 축에서 $Z_i$축을 표현하기 위해서 얼마만큼 rotation되어 있느냐는 결국에는 이 $\theta$와 $\alpha$로써 rotation을 설명할 수 있게 되는 것이고요.마찬가지로 이 두축 사이에 떨어진 거리. 두 축 사이의 원점 간의 떨어진 거리는 이쪽에 있는 translation하게 되는 벡터로써 표현이 되게 자동적으로 이러한 ‘Homogeneous Transformation’를 D-H parameter를 이용해서 구성할 수 있게 됩니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(18).jpg)| 그렇다면 앞서서 Z축을 잡는데 있어서 Revolute joint와 Prismatic joint에 대해서 두가지 조인트가 존재한다고 여러분께 말씀을 드렸는데이제 Revolute joint 같은 경우에는 결과적으로는 $\theta$가 앞서서 4개의 파라미터 중에서 $\theta$가 Variable. 변하는 인자가 되겠습니다.나머지 $d$, $a$, $\alpha같은 경우에는 Constant, 즉 한번 로봇이 만들어지면 이 값들은 변하지 않고 그대로 자기 값을 유지하는 Constant값이 되고요. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(19).jpg)| 마찬가지로 Prismatic joint, 이번에는 Z축 방향으로 $d$가 움직이죠?$d$가 움직이기 때문에 $d$가 Variable이 되겠고, 마찬가지로 $θ_i$, $a_i$, $\alpha_i$가 Constant값을 갖는 그러한 변수로 여러분들이 이해하시고 적용하시면 되겠습니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-3_(20).jpg)| 앞서서 말씀드렸던걸 요약을 하자면 결국은 Revolute joint에 대해서는 $a$, $d$, $\alpha$라는 값이 고정 값이 되겠고 변하는 값은 회전하는 $\theta$의 값이 되겠습니다.그리고 Prismatic joint는 $a$, $\theta$, $\alpha$가 역시 Constant값이 되겠고, 그리고 $d_i$ 즉, Prismatic joint의 진행방향으로의 그 값이 변하는 값이 되겠습니다.그래서 총 각각 어떠한 joint 이던지 간에 3개의 parameter는 고정되어 있고 하나의 parameter만 변하는 그러한 구조를 띄고 있다고 생각하시면 되겠습니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 4주차 4-2 2D-Example]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-09-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-4-Weak-4-2%2F</url>
    <content type="text"><![CDATA[Transform joint coordinates to end-effector coordinates|![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-2.jpg)| 위에 보이는 평면상에 3개의 joint와 3개의 link로 구성되어 있는 이러한 평면 3자유도 로봇이 있습니다. 평면 3자유도 로봇이라고 하는 것은 3개의 관절로 이루어져 있기 때문에 3자유도 로봇이 됩니다. 또 3개의 자유도를 가지고 있는 끝점(end-effector)의 위치와 자세까지를 3자유도로 표현이 가능합니다. 관절공간의 자유도 3자유도 그리고 작업공간의 자유도 3자유도로 매칭을 시켜보도록 하겠습니다. 가장 기본적으로 사용하는 수학적인 표현법. ‘$sin$, $cos$과 관절의 길이($l$)’를 바탕으로 식을 써보면 옆과 같이 표현 됩니다. 즉, $x$는 $x$의 위치, 이 로봇의 끝점에서의 $x$의 위치를 관절공간에서의 자유도인 $\theta_1$, $\theta_2$, $\theta_3$를 이용해서 이 점의 끝을 기술해 볼텐데요. $sin$, $cos$을 활용해서 $l_1cos\theta_1 + l_2 cos(\theta_1 + \theta_2) + l_3cos(\theta_1+\theta_2+\theta_3)$으로 $x$점의 좌표가 위의 식처럼 표현된다는 것을 확인할 수가 있습니다. 마찬가지로 $y$에 대해서는 $sin$방법을 이용해서 $y$의 값들을 구할 수 있습니다.그리고 끝점에서의 로봇이 위치하고 있는 각도는 결국에는 $\theta_1$과 $\theta_2$, $\theta_3$의 각의 합에 의해서 결과적으로 로봇의 끝점, 끝 방향 이 결정되게 됩니다. 그래서 결론적으로 이 $x$, $y$, $\phi$라고 하는 3개의 공간상의 좌표는 옆에 보 이는 식처럼 $\theta_1$, $\theta_2$, $\theta_3$ 이 세가지 관절공간의 좌표에 의해서 관절공 간의 자유도의 그리고 값들에 의해서 표현이 가능해 지는 것입니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-2_(2).jpg)| 그렇다면 이렇게 표현된 방법을 새로운 로봇이 주어진다면 여러분들은 새롭게 주어진 로봇들을 역시 마찬가지로 바로 이전 표현법처럼 각각의 끝점이 어떻게 위치하는지는 $sin$, $cos$ 대부분 아마 $sin$, $cos$으로 표현이 가능합니다. 그래서 $sin$, $cos$을 이용해서 어느점이 어떻게 매칭이 되는지를 식을 구하면 됩니다. 그런데 새로운 로봇을 만들때마다 그걸 전부다 일일이 손으로 푼다는 것은 사실상 쉽지 않은 일입니다. 그래서 이러한 위치 표현법들을 조금 systematic하게 수행하기 위해서 추후에 D-H parameter를 통해서 자동화하는 기법을 살펴볼 예정입니다. 그 전에 앞서서 배웠던 내용들을 바탕으로 어떻게 이러한 이 좌표, 이 끝점에 대한 표현을 ‘좌표변환’이란걸 통해서 그리고 ‘Homogeneous Transformation’을 통해서 표현할 수 있는지에 대해서 살펴보도록 하겠습니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-2_(3).jpg)| 다시 앞서 바로 설명해드렸던 예제인 3 link manipulator 평면 3자유도 manipulator를 가지고 설명을 드리도록 하겠습니다.각각의 길이는 $l_1$, $l_2$, $l_3$로 동일합니다. 그리고 ‘Homogeneous Transformation’ 을 사용하기 위해선 좌표계를 설정해보도록 하겠습니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-2_(4).jpg)| 위에 그림에서처럼 전체의 manipulator가 고정되어 있는 이 바닥점을 $x_0$와$y_0$, 그리고 $link_1$의 시작점을 $x_1$와$y_1$,$link_2$의 시작점을 $x_2$와$y_2$, 그리고 $link_3$의 시작점을 $x_3$와$y_3$. 이렇게 좌표계를 설정 합니다. 원하는 이 로봇의 끝점을 위에 있는 그림에서 이 노란색점을 끝점의 위치라고 한다면 위치를 정확하게 표현하기 위해서 앞서 배운 Homogeneous Transformation을 통해서 설명을 해본다면 이 끝 점의 위치는 결과적으로 세 번째 좌표계에서 $l_3$즉, $x$방향으로는 $l_3$위치에 존재하고, $y$방향으로는 0, 그다음에 평면이기 때문에 $z$방향, $z$축 방향으로 0의 위치에 있을 것입니다. 이렇게 $l_3$, 0, 0을 옆에 보이는 식처럼 Homogeneous Transformation H를 찾기 위해서 $l_3$, 0, 0, 1이라는 값을 통해서 결과적으로는 ‘0번 좌표계에서 어떻게 $x$, $y$, $z$로 표현되느냐?’라는 것을 찾고 싶은 것입니다.그러기 위해서 전주 차에 배웠던 내용을 바탕으로 하나씩 적용을 해보도록 합시다.먼저, 이 노란점을 가지고 두 번째 좌표계로 가져오기 위해서 두 번째 좌표계와 각도를 일치시킬 필요가 있습니다.그리고 두 번째 좌표계와 세 번째 좌표계의 틀어진 각도만큼 회전시킨다라고 볼 수도 있는 것이죠.그래서 마지막 이 위에 있는 보이는 식처럼 마지막에 있는 $R_z$축 방향으로 $\theta_3$만큼 회전을 하면 결국 이 세 번째 축에 있는 노란색점을 두 번째 축에 있는 두 번째 좌표계의 점으로써 표현을 할 수 있습니다.그렇게 해서 먼저 $R_z$를 통해서 거꾸로 이 마지막에 있는 $R_z$를 통해서 $\theta_3$만큼 회전을 시키게 되겠고요. 그 다음에 그 회전시켰기 때문에 이제는 두 번째 좌표계에서 표현이 되고 있습니다.두 번째 좌표계에서 표현이 되고 있는데 이 좌표축 자체가 $l_2$만큼. 이 link, 두 번째 link 길이의 $l_2$만큼 떨어져 있는 것이죠.9래서 그 $l_2$의 길이만큼 당겨올 필요가 있죠. 그래서 이 두개의 좌표축을 이동시키기 위해서, 다시 말해 이 두가지 좌표계를 일치시키기 위해서 $l_2$만큼 떨어져 있는 것을 translation 시키는 ‘Homogeneous Transformation’을 이용해서 가지고 올 수 있습니다. 다시, 이제는 두번째 축에서 정확하게 표현이 되고 있으니깐 이걸 다시 첫 번째 축으로 가져 오기 위해서역시 마찬가지로 rotation, 즉 두 번째 축을 첫 번째 축하고 얼마나 틀어져 있는지를 살펴봐서 즉, $\theta_2$만큼 틀어져 있기 때문에 $\theta_2$만큼 회전을 시키고 다시 $link_1$의 길이만큼 가지고 오고 마지막으로 $\theta_1$, 즉, zero좌표계 이전에 정의한 기존 base frame과 첫 번째 frame, 두개의 기존 좌표계가 틀어져 있는 $\theta$만큼 회전을 시키면 이 노란색 점이 결과적으로는 $x_0$, $y_0$ 상의 어디에 있는 점인지 이러한 관계식을 통해서 살펴볼 수가 있게 됩니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-2_(5).jpg)| 어떻게 보면 앞서 나왔던 $x$, $y$, $z$ 각각의 점들을 표현하는 방법들이 더 쉬워 보일 수도 있는데, 조금 복잡해 보일 수 있지만 이러한 방법을 쓰는 이유는 앞으로 자동화하고어떠한 그 로봇이 설계 되더라도 여러분들은 systematic하게 표현할 수 있는 방법을 배우기 위해서 이렇게 복잡하지만 체계적으로 살펴보았습니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-2_(6).jpg)| 이렇게 살펴본 것들을 조금 다른 측면에서 본다면 이번에는 그 노란색 점이 그 세 번째 좌표축의 끝점에 있다라고 생각할 것이 아니라그 마지막 끝점에다가도 역시 좌표계를, 즉 네 번째 좌표계를 붙인다 라고도 한번 생각해 볼 수가 있겠습니다.그렇게 네 번째 좌표계를 붙이면 이 노란색 점은 네 번째 좌표계의 원점이 되는 것이죠. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-2_(7).jpg)| 그래서 위에 보이는 식처럼 이번에는 ‘Homogeneous Transformation’상에서 0, 0, 0, 즉, 원점을 위하는 네 번째 좌표계의 원점을 의미하는 0, 0, 0 이라는 값으로서0번째 기준 frame에 좌표계로 가져오는 이런 ‘Homogeneous Transformation’을 만들게 될텐데요.그러면 새롭게 이렇게 좌표계를 추가함으로써 추가적으로 들어가는 식은 앞서 식에서 여기 마지막 식에서 보이는 것처럼 $l_3$만큼을 translation 시키는 즉, 좌표계 네 번째 좌표계와 세 번째 좌표계가 얼마만큼 떨어져 있느냐 라는 것을 표현하는 $T_{x_3}$는 $l_3$라고 하는 마지막 ‘Homogeneous Transformation’만 하나 더 추가를 하면 되는 것이죠.이렇게 추가함으로써 왜 또 한번 더 복잡하게 만드는 것인가라는 의문이 들 수 있는데 이렇게 추가함으로써 이 식을 거꾸로보시면 $TR$ $TR$ $TR$ 이렇게 각각 두개씩 묶어서즉, translation, rotation, translation, rotation, translation, rotation과 같은 규칙성을 가지고하나의 좌표축에서 좌표축으로 좌표축에서 좌표축으로의 변환을 systematic하게 표현하기 위한 기본적인 단계를 완성할 수가 있게 되는 것입니다. |![alt](/images/K-MOOC/RMaURA/K-MOOC_RMaURA4-2_(8).jpg)|]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 4주차 4-1 Introduction]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-08-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-4-Weak-4-1%2F</url>
    <content type="text"><![CDATA[Kinematics|우리는 주로 기구학에서 정기구학, 역기구학, 이 두 가지 topics을 다룰 예정 입니다.| Foward Kinematics 먼저 Forward Kinematics라고 하는 것은 만약에 로봇을 설계했다라고 하면, 설계된 로봇의 각각의 Link에 대한 길이를 가지고 있을 것이고 그 각각의 Link들을 joint 즉, 모터를 활용한 joint를 통해서 두 개의 Link를 연결을 했을 것입니다.그리고 그 각각의 조인트(joint)는 역시 앞서 설명 드렸던 것처럼, encoder라던지 위치를 측정할 수 있는 Sensor들을 바탕으로 각도를 측정할 수 있습니다. 이렇게 준비되었다면, 이러한 정보들을 바탕으로 로봇의 Link와 Joint에서 나오는 각도 정보를 바탕으로 각각의 각도들을 이용해서마지막 끝점(end-effector)의 위치를 어떻게 찾아낼 수 있느냐. 하는 부분들을 ‘Forward Kinematics’라고 얘기를 하는 것입니다. Inverse Kinematics 이와 반대로 ‘Inverse Kinematics’라고 하는 것은 끝점의 위치를 고려했을때, ‘로봇의 끝점의 위치를 이렇게 이동하겠다.’ ‘직선 방향으로 이동하겠다.’ 라고 하는 공간상에서의 위치 점들이 있을 것입니다. 이런 위치 점들에 가기 위해서는 각각의 관절들의 각도들을 어떻게 꺾어 줘야지만 이 위치로 갈 수 있는지라는 이 문제의 Solution을 찾아내는 방법을 역 기구학 또는 Inverse Kinematics라고 부릅니다. Basic Problems in Robotic Kinematics 그렇다면 앞서 설명 드렸던 내용들을 그림으로 살펴보시면 이렇게 각각의 각도들 $\theta_1$, $\theta_2$, $\theta_3$, $\theta_4$, $\theta_n$까지 이러한 $\theta$에 대한 정보들을 각각의 로봇이 가지고 있는 joint에서의 각도, joint 각도를 순서대로 아래쪽에서부터 연결을 해서 그것들을 표현한 것을 각도 $\theta$에 대한 벡터라고 표현할 수 있습니다. 그리고 끝점에서의 위치는 $x$, $y$, $z$만이 될 수도 있을 거고 또는 $x$, $y$ ,$z$와 각각의 Roll pitch yaw 각도, 아니면 특정한 오일러각도 아니면 여러분들이 앞서 배운 각도 표현법에 따른 그러한 각도를 포함하는 위치 백터인 $X$에 대해 이렇게 표현할 수가 있습니다. 그래서 정리하자면, 이런 $\theta$가 주어졌을 때, $X$를 찾는 문제를 ‘정 기구학’ 또는 ‘Forward kinematics’라고 얘기합니다. 그리고 $X$가 주어졌을 때, $\theta$를 찾는 것을 ‘Inverse Kinematics’라고 얘기를 하고 “역 기구학을 찾는다.”라고 표현을 하고 있습니다. |그리고 그 역인 $X$가 주어 졌을때, $\theta$를 구하는 문제를 또다시 위치 수준과 속도 수준으로 나눠서 생각을 해 보실 수 있습니다.| 위치 수준에서는 우리가 앞서서 구했던 ‘homogeneous transformation’을 통해서 어떻게 이 $\theta$가 주어졌을 때, 어떻게 $X$가 나오게 되는지에 대한 수식을 살펴볼 텐데요. 이렇게 나오는 수식은 기본적으로 ‘Non-Linear 하다.’라는 성질을 가지고 있습니다.여러분들이 ‘Non-Linear하다.’라고 하는 개념에 대해 생각을 해보게 된다면 어떠한 $\theta$값 1을 줬을 때, 나오는 $X_1$와 $\theta$값 2를 줬을 때 나오는 $X_2$ 이 두 개의 값이 더해서 새로운 앞에서 말씀드렸던 $X_1$과 $X_2$가 $\theta_1+\theta_2$에 들어갔을 때, 나오는 값과는 다르다는 그러한 개념이 보통 ‘비선형’ ‘Non-Linear하다.’라고 얘기를 합니다.이러한, Non-Linear 문제는 생각보다 풀기 쉽지 않고, ‘각각의 Solution들을 따로따로 찾아내야한다’라는 문제점들을 가지고 있습니다. 그에 반해서 속도 수준에서는 추후에 ‘Jacobian’이라는 것에 대해서 공부를 하게 될 텐데 이 ‘Jacobian’에 의해서 속도 즉, 관절을 움직이는 속도와 그리고 그 관절 속도에 의해서 나타나는 끝점에 속도는 ‘Jacobian’이라고 하는 그런 matrix를 통해서 선형 관계식으로 만들어낼 수가 있습니다. 그래서 위치 관계식과 속도 관계식에서, 위치 관계식은 비선형이고 속도 관계식은 선형 이라는 그러한 장점을 가지고 있습니다. 그렇지만 속도 관계식을 이용을 했을 때는 원하는 것은 각각의 위치 점을 찾아가는 문제이기 때문에 역시 또 속도 관계식만 가지고 쓰기에는 좀 한계가 있습니다. 이어서 ‘Degree of freedom’에 대한 설명을 잠깐 드리겠습니다. ‘Degree of freedom’이라고 하는 것은 어떠한 움직임을 만들어 내기 위해서 필요한 변수들의 집합이라고 보면 되겠습니다.자유도 라고 불리는 것에 대해서 여러분들은 두 가지 측면에서 접근해 볼 필요가 있습니다.자유도 라고 하는 것은 물체를 완벽하게 어떻게 위치하고 있고 자세를 취하고 있는지를 기술할 수 있는그러기 위해서 필요한 $x$, $y$, $z$와 같은 위치, 그리고 Roll, pitch, yaw와 같은 각도, 이런 정보들의 집합을 자유도 라고 합니다. 만약에 공간상에 떠다니는 비행기를 생각해 본다면 비행기의 기준점으로 부터의 위치 즉, $x$, $y$, $z$라는 공간상에서의 세 개의 좌표로 표현되는 점이 있을 거고요. 그리고 이 비행기가 취하는 자세에 따라서 우리는 정확하게 이 비행기를 설명하기 위해서는 여섯 개의 좌표가 필요하게 됩니다. 그렇기 때문에 우리는 이 비행기를 ‘6 자유도 운동을 하는 물체다.’라고 설명을 할 수가 있습니다. 그리고 평면상에서 움직이는 자동차의 경우에는 평면상에서의 자동차는 $x$, $y$, 하고 자동차의 방향, 이 세 가지를 가지고 자동차를 정확하게 ‘어디에 있다.’ 라고 기술할 수 있기 때문에 ‘자동차는 3 자유도를 가지고 있다.’라고 할 수 있겠습니다. 이렇게 공간상에서의 자유도라고 하는 것은 공간상에 자유도는 이렇게 위치에 대한 Translation에 대한 자유도를 보통 3차원이면 3개, 2차원이면 2개를 가지게 됩니다. 그리고 자세에 대한 자유도는 2차원이면 위에 보이는 식처럼 이 Rotation에 대한 자유도 2 dimensions이면, d 에다가 2를 대입해서 계산을 해 보면 1이라는 자유도 즉, 방향각에 대한 자유도만 나오게 됩니다. 3차원이 되면 3개의 자유도 ‘Roll pitch yaw’와 같은 ‘3개의 오일러 각’에 의한 자유도를 나타내게 됩니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 3주차 3-5 Homogeneous Transformation]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-07-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-3-Weak-3-5%2F</url>
    <content type="text"><![CDATA[Homogeneous Transformation Homogeneous Transformation이라고 하는 것은 어떠한 2개의 좌표계 사이의 2번째 좌표계에 있는 한 점, $P$라는 그 점을 1번째 좌표계에 있는 점으로서 1번째 좌표계에서 표현하는 방법을 나타낼 수 있습니다. 위의 그림에서 보이는 것처럼, B라는 프레임에서의 한 점 P가 존재하는 데 이 B라는 프레임에서 점 P라는 부분을 A라는 프레임에서 살펴보고 싶다는 것이죠. 예를 들면 여러분들은 지구에 있고 지구에서 달에 있는 한 점의 위치를 알고 싶을 때, 그 점에 대해 정확히 알기 위해서는 여러분들은 지금 여기 서 있는 지점에서 달의 중심까지의 거리(좌표)와 그 달에서 점까지 위치, 그 2개의 벡터를 가지고 설명한다면 조금 더 쉽게 그 점을 표현할 수 있을 겁니다. 마찬가지로, 어떠한 B라는 프레임 위의 점을 설명하기 위해서 B라는 프레임에서의 점을 A라는 프레임에서 어떻게 설명되는지 프레임을 돌리는 로테이션시켜서 A라는 프레임에 대해서 설명하는 방법, 그 B라는 프레임에서의 P점을 A로 가지고 오는 로테이션 매트릭스즉 로테이션 매트릭스 B from A 라고 하는 $ARB$라는 것을 통해서 이 B라는 프레임에서의 P점의 위치를 설명할 수 있습니다. 그 위치를 설명한 내용에다 추가를 해서 B라는 프레임에서의 기준점 그 원점을 A라는 프레임에서 설명하는, 즉 B의 origin을 A에 대해서 설명한 위치 이 2가지로 설명할 수 있습니다. 이렇게 설명하는 것이 바로 위의 식처럼 A라는 프레임에서의 P점은 B라는 프레임에서의 P점을 가지고 옮겨오는 이러한 수식으로서 표현이 가능합니다. 이렇게 설명되어있는 것을 4차원으로 확장해보도록 하겠습니다. 먼저, 이 Position vector에다가는 1이라고 하는 이 숫자를 더함으로서 원래 Position vector는 3X1벡터가 되는데 이를 4X1의 벡터의 형태로 바꾸게 됩니다.그 다음에, Transposition Matrix라는 부분을 4 by 4 매트릭스로 확장하게 되는데, 이 4 by 4 매트릭스 같은 경우에는 앞의 3 by 3 부분은 Rotation Matrix를 배치하고 그 옆의 3 by 1벡터는 B의 origin에서부터 A까지, 즉 A프레임에 대한 B origin의 좌표를 배치하고 아랫쪽에는 [0,0,0,1]이라는 4가지 숫자를 배치함으로서, 4 by 4 매트릭스로 확장하게 됩니다. 여러분이 이 식을 매트릭스의 연산을 배웠기 때문에 이 매트릭스의 연산을 계산해보면, 즉 $A^p$, A프레임에 대한 P점은 Rotation MatrixP, B프레임에 대한 P와, P점인데, 이 P점은 A프레임에 대한 origin1을 하기 때문에 결과적으로, 위에 표현되는 식은 앞서서 설명했던 이 위에 있는 식, 즉 P가 A에서 어떻게 기술되는지를, 즉 B의 origin과 Rotation Matrix를 통해서 설명한 이 식과 정확하게 일치하는 것을 확인할 수 있습니다. 위에 있는 식은 분명히 Rotation Matrix에 $p$라는 포지션 벡터를 곱하고 그에 다시 origin이라는 position vector를 더해주어 얻어지는 즉 곱하기와 더하기로 이루어진 연산이었는데밑의 식은 이러한 4 by 4 매트릭스로 확장하는 하나의 매트릭스에 벡터 하나를 곱하기 한번의 연산으로 바꿔주기 때문에 내부적으로 똑같지만 우리의 생각은 단순해질 수 있죠. 수식적으로는 똑같지만 생각에 있어서는 곱하고 더하는 과정을 단순하게 하나의 곱하기로 바꾸어서 대체해서 생각할 수 있다는 장점을 가질 수 있기 때문에이렇게 Homogeneous Transformation이라고 부르는 것을 4 by 4 매트릭스를 도입하게 된 것입니다. 더 나아가서, 이러한 Homogeneous Transformation의 의미를 좀 더 살펴볼 필요가 있는데 이 Homogeneous Transformation을 T라고 표현할 때 위의 3 by 3, 일반적으로 Rotation Matrix, 두 좌표계 상의 로테이션을 각도를 맞춰주는 로테이션을 의미하고그 옆에 있는 Translation Vector 같은 경우에는 두 좌표계 사이의 원점 사이의 거리 Vector를 의미하게 됩니다. 추가적으로, 밑에는 아까 [0,0,0,1]이라고 말씀드렸는데, 로보틱스나 일반적인 트랜스포메이션 매트릭스에서는 항상 밑에는 [0,0,0,1]이라는 개념을 활용해서 쓸 텐데이것들이 꼭 [0,0,0,1]로만 고정되는 것은 아닙니다. 컴퓨터 그래픽스나 캐드 툴 등에서 사용할 때, 한쪽으로 확장시킨다거나 Zoom out/In을 할 때에는바로 이러한 밑의 Perspective Translation과 scaling vector 등을 활용해서, zooming이나 한쪽으로만 비대칭적으로 더 확장할 때,이러한 개념으로서의 확장을 할 수 있는 여지도 밑의 4개의 항들로 구현이 가능하기 때문에 Homogeneous Transformation의 매트릭스, T는 굉장히 다양한 분야에서 널리 쓰일 수 있습니다. 그렇지만, 이 로보틱스 분야와 기구학 분야에서는 Perspective Translation에서는 [0,0,0]로, Scaling Factor는 1로 고정하는 그런 매트릭스로 활용할 예정입니다. 추가적으로, Homogeneous Transformation을 앞서서 살펴본 것처럼 이렇게 3 by 1 벡터는 3 by 3 매트릭스*3 by 1 벡터를 하고 3 by 1 벡터를 더해주는 과정을, 4 by 1 백터는 4 by 4 매트릭스에 4 by 1 벡터를 곱해주는이러한 Homogeneous Transformation으로 바꿀 수 있는 방법에 대해서 말씀드렸습니다. 이렇게 A라는 프레임과 B라는 프레임이 있고 B라는 프레임의 한 점 [0,1,1]이라는 점이 있고 이 B라는 프레임은 A와 비교해서 로테이션이 이루어져 있습니다. 그리고 B라는 프레임에서의 원점은 A라는 프레임에 대해서 [0,3,1]만큼 떨어져 있습니다. 그렇다면, B라는 프레임에 있는 [0,1,1]이라는 점을, Homogeneous Transformation에서 Rotation Matrix를 먼저 정의할 필요가 있는데 여기서는 여러분들이 B라는 프레임에서의 XYZ, 즉 $X_B$, $Y_B$, $Z_B$를 A라는 프레임에서, 표현하게 되면, [1,0,0], [0,0,1], [0,-1,0]이렇게 3 by 3 로테이션 매트릭스로 표현 가능하고 떨어진 거리는 [0,3,1]의 거리만큼 떨어져 있음을 알 수 있기 때문에이와 같이, Homogeneous Transformation Matrix로 정의할 수 있고 두개를 곱했을 때, 바로 [0,2,2]로 앞서서 말씀드렸던 것처럼 뭔가 2개의 조합이 한가지로 이루어졌는데, 바로 이렇게 곱해버리니까, [0,2,2] 물론 밑에 1이라는 더미 텀이 있지만 이 더미 텀을 빼버리면손쉽게, [0,1,1]이라는 B프레임에서의 점을 [0,2,2]라는 A프레임의 점으로 A프레임으로 충분히 쉽게 Homogeneous Transformation을 통해서 표현할 수 있습니다. 그렇다면, Homogeneous Transformation을 앞에서 살펴본 것처럼, 이렇게 2가지의 곱하기와 더하기로 이루어진 연산을 하나의 곱하기로 바꿔서 표현할 수 있다는 장점이 있다고 말씀드렸는데그러한 장점만이 있는 게 아니라 좀 더 살펴보게 되면, 즉 한번의 Transformation 한번의 변환만을 할 경우에는 그다지 큰 장점이 없을 수도 있습니다. 그렇지만 위의 식처럼 C에서부터 B로 오고 B에서부터 A로가는 즉, C프레임에 있는 한 점을, B프레임으로 옮기고, 다시 A프레임으로 옮겨오는,이러한 2가지의 수식을 가지고 설명하는 것을 한번 진행해보도록 하겠습니다. 이 2개의 연산을 연속적으로 진행하게 되면, 즉 어떤 P라는 점을 A라는 프레임에서 설명하기 위해서 우리는 바로 이 A라는 프레임, B라는 프레임, C라는 프레임을 하나하나 거쳐서 이와 같은 조금은 복잡해보이는 수식으로 표현을 해야 합니다. 그렇지만 이 수식을 Homogeneous Transformation으로 표현한다면 C라는 프레임의 P는 바로 Homogeneous Transformation C에서부터 B까지 온 것을 곱해주고 그 곱한 것을 다시 B에서 A까지 가져오는 Homogeneous Transformation의 곱으로 곱해주면 바로 A라는 프레임으로 넘어오게 되죠.앞서서 본 위의 이 수식 즉, 그냥 어떤 Rotation Matrix와 Origin의 어떤 거리 관계식 이런 것들로 설명한 식보다 훨씬 간단해진다는 것을 알 수 있습니다. 수식적으로는 동일한 효과를 얻으면서 생각은 훨씬 더 쉽게할 수 있습니다. 바로 이 Homogeneous Transformation 하나만으로 얼마든지 한번이건 두번이건 세번이건, 연속적으로 진행될때마다 각각을 곱해주기만 하면 바로 이 복잡한 연산이 단순하게 바뀔 수 있다는 장점도 가지고 있습니다. Inverse Transformation 일반적으로 Rotation Matrix같은 경우에는 앞서서, SO(3)에 포함되어 있기 때문에 자기 자신의 역행렬은 Transpose하고 일치한다,라고 설명한 바 있습니다. 그렇지만 Homogeneous Transformation같은 경우에는 확장을 했기 때문에 Homogeneous Transformation의 Inverse는 Homogeneous Transformation를 Transpose한 것과 완전히 같지는 않습니다.특별히 그런 부분을 주의하기 위해서, Homogeneous Transformation의 Inverse를 다음과 같이 정의합니다. 즉, 앞서서 로테이션 매트릭스 자체는, 트랜스포즈 자체가 역행렬을 의미하기 때문에, 로테이션의 자리에는 바로 로테이션 매트릭스의 트랜스포즈 형태로 배치할 수 있고 원래 이 위치에는 B에서부터 A 즉, A에 대해서 B의 origin이 어떻게 표현되는지를 설명해야 하는데 역행렬이니까, 반대로 B라는 프레임에 대해서 A Origin의 위치를 써 넣어줘야 하는 것입니다. 그럼 어떻게 해야 하느냐, 바로 Inverse Transformation을 하게 된다면 즉 Rotation을 즉, B라는 프레임에 대해서 A라는 프레임으로 가져가게 된다면 Transpose한다면 역행렬이 된다고 했습니다. 그 역행렬에다가 B라는 origin을 A로 바꾸는 것을 곱해줌으로서 -를 곱해줌으로서 A origin을 B프레임에 대해서 설명할 수 있는 이러한 식이 완성되게 됩니다. 그래서, 이 두 식을 여러분들이 Homogeneous Transformation Matrix, 즉 B에서부터 A까지 오는 Homogeneous Transformation의 Inverse는 이와 같이 정의되고 이 정의된 것을 A에서 B까지 오는 Homogeneous Transformation이라 말할 수도 있습니다. 그리고 그 2개를 같이 곱했을 때, 당연히 역행렬이기 때문에, Identity Matrix를 만족해야합니다.수식으로 계산해보면 쉽게 Identity Matrix가 된다는 것을 확인할 수 있게 됩니다. 여기까지 살펴봄으로서, 여러분들은 Homogeneous Transformation이 어떻게 구성되고 어떤 장점을 갖고 있는지 이 Homogeneous Transformation의 Inverse는 어떻게 계산하는지를 살펴보았습니다]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 3주차 3-4 Euler Angles(2)]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-07-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-3-Weak-3-4%2F</url>
    <content type="text"><![CDATA[Euler’s Theorem $\lambda$라는 축을 찾았다고 가정해보자, 이 축에 대해 완벽하게 한번만 회전한다면 회전을 이룰 수 있다 라고 했을 때, 이 $\lambda$를 우리가 가지고 있는 $z$축과 일치시키면 그 $z$축에 대해서 마지막에서 회전을 하고 다시 일치시킨 것을 거꾸로 되돌아온다면 그 $\lambda$라는 축에 대해서 $\delta$만큼의 회전을 충분히 쉽게 표현할 수 있을 겁니다. 이 방식을 오일러가 제안한 방법대로 위에 있는 그림에서 1번으로 먼저 $x$축에 대해서 $\alpha$만큼 회전하고 다시 2번째로 $y$축에 대해서는 $-\beta$, 즉 돌리는 방향이 반대입니다.그렇기 때문에, $y$축에 대해서 $-\beta$로 가지고 오게 되면 $\lambda$가 $z$축과 일치하도록 위치시킬 수 있게 됩니다. 이 상태에서 이 $z$축에 대해서 $\delta$만큼 회전을 하는 것은 $\lambda$에 대해서 $\lambda$만큼 회전하는 것과 같은 표현법이 되겠습니다. 이렇게 표현해서 바꿔서 회전한 다음에 다시 원래대로 거꾸로 돌아갑니다. $y$축에 대해서, 아까는 $-\beta$만큼 회전했기 때문에, 이번에는 $+\beta$만큼 회전하고 다시 $x$축에 대해서 $-\alpha$만큼 회전하게 된다면 람다에 대해서 $\delta$만큼 회전하는 것을 이렇게 5번의 로테이션으로 5개의 로테이션으로 나누어서 2 방법의 일치성을 찾아볼 수 있게 됩니다. 이렇게 5개의 회전을 위에 보이는 수식처럼 차례대로 기술해보았습니다. $x$축에 대한 $\alpha$만큼의 회전 다시 $y$축의 $-\beta$만큼 회전 $z$축에 대해서 $\delta$만큼 회전, $y$축에 대해서 $\beta$만큼 회전 다시 $x$축에 대해서 $–\alpha$만큼 회전 이런 식으로 5가지 로테이션 매트릭스로 오일러 정리라고 불리는, $\lambda$에 대해서 $\delta$만큼의 회전을 이렇게 표현해보았습니다. 조금 수식이 복잡하기 때문에 약간씩 약어를 써서 $v\delta$라고 하는 것은, $1-cos\delta$라는 것으로 표현했고 이런 식으로 5가지 매트릭스를 1개의 식으로 썼을 때 이렇게 매트릭스로 으;에 보이는 매트릭스대로 나오는 것을 확인할 수 있습니다. Rodrigues’formula 이 복잡해 보이는 것을 Identity Matrix와 그리고 skew-symmetric matrix, 그리고 $\delta$, $sin\delta$, $cos\delta$의 값을 이용해서 위와 같은 값으로 나타낼 수 있게 됩니다. 이렇게 식으로 나타내는 것을, 바로 Rodrigues’formula라고 말합니다. 이렇게 오일러 정리를 나타내는 매트릭스의 조합을 이와 같은 수식으로 표현할 수 있습니다.이렇게 Rodrigues’formula가 주어지고 여기에서 $\lambda$가, 아까 말했던 Euler axis가 되겠고,그리고 $\lambda$의 옆의 곱하기 표현된 것이 매트릭스에서 살펴본 skew-symmetric matrix를 만든 것과 같은 식입니다.그렇게 $\lambda$ 옆에 곱하기 표시가 있는 것은 $\lambda$를 이용한 skew-symmetric matrix, cross product를 만들기 위한 skew-symmetric matrix를 표현한 것과 같고$[\lambda X]^2$은, 계산해 보면, $\lambda\lambda^T-I_{3X3}$ 라고 하는 것으로 얻어진다는 것을 확인할 수 있게 됩니다. 그래서, 최종적으로, Rodrigues’formula를 이렇게 얻어지는 것을, 앞서서의 식으로서 충분히 설명이 가능하다, 단순한 식으로 이렇게 간단하게 표현할 수 있다는 것을 살펴봤습니다. 일단은 우리는 매트릭스가 주어지게 되면 그 매트릭스로부터 알고 싶은 것은 얼마만큼의 회전이 이루어졌는지에 대한 그 회전량, 즉 $\delta$가 되겠고 그 다음에 어느 축에 대해서 회전이 이루어졌는지 그 축에 대한 $\lambda$ 라고 하는 그 축을 얻고 싶을 겁니다. 이랬을 때 이 Rodrigues’formula를 통해서, 3 by 3 매트릭스를 가지고 있었을 때 그 매트릭스의 각 인자들을 각각 $r_{32}$, $r_{23}$, $r_{11}$, 이런 식으로 명명했을 때 그 값들을 이용해서 옆에 보이는 식처럼 $tan\delta$는 이와 같은 식으로 $tan\delta$를 정의할 수 있고 이렇게 얻어진 것을 $arctan\delta$를 취해서 $\delta$를 얻게 됩니다. 역시 이렇게 얻어진 $\delta$를 활용해서 옆에 보이는 $\lambda$를 구하는 식즉 $1 \above 1pt 2sin\delta$을 곱한 다음 $r_{32}$에서 $r_{23}$을 빼고, $r_{13}$에서 $r_{31}$을 빼고, 이런 식으로 각각의 파라미터 값을 가져와서 빼게 되면 $\delta$와 $\lambda$를 얻을 수 있게 됩니다. 이러한 Rodrigues’formula에서 대표적으로 $z$축에 대한 회전도 역시 Euler’s Theorem 에서 어떤 2가지의 두 프레임은 무조건 어떠한 이 회전하는 축과 그 회전하는 축에 대한 일정한 각도, 이 2가지로 표현이 가능하다라고 했는데 이 2가지 표현을 $z$축에 대한 로테이션, 이 역시 마찬가지로 성립해야 하기 때문에 이와 같이, $z$축에 대한 $\theta$만큼의 회전 매트릭스, 즉 $cos\theta$, $-sin\theta$, 0, $sin\theta$, $cos\theta$, 0, 0,0,1 로 되어 있는 이러한 $z$축에 대한 $\theta$만큼의 회전 매트릭스에서 먼저 $tan\delta$를 뽑아내게 되면 앞서서의 formula를 그대로 사용하게 되면 $2sin\theta \above 1pt 2cos\theta$, 즉 $tan\theta$가 얻어지게 되고 이 $\delta$와 $\theta$가 같다, 즉 $\delta$가 $\theta$가 된다는 것을 확인할 수 있습니다. $\lambda$는 $ 1 above 1pt 2sin\theta$인데 앞서서의 수식을 그대로 적용하게 되면 0,0,$2sin\theta$라는 것을 얻을 수 있게 됩니다. 그래서, 결과적으로는 0,0,1, $z$축에 대해서, $z$축은 0,0,1로 표현됩니다 $z$축에 대한 $\theta$만큼의 회전으로서 Rodrigues’formula가 그대로 성립한다는 것을 예제를 통해 살펴볼 수 있었습니다. 앞서서, 어떤 로테이션 같은 경우에는 ‘commute하지 않다’, 즉 로테이션은 순서에 따라서 어떤 식을 먼저 돌리느냐에 따라서 결과값이 달라진다고 했습니다. 그렇지만, 로테이션 매트릭스가 굉장히 작은 로테이션만큼 이루어졌을 경우에는 그 로테이션끼리는 서로 commute하게 됩니다. 무슨 말인고 하니 만약에 $R_1$로테이션을 $x$에 대해서 $\delta_1, $\delta_1은 굉장히 작은 값이라는 의미입니다.이 $\delta_1$만큼의 회전이 이루어졌을 때 옆에서 보는 것처럼 $x$에 대한 회전에서 $cos $sin이 $\delta$ 값이 굉장히 작을 경우에는 $cos$은 1로 수렴하게 되고, $sin$은 $\delta$로 수렴하게 됩니다.3그래서, 위에 보이는 것처럼, [1,0,0], [0,1,$-\delta_1$], [0,$\delta_1$,1] 이런 형식의 매트릭스를 얻게 됩니다. 마찬가지로, $z$축에 대해서 $\delta_2$ 만큼의 회전을, 역시 $\delta_2$가 굉장히 작다고 했을 경우결과적으로, [1,$-\delta_2$, 0], [$\delta_2$, 1, 0], [0,0,1]이라 하는 이러한 매트릭스를 얻게 됩니다.이랬을 경우에는, $R_1$ 과 $R_2$를 곱했을 때 이 결과값이 $\delta_2$와 $\delta_1$을 곱했을 때, 즉 어떤 식으로 먼저 로테이션을 수행하는지 $x$축으로 먼저 돌리고 나중에 $y$축을 돌리고, $y$축을 먼저 돌리고, $x$축을 나중에 돌리고 이런 굉장히 작은 로테이션일 경우에는 로테이션도 마치 $x$방향, $y$방향, $y$방향, $x$방향, 이런 식으로 Translation, 즉 이동하는 것과 마찬가지로 commute하다는 것을 확인할 수 있습니다. Rodrigues’formula를 통해서, 이것들이 어떻게 증명이 되는지 한변 살펴보도록 하겠습니다.Rodrigues’formula에서 이 formula를 이용해서 나온 $\delta$값이 굉장히 작다고 했을 때 위에 보이는 식처럼, $I+\delta[\lambda X]$로 이렇게, 마지막에 있는 항이, $1-cos\delta$였기 때문에 $cos\delta$가 1이었습니다. 그래서 $cos\delta$를 1로 치환하게 되면 사라지고 이와 같은 $sin\delta$가, $\delta$가 되는 이런 식으로 주어지게 되고 이런 식으로 주어진 것들을 바탕으로 해서 $\delta_1$만큼 $\delta_2$만큼,어떤 식으로 이루어지더라도, 결과적으로는 이와 같이, 위에 보이는 식처럼, $I+\delta_1,\delta_2$의 조합으로 표현이 된다는 것을 확인할 수 있습니다. 그래서, 굉장히 작게 돌아갔을 경우에는 작은 각도로 회전할 경우에는 로테이션끼리 서로 commute한다는 것을 확인할 수 있게 됩니다. 한번 정리를 하고 넘어가자면, 우리는 로테이션을 표현하는 여러 방법이 있다고 이야기했고,그 로테이션을 표현하는 방법은, 로테이션 매트릭스라고 얘기하기도 하고, direction cosine matrix라고 하기도 합니다. 다른 한편으로는, 어떠한 프레임에 대해서, 다른 프레임의 기준좌표, XYZ를 어떻게 표현하는지에 대한, 그 표현법이라고 말하기도 하고, 그 다른 프레임 원래의 프레임에서의 한 벡터를 이 기준좌표계의 XYZ로 투영시키기 위해서, projection시키기 위해서, projection시키는 개념으로, 로테이션 매트릭스를 살펴보기도 했습니다. 또는, Euler angle로서 2개의 좌표계를 바꾸는 방법 12개의 각기 다른 센서를 통해서 표현하는 방법도 살펴보았고 그리고, Euler axis와 Euler angle을 통해서 하나의 축과 회전 각도를 통해서 2개의 좌표계를 완벽하게 일치시킬 수 있다는 것도 살펴보았습니다. 이 2가지, 금방 말했던 축과 각도를 가지고 표현하는 4가지의 파라미터로 표현하는, quaternion 방법을 통해서, 각도를 표현하는 것도 가능하고 이런 것들은 Euler angle이 가지는 단점들을 어느 정도는 제거할 수 있다고 말씀드렸습니다. 여기까지 배운, 이러한 내용을 바탕으로, 어떤 좌표에서 어떻게, 이러한 개념에서 저러한 개념으로, 그 표현하는 방법과 개념들 사이의 상관관계와, 모든 것들을 왔다갔다할 수 있는 그러한 방법들에 대해서 익숙해질 필요가 있을 것입니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 3주차 3-3 Euler Angles(1)]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-04-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-3-Weak-3-3%2F</url>
    <content type="text"><![CDATA[Euler Angles 어떤 Rotation을 한 프레임에서 다른 프레임으로 바꿀 때 3개의 Simple Rotation 앞서서 3개의 Rotation Matrix에서는 3개의 파라미터를 가지고 표현할 수 있습니다. 이렇게 이 3개의 파라미터를 3개의 Simple Rotation이라는 개념을 통해서 어떠한 프레임에서 다른 프레임으로 항상 바꿀 수 있다 라고 하는 그러한 방법이 바로 Euler Angles입니다. 이 Euler Angles은 하나의 어떤 Angle을 어떤 특별한 축에 대해서 하나의 Angle을 통해서 Rotation이 이루어지고 그러한 것들을 조합함으로서 원하는 Rotation Matrix를 만들어낼 수 있다 라고 하는 것이 바로 Euler Angles입니다. 이런 축이 아닌 즉 $x$방향 $y$방향, $z$방향, 또는 $x$방향, $z$방향, $x$방향, 즉 이렇게 3개의 Rotation을 만들어내는 어떠한 조합 이러한 3개의 회전이 만들어지는 조합은 총 12개를 만들어낼 수가 있습니다. 12개의 이 조합 중에서 가장 널리 쓰이는 조합은, ZYZ라고 하는 조합입니다. 마지막에 로봇 축 같은 것들이 회전하고 이동하고 마지막에 그리퍼까지 만들어지는 과정이 $z$축에 대한 회전, $y$축에 대한 회전,다시 $z$축에 대한 회전 으로 로봇의 구조와 회전이 이루어지기 때문에 이러한 회전으로서 마지막 말단 장치의 좌표계를 설명할 수가 있습니다. ZYZ라는 이러한 방법을 통해서 이 로봇의 회전을 표현하는 데에 유리하기 때문에 ZYZ라는 방법이 널리 쓰입니다. 첫번째로 $z$축에 대해서 회전을 한번 구현하고 그 다음에 돌아간 회전축에 대해서 다시 한번 $y$축에 대한 회전을 구현하고 다시 돌아간 축에 대해서 마지막으로 $z$축에 대해서 회전을 하게 되면, 즉 $z$축, $y$축, $z$축을 한번씩 돌아가면서 반복함으로서 어떠한 회전을 정확하게 표현하는 방법 입니다. 이렇게 표현하는 방법들을, 총 12가지의 시스템, 즉 트웰브 시스템이라고 말하는데, 표현 방법에 따라서, 이런저런 조합에 따라, 각각 표현하는 방법들이 달라질 수 있다. A라는 프레임과 B라는 프레임이 이렇게 존재하고 있을 때, 이 A라는 프레임과 B라는 프레임을 회전하는 규칙을 나는 Z축을 한번 돌리고, Y축을 한번 돌리고, Z축을 다시 한번 돌려서, 이렇게 만들거야, 라는 규칙을 정했다면 나는 ZYZ라는 방법으로 각각 10도, 20도, 30도 만큼 돌렸을 때 이렇게 나왔다, 라는 것을 다른 사람에게 설명한다면 다른 사람들도 역시 마찬가지로 이 사람이 Z축으로 10도, Y축으로 20도, Z축으로 30도 돌렸을 때 되는구나, 하고 납득하고 똑같이 하면 된다는 규칙 설명에 있어서, 좋게 작용할 것이기 때문에 이러한 규칙에 의해서 이렇게 회전을 정의하게 됩니다. Euler Angles의 장단점 장점은 어떤 로봇을 만들 때, 대부분 아까 설명했던 것처럼 로봇의 말단 장치에서 이 말단 장치가 마지막에 구성되는 방식이$z$축과 $y$축과 $z$축의 어떤 순서로 마지막 손목 조인트들이 구성됩니다. 이러한 방식처럼 또 다른 방법으로 손목을 만들 수도 있는데 이렇게 만들었을 때, 이러한 방법들대로 회전이 이루어지기 때문에, 마지막 말단 장치에서의 어떤 회전의 방향을 정확하게 표현할 수 있다, 라는 장점을 가지고 있습니다. 단점은 2가지 솔루션이 존재할 수 있다는 것입니다. 즉 회전을 어떤 방식으로 표현하느냐에 따라서, 이 표현하는 방법이 동일한 표현, 동일한 어떤 각도 표현법이지만 여러가지로 표현될 수 있지만 그에 대해 명확한 정의를 내리지 않는 경우에는, 혼동을 불러일으킬 수 있다는 점이 단점으로 작용합니다. ZYZ의 종류 그렇다면, 금방 말씀드렸던 이런 혼동이 있을 수 있다 라는 것들에 대해서 살펴보게 되면 ZYX relative rotation, XYZ absolute rotation이라고 하는, 두 가지의 Euler Angles이 존재합니다 다른 말로 하자면 Roll, Pitch, Yaw라고 정의되어 있는, Roll, Pitch, Yaw을 바로 ZYX relative rotation, 또는 XYZ absolute rotation이라고 정의할 수가 있는데 ZYX relative rotation과, XYZ absolute rotation은 결과를 먼저 말씀드리자면 동일한 Rotation을 의미합니다. ZYX relative rotation ZYX relative rotation이라고 하는 것은 먼저 $z_0$에 대해서 $\alpha$만큼 회전하고, $y_1$축에 대해서 $\beta$만큼 회전하고 다시 $x_2$축에 대해서 $\gamma$만큼 회전하는 것들이 차례대로 이루어져 있는 것들을 말합니다. 이 relative rotation의 의미는 이 Matrix 자체가 앞에서부터 $\alpha$, $\beta$, $\gamma$의 순으로 위의 수식대로 배치되어 있지만 사실상 앞에서 말씀드린 것처럼 어떤 좌표상에서 표현되는 점 $p$를 1번 좌표계에서 0번 좌표계로 이동을 해올 때 맨 오른쪽에 먼저 곱해주게 되면 오른쪽에 곱해졌을 때 먼저 반영되는 것은 먼저 $x$축에 대해서 $\gamma$만큼 회전하고 그렇게 이루어진 다음에 다시 $y$축에 대해서 $\beta$만큼 회전이 이루어지고 다시 회전이 다 이루어진 그 좌표계를 다시 $z$축에 대해서 $\alpha$만큼 회전하는 즉 차례대로, 거꾸로, 하나씩 오게 되기 때문에 그 순서가 ZYX, 즉 표현법과 매트릭스 자체는 $z$축에 대한 회전, $y$축에 대한 회전, $x$축에 대한 회전이라고 할 수 있습니다. 그렇지만 실제로는 $x_2$, $y_1$, $z_0$에 대해서 순차적으로 회전이 이루어지는 그런 방식이기 때문에, ZYX relative rotation이라고 표현하게 됩니다. 그래서 위에 있는 그림처럼, 실제로는 여러분들이 회전에 대한 이해를 거꾸로 $x_2$와 $x_3$가 일치되어 있는 마지막 그림에서부터 먼저 $x_2$축에 대해서 회전, $y_1$축에 대한 회전, $z_0$축에 대한 회전을 통해서 마지막 전이, 즉 1프레임에 있었던 어떤 포지션이 다른 프레임으로 회전이 이루어지는 과정의 일환이라고 보면 되겠습니다. XYZ absolute rotation 반대로, XYZ absolute rotation이라고 하는 것은 먼저 쓰는 방식으로 보면,$x$축에 대한 회전, $y$축에 대한 회전, $z$축에 대한 회전 즉, 이 순서로 보면 위에 있는 이 매트릭스처럼 처음에 놓여 있는 순서는 분명히 $z$축에 대한 $\alpha$만큼 회전 그 다음에 그 $y$축에 대한 $\beta$만큼의 회전 그리고 $x$축에 대한 $\gamma$만큼의 회전이 있는데 XYZ, 즉 $x$축에 대한 회전이 먼저 일어나고 다음에 $y$축에 대한 회전이 일어나고 $z$축에 대한 회전이 일어난다는 의미에서 XYZ로 표현하는 방식은 absolute 방식입니다. 왜 absolute 방식인지에 대한 설명은 이 그림을 통해 살펴볼 수 있습니다. 즉, 회전이 이루어지는데 먼저 이 $x$축에 대한 회전을 수행한 다음, 다시 $x_0$, $y_0$, $z_0$에서의, $y_0$방향으로의 $y$축 회전을 이루고 그 다음에 역시 $x_0$, $y_0$, $z_0$에서 $z_0$ 방향으로 $z$축 회전을 수행하는 즉, 고정되어 있는 첫번째 좌표계는 그대로 유지하면서 각각의 좌표계가 그 좌표계에 대한 회전으로 표현하는 방법을 XYZ absolute rotation이라고 말합니다. ZYX relative rotation과 XYZ absolute rotation 2가지 방식은 동일한 회전을 의미 합니다. 이 두가지 동일한 회전은 어떤 측면에서 봤을 때는 이 회전이 이렇게 이루어졌는지, 다르게 이루어졌는지 정확히 설명하지 않으면 혼동이 일어날 수 있다. 즉 이렇게 2가지 표현방식으로 이루어질 수 있다는 단점을 가지고 있기 때문에 명확하게 어떤 식으로 회전을 한다는 것을 명확하게 설명하는 것이 필요합니다. 그렇다면, 이러한 회전이 이루어졌을 때, 앞서서 말한 것처럼, 우리는 총 9개의 파라미터들을 갖고 있고 9개의 equations 중에서 3개의 파라미터 즉 회전이 각각 ZYX 방향으로 얼마만큼 이루어졌는지 XYZ에 의한 회전에 의해서 회전하게 되면 위에서 보는 것처럼 이러한 Rotation Matrix를 얻을 수 있습니다. 대표적으로, 여러분들이 나중에 접하게 될 자이로라던가 기타 각도를 측정할 수 있는 센서들을 통해서 얻게 되는 경우에는대부분 Rotation Matrix, 또는 Roll, Pitch, Yaw라고 하는 각도를 얻게 됩니다. Rotation Matrix로부터 내가 원하는 어떤 값은? 그러한 Rotation Matrix로부터 내가 원하는 어떤 값은 3가지입니다. 전체적인 Rotation을 표현하기 위한 3가지 값을 얻어내는 것이 필요하게 됩니다. 첫번째로 얻는 것이 $\beta$값입니다. 위에서 보이는 식처럼, arctan라는, 즉 이 위에 보이는 매트릭스의 3번째 줄을 보면 $-sin(\beta)$, $cos(\beta)sin(\gamma)$, $cos(\beta)cos(\gamma)$ 이 값을 이루어서 3번째 줄의 2번째/3번째 요소를 제곱해서 더하게 되면 $\gamma$에 의한 성분은 사라지고, $\beta$에 의한 성분만 나타나게 됩니다. 위에서 보이는 식처럼, arctan라는 즉 위에 보이는 매트릭스의 3번째 줄을 보면 $-sin(\beta)$, $cos(\beta)sin(\gamma)$, $cos(\beta)cos(\gamma)$들이 값을 이루어서 3번째 줄의 2번째/3번째 요소를 제곱해서 더하게 되면 $\gamma$에 의한 성분은 사라지고, $\beta$에 의한 성분만 나타나게 됩니다. 이것은 $arctan의 표현과 어떤 모호성을 제거하기 위해서 컴퓨터 프로그램 상에서 제공하는 식이다. $x$와 $y$의 component $x$와 $y$가 어느 축에 존재하는지 즉 $\pm$로 어느 축에 존재하는지에 따라서, arctan라고 하게 되면, ±90도 사이에서 정의되기 때문에 90도일 경우에는 불연속이 발생하는 것을 확인할 수 있습니다. 따라서, 어떤 각도를 표현하는 데에 있어서 일반적인 arctan를 취하면 모호한 값이 나오는 경우가 있어서 이를 방지하기 위해, arctan2는 정확하게 1,2,3,4분면의 각각의 포지션을 즉 X방향, Y방향에 대해서 정확하게 $\pm$인지를 확인해서 각도에 대한 모호성을 없앤 어떤 수식 명령어라고 보면 됩니다. 그래서 arctan2라는 것을 통해서 $\beta$ 값을 찾아낼 수 있게 되고 이렇게 $\beta$ 값을 찾아내게 되면 이 $cos(\beta)$가 0이 아니라는 가정 하에서 우리는 이 $\beta$값들을 활용해서 1번째 column과, 3번째 row을, 다시 잘 조합함으로서 arctan2 라는 명령어를 활용해서, $\alpha$와 $\gamma$를 역시 똑같이 옆에 보이는 식처럼 얻어낼 수 있게 됩니다. 즉, $cos(\beta)$가 0이 아니라는 조건하에서 이렇게 얻어진 Rotation Matrix를 통해서 $\beta$값을 먼저 얻어내고그 $\beta$값을 활용하고 $cos(\beta)$가 0이 아니라는 전제 조건 하에서 $\alpha$와 $\gamma$를 얻어내게 됩니다. 그런데 만약에, 이 $\beta$가 $\pm$90도일 때 $cos(\beta)$가 결국 0이 되기 때문에, 앞서서 살펴본 것처럼 $\alpha$와 $\gamma$ 값을 얻어낼 수 없게 됩니다. 비행기에서 $\beta$ 값이 0이라는 의미는 비행기가 완전히 하늘을 향하거나 완전히 바닥을 향하는 경우 이렇게 놓여져 있을 때 $\beta$값은 -90도, 또는 90도로 나타나게 됩니다. 짐벌락(GimBal Lock) 이렇게 주어졌을 경우에, 흔히 다른 영역에서는 보통 이것을 ‘GimBal Lock’이라고 말합니다. 이 GimBal Lock에서는 2개의 축이, 이 베타각 ±90도 이루어짐으로서 $x$축에 대한 회전과 $z$축에 대한 회전이 겹치는 경향이 생기게 됩니다. 이 $x$축에 대한 회전과, $z$축에 대한 회전이, 겹쳐버리기 때문에, 2개의 회전이 하나로서 표현되는 것이 아니라 각각에 대한 $\alpha$, $\gamma$에 대한 회전의 합에 의해서 회전한다. 즉, 회전 자체가 조금 모호해지는 그러한 단점이 발생하게 되는게 바로 ±90도가 $\beta$값에서 얻게 되는 때에 발생하게 됩니다. 그래서, 그렇다면 이러한 단점들 때문에 Euler Angles을 사용할 수 없느냐 하면 사실 일반적으로 많은 경우에서 여러분들이 응용해서 사용하는 경우에서는 $\beta$ 값이 ±90도가 되는 경우가 드물 겁니다.다음에, 여러분들이 Moving Platform, 즉 움직이는 비행기나 수중 Platform을 다룰 경우 금방 말씀드린 이 $\beta$값이 90도가 되는 경우가 충분히 발생할 수 있기에 주의해야겠지만일반적인 Robot Manipulator에서는 $\beta$값이 ±90가 되는 경우는 그다지 많지 않기 때문에 이에 대해서는 크게 신경쓰지 않아도 좋을 것 같습니다 Roll, Pitch and Yaw Angles ZYX, XYZ, 이런 식의 표현법보다는 일반적으로 Roll, Pitch and Yaw Angles 즉, 위에 보이는 식처럼, $R_z$, 즉 $z$축에서의 $\psi$만큼의 회전, $y$축에서의 $\theta$만큼의 회전, $x$축에서의 $phi$만큼의 회전 이런 연속적으로 이루어진 방식 즉 ZYX의 이 방식을 따르는 것을, Roll pitch Yaw Angles에 의한 Euler Angles 표현법이라고 설명합니다. Euler’s Theorem 이 3가지의 회전에 대해서 어떤 2개의 프레임이 존재할 때, 어떻게 $x$축, $y$축, $z$축으로 회전을 했을 경우에 2개를 일치시킬 수 있다, 라고 하는 게 Euler Angles이라고 말했었는데,이 Euler’s Theorem라는 것은 한번의 회전으로 한 프레임에 대해서 다른 프레임으로 갈 수 있는 방법이 있다 라고 하는 것이 Euler’s Theorem 입니다. 그래서, 이 Euler’s Theorem라 하는 것은 어떠한 임의의 한 축에 대해서 그 축을 찾을 수 있고 그 축에 대해서 회전하는 각도만큼만 확인할 수 있다면 그 2가지의 축과 회전하는 각도에 의해서 한 프레임에서 다른 프레임으로, 한번의 회전으로 갈 수 있다는, 어떻게 보면 한번의 회전으로 2개의 축을 일치시킬 수 있다. 반면에, 이 회전하는 축을 찾아내는 보통 $\lambda$라는 하는 이 축을 찾아내는 것은 항상 쉽지만은 않기 때문에 이 표현법에 대해서 수학적으로 접근하는 데에 있어서는 매력적일 수 있는데 반대로 실용적인 면에서는 이 축을 정확하게 찾아내기는 어렵기 때문에 그다지 실용적이지는 않다는 것을 유념해야 합니다.그래서 이러한 축을, 일반적으로 $\lambda$ 라고 말하고, 이 $\lambda$를 eigen-axis 아니면 오일러가 찾아냈기 때문에 Euler axis라고도 합니다. 이 회전축에 대해서는 $\lambda$라고 말하고 이 회전축이 얼마만큼 회전했느냐를 $\delta$라고 해서 이를 Euler Angle, Principal Euler Angle이라 말합니다,이렇게 $\delta$와 $\lambda$를 통해, 즉 $\lambda$라고 하면 벡터기 때문에 3개의 파라미터를 갖습니다. 거기에 추가적으로, 이 $\delta$만큼의 각도를 가지고 표현하는 이 4가지 파라미터로 표현하는 것을 quaternion 표현법이라고 말합니다. Quaternion 표현법 이 Quaternion 표현법 같은 경우에는 명확하게 한 축과 돌아가는 각도를 통해 설명하기 때문에 앞서서 설명했던 어떤 표현법에 있어서의 2가지의 각도 표현법이라는 어떤 중복성 문제라던가 또는 아까 말씀드린 $y$축에 대해 ±90도의 값을 통해 GimBal Lock이 생긴다는 문제점을 해결할 수 있는 좋은 방법이기 때문에 Quaternion 표현법도 각도를 표현하는 데에 있어서 많이 사용되는 방법 중 하나입니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 3주차 3-2 Single Rigid Body]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-04-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-3-Weak-3-2%2F</url>
    <content type="text"><![CDATA[강체(Rigid Body)alt|동역학, 기구학적 해석을 많이 사용| alt alt|이 2가지를 안다면 물체의 위치와 물체에 대해서 정확하게 설명할 수 있다.| 변위(Displacement): 나로부터 떨어진 거리, 즉 reference frame(기준틀)이라고 불리는 것부터 떨어져 있는 어떤 object까지의 거리 각도(Attitude or Orientation): 물체가 나를 기준으로 어덯게 틀어져있는지 나타내는 지표 alt Coordinate Framesalt Camera Frame, Tool Frame, Link Frame, Base Frame, Goal Frame, 즉 각각의 좌표들은 내가 어떤 기준점을 가지고 보고 있는지 나타냄 alt 예를 들면 Camera Frame에서는 카메라의 렌즈를 기준으로 내가 무엇을, 어떤 방식으로 보고 있는지를 나타낸다. 각각의 자기만의 Frame을 가지고 있을 것이고 그 Frame에 맞춰서 Coordinate System에 맞춰서 해석을 하는 게 훨씬 쉬운 일이라는 것을 알 수 있습니다. alt 그렇지만, 일반적으로 카메라에서 보고 있는 것을, 내 기준으로 살펴보거나 로봇의 기준으로 살펴보기 위해서는 각각의 프레임 간의 변형 또는 변환이 필요할 수 있습니다. Single Rigid Body Objectalt $frame$: 어떤 좌표계에서 $x$,$y$,$z$ 방향의 기준되는 벡터, 단위 벡터가 어떤 식으로 배치되어 있는지를 설명하는 것 alt $^\textit{frame}\textbf{p}$: 나는 어느 frame에서 표현된 벡터입니다. 라는 것을 설명하는 표현 alt $^\textit{frame}\textbf{v}_\textbf{p}$: frame에 대해서 Point $p$의 속도 alt $^\textit{frame1}\textbf{r}_\textit{frame2}$: $frame1$을 기준으로 $frame2$가 어디에 있는지 나타내는 표현 alt $^\textit{frame1}\textbf{R}_\textit{frame2}$: $frame1$에 대해서 $frame2$가 얼마만큼 돌아가 있는지 나나태는 표현 (R: Rotation Matrix) alt Spatial frame(= reference frame): 절대로 움직이지 않고, 절대로 고정되어 있는 frame alt Body frame: 내가 관심이 있는, 내가 바라봤을 때 움직이고 있는 어떠한 시스템 alt $^\textit{frame1}\textbf{v}_\textit{frame2}$: $frame1$에 대해서 $frame2$의 속도 alt $^\textit{frame1}\textbf{w}_\textit{frame2}$: $frame1$에 대해서 $frame2$의 각속도 Kinematic Relationshipalt alt 물체와 나와의 관계를 정확하게 설명하기 위해, 우리는 4X4의 Homogeneous Transformation Matrix라고 불리는, 위에 보이는 이러한 Matrix의 형태로 표현한다. Description of a Positionalt 만약에 어떤 coordinate system상에 존재하는 $P$라는 점이 있다면, 그 $P$라는 점은, $A$라는 어떤 coordinate system이라면, $A$라는 프레임상에서 나의 위치는, $P_x$, $P_y$, $P_z$로, 이러한 Position Vector로서 간단하게 표현이 가능합니다. alt 그렇다면, $frame0$,$frame1$ 2개의 프레임이 존재하게 되겠고 그 2개의 떨어진 거리는, 지금은 신경쓰지 말고, 지금은 떨어진 거리가 존재하지 않고 일치한다 즉 원점이 일치하도록 위치시킨 다음에. 이들의 어떤 각도가 얼만큼 틀어져있는지만 살펴본다라고 했을 때 우리는 이러한 각도관계를 쉽게 찾아보기 위해서 내적이라는 개념을 쓰면 손쉽게 알아낼 수가 있습니다. alt alt 이 내적이라는 것은 어떠한 프레임에서 다른 프레임에의 영사, projection 시킴으로서 둘 사이의 관계를 알아낼 수가 있습니다. alt 그래서, 위에 보이는 식과 같이, $x$, $y$, $z$를 각각 내적을 취함으로서 이렇게 component 별로 얻어낼 수가 있고, 이것을 보다 더 일반적으로 다시 기술하면, 아래와 같이, 매트릭스의 형태로 표현이 가능합니다. 이렇게 매트릭스 형태로 표현했을 때 즉, 일반 프레임에서의 어떤 $P$점을, 1번 frame에서의 $P$점을, 0번 frame에서 그 $P$점을 어떻게 위치하는 지를 알아내기 위해서 우리는 중간의 이러한 연산관계들을 정리한 Direction cosines의 형태로 Matrix를 곱해줌으로서, 바로 이 $P$점이 1번 프레임이 아닌, 0번 프레임에서 어떻게 설명이 되는지를 알아낼 수가 있게 됩니다. alt Orientation은 1번 프레임에서 표현되어 있었던 점을, 0번 프레임으로 가져오기 위해서 변환 관계식을 필요로 한다는 것을 알 수 있겠고 이 변환관계식은 바로 내적이라는 개념을 통해서 뽑아낼 수 있다는 것을 바로 위에서 살펴봤던 식을 통해서 알 수 있게 됩니다. alt alt 2개의 축이 존재할 때, 그 2개의 축을 회전시키면 일치시킬 수 있습니다.만약에 2개의 축이 $z$축, 즉 $z$축 방향으로 $\theta$만큼 돌아갔을 때, 일치시킬 수 있을때 위에 보이는 식처럼, 이렇게 $cos\theta$와 $sin\theta$,의 관계를 통해서, Rotation Matrix로 표현이 가능합니다.역시, $x$방향에 대해서도, $y$방향에 대해서도, 각각 Rotation Matrix를 통해서, 2개의 축을 일치시킬 수 있습니다. 2개의 어떤 프레임 상에서의 내적을 통해서, 그 내적을 통해서 만들어낸 Matrix와 어떤 관계를 가지고 있는지, 그 2개는 어떠한 상관관계를 갖고 있는지, 조금 더 살펴볼 필요가 있습니다. Rotationalt Rotation이라고 하는 것은 일반적으로 1번 축에서부터 0번 축까지 가지고 오는 회전의 관계 또는 2개의 일치관계를 찾아내는 것이다 이 일치관계를 찾아내는 것을 일반적으로 Rotation Matrix, 즉 1번부터 0번까지 가지고오는 Rotation Matrix로 볼 수도 있고, 또는 2개를 단순히 coordinate trasformation으로 볼 수도 있고 어떠한 한 축에 대한 다른 축, 한 프레임에 대한 다른 프레임에서의 표현법으로 나타낼 수도 있습니다. alt 첫 번째로는 보통 1에서부터 0까지 가지고 오는, $R$ Matrix를 1번에서 표현되어 있는 $P$점, 즉 1번 프레임에서 표현되는 $P$점을 0번 프레임으로 mapping시킨다는 개념으로 위에서 보여주는 것처럼, 이러한 Matrix가 주어졌을 때 1번 프레임 상에 놓여 있는 [1,0,0]이라는 점을 $R$이라는 Matrix로 mapping, transformation을 통해서, [0,1,0]으로 mapping되는 것이라고 생각할 수 있고요. alt 두번째로는, Rotation을 시킨다는 개념, 즉 (1,0,0)이라고 하는 이 점을 이 Rotation Matrix로 Rotation시키면, 그 다음 프레임에서의 점으로 가지고 올 수 있다, 또는 회전시킬 수 있다, 라는 개념, 즉 Rotation Matrix로 볼 수 있습니다. alt 세번째로는, 1번 프레임에서의 기준 좌표가 0번 프레임에서 어떻게 표현되는지, 즉 1번 프레임에서 2개의 좌표를 보면,1번 프레임에서 [1,0,0], [0,1,0], [0,0,1]이라고 하는 3개의 기준 좌표를, 0번 좌표계에서 표현하게 되면, [0,1,0], [-1,0,0], [0,0,1]로 각각의 좌표계를 표현할 수 있게 됩니다. alt Rotational Matricesalt 이러한 Rotation Matrix는 special orthogonal group, 즉 $SO(3)$라고 하는 그룹에 포함되어 있다고 설명을 할 수 있겠습니다. alt special orthogonal group이라는 것은 그 그룹에 포합된 벡터들이 상호간에는 orthogonal, 즉 내적을 취했을 때 0이 되고 자기 자신을 내적했을 때에는 크기가 1이 되는 성질을 가지고 있는 그러한 벡터들의 집합을 보통 special orthogonal group이라고 말합니다.그 중에서, 삼차원 벡터 3개로 이루어진 집합을 special orthogonal group(3)이라고 표현합니다.이러한 special orthogonal group에 포함되어 있는, 벡터들의 집합이 매트릭스를 이루고 이를 $R$이라고 했을 때,R에 대한 역행렬을 구할 때 자기 자신을 Transpose한 것을 가지고 곱하게 되면 역행렬을 얻을 수 있고 Determinant를 했을 때, 1이 나온다는 좋은 성질들을 가지고 있습니다. 이러한 성질들은 추후에 어떤 연산을 진행할 때, 편하게 연산이 진행될 수 있기 때문에, 연산들의 특징을 가지고 있는 Rotation Matrix는 special orthogonal group에 포함되어 있는 Matrix 라고 알아두면 됩니다. alt Rotation Matrix의 조금 전에 말했던 special orthogonal group(3)에 포함되어 있는 성질들 때문에 Rotation Matrix는 3X3 Matrix이기 때문에, 9개의 variable로 이루어져 있는데 그 중에서, 각각의 벡터들끼리는 서로 직교한다는 성질 3가지,즉 $R_1$, $R_2$, $R_3$가 $R_1$과 $R_2$, $R_1$과 $R_3$, $R_2$와 $R_3$가 서로 직교한다는 성질 3가지,그리고 $R_1$의 크기는 1, $R_2$의 크기는 1, $R_3$의 크기는 1, 이라고 하는 3가지,즉 6가지의 constraints를 가지고 있기 때문에, 총 9개의 variable이지만, 3개의 parameter만 독립인 매트릭스가 되겠습니다. 좌표계가 공간 상에서 3개의 각도를 가진다, 라는 개념과 동일합니다.즉 3개의 각도를 가진다는 개념과 동일하게 생각할 수 있어 역시 Rotaion Matrix에서도, 3가지의 parameter만 서로 독립인 그러한 성질을 갖게 됩니다. 이러한 로테이션 매트릭스는 우리한테 이것들이 commute하지 않는다는 어려움을 줍니다. 즉 내가 Rotation을 어느 방향으로 먼저 하느냐에 따라서 이 순서를 지키지 않으면, 엉뚱한 결과가 나올 수 있는, 즉 commute하지 않다는 성질들 때문에 직관적이지 않다는 측면 때문에 많은 사람들이 어려움을 갖습니다. alt 이러한 Rotation Matrix는 어떻게 표현하느냐에 따라서 여러가지 이름으로 불리고 있습니다.대표적으로 비행기의 어떤 자세각도를 표현하기 위해서, Roll, Pitch, yaw라는 angles을 통해서 설명하기도 하고,그리고 로봇에서 범용적으로 널리 사용되는 것은 Euler Angle이라는 방법 입니다. Euler Angle의 parameters들을 활용해서 Quaternions이라는 개념을 통해서 각도를 기술할 수도 있습니다.또는, Rodrigues parameters라는 Gibbs vectors를 활용해서 표현하는 기법도 있고,이 Rodrigues parameters가 표현하기가 어렵기 때문에 Modified Rodrigues parameters로 표현하는 경우도 있습니다. alt 조금 전에 말했던 것처럼 각도를 표현하는 데에 있어서 어려움이 있는 이유는 일반적으로 로테이션은 commute하지 않기 때문입니다. alt alt alt 먼저 $x$방향으로 먼저 돌리고 $z$축방향으로 돌렸을 때, 나타나는 위에 보이는 그림에서의 책이 마지막에 이렇게 서 있는 모습과반대로 $z$축으로 먼저 돌리고 $x$축으로 돌렸을 때, 서 있는 그림은 다르다는 것을 알 수가 있습니다. alt 여러분들이 길을 찾아갈 때, 어떤 블록에서 좌회전을 먼저 하고 우회전을 하는 것과 우회전을 먼저 하고 좌회전을 했을 경우에, 여러분들이 최종적으로 위치하게 되는 지점은 다르다는 것을 쉽게 이해할 수 있을 겁니다. 즉 이러한 Rotation은 어떻게 회전하느냐에 따라서 다른 결과를 내기 때문에, 생각하는 데에 있어서, 조심할 필요가 있습니다. alt 반면에, 위치 같은 경우에는 $x$방향으로 얼마만큼 가고 $y$방향으로 얼마만큼 갔을 때와 $y$방향으로 먼저 가고 $x$방향으로 갔을 때의 그 두 위치는 같다라는 것을 알 수 있습니다. 그 두 가지의 차이에 의해서, 위치와 각도에 대한 생각의 방향들을 달리해볼 필요가 있을 것입니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 3주차 3-1 Robot Kinematics Introduction]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-03-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-3-Weak-3-1%2F</url>
    <content type="text"><![CDATA[Robot Manipulator 종류Open loop manipulator(Serial robot manipulator)alt Open loop Manipulator라고 하면 위에 보이는 그림처럼 로봇들이 사람의 팔과 같이 하나하나하나 연속해서 serial과 같이 open loop로 연결되어 있는 Manipulator을 지칭합니다. Open loop Manipulator 장점alt Open loop Manipulator 단점alt Parallel robot manipulatoralt Parallel robot manipulator 같은 경우에는 각각의 어떤 로봇의 지지체, 구동부를 여러 로봇의 어떤 다리들이 연속적으로, 평행하게 받쳐주면서 구동하도록 하는 로봇을 말한다. Parallel robot manipulator 장점alt Parallel robot manipulator 단점alt 기구학(Kinematics)alt 직렬형, 병렬형 로봇 매니퓰레이터에 있어서 각각의 링크가 각각의 조인트가 구동됨에 따라서, 끝점, 내가 원하는 어떠한 로봇 팔의 끝점, 작업하는 점이 어떻게 움직이는지에 대한 위치, 속도, 가속도의 측면에서 해석하는 것을 기구학이라고 부릅니다. 역기구학(Inverse Kinematics)alt Inverse Kinematics, 즉 역기구학이라고 하는 것은, 끝점이 이렇게 움직이기 위해서 각각의 관절들을 어떻게 구동시켜야 되는지에 대한 해석을 하는 것들을 Inverse Kinematics라고 합니다. alt 관절(Joints)의 종류alt 회전형 관절(Revolute)alt 모터 하나를 가지고 이러한 회전형 Joint를 구동하는 방식으로 많이 이루어집니다 직선형 관절(Prismatic)alt 유압 actuator에서 가장 많이 사용된다. 회전 및 직선형 관절(Cylindrical)alt Universal &amp; Sphericalalt 구동기의 종류alt 기구학 해석alt alt 기구학을 해석하는 데에 있어서 또 한가지 알아볼 필요가 있는 것은 어떻게 조인트들의 움직임을 로봇에서 구현할 수 있는지 또는 알아낼 수 있는지에 대한 것들을 살펴보는 것이다. 이 관절들이 얼마만큼 구동되는지 알아야 순기구학, 즉 끝점이 어딘지를 알아낼 수 있기 때문에, 이러한 관절들의 움직임을 정확하게 측정할 필요가 있습니다. 관절 각도 측정(Sensing angular displacement of a joint)alt Resolver, RVDT대표적인 관절각도 측정 장치로는 Resolver, RVDT가 있고 모터가 돌아갈 때 일종의 발전의 원리로서 모터가 빨리 돌아가거나, 아니면 어느 정도의 위치에 돌아갔을 경우에 그 값을 그 전류값을 통해서 모터가 얼마만큼 돌아가는지를 측정하는 방식의 어떤 센서가 되겠습니다. Potentiometer, Rotary Encoder거의 모든, 여러분들이 모터를 사용하는 어떤 어플리케이션이라고 생각한다면, 거의 모든 관절에는 Rotary Encoder가 포함되어 있다고 보면 되겠습니다. Optical Encoderalt 위에 나오는 그림처럼, 이렇게 Encoder 같은 경우에는 얇은 판에 구멍이 뚫려있고, 그 구멍 주변을 중심으로 앞뒤로, 한쪽에서는 발광, 한쪽에서는 수광하는 센서장치가 놓여있게 됩니다. 그래서, 발광하는 센서를 통해서, 만약에 모터가 돌아가게 되면, 많은 구멍이 뚫려 있는 판이 돌아가면서, 빛이 들어왔다, 안 들어왔다를 반복하게 됩니다. alt alt 이렇게 빛이 들어왔다가, 안 들어왔다를 반복하는 그러한 판에,보통 2가지의 신호를 받을 수 있도록, 2개의 구멍을 팠을 때, 보통 A상과 B상, 즉 A시그널과 B시그널을 약 90도 간격으로 어긋나게 배치함으로서,어느 쪽 엣지가 먼저 신호가 발생하는 지를 판단해서, 방향을 읽어낼 수 있고,그리고 더 정밀하게, 즉 막혀있는 구멍이 100개, 뚫려있는 구멍이 100개라고 한다면, 전체적으로 총 200번으로 전체 구간을 나눌 수 있을 겁니다.그런데, A와 B처럼 이렇게 약간씩 어긋나서, 0과 1이 반복되는 것을 어긋나게 했을 경우에는 A와 B의 각각에 대한 값들에 대한 조합으로,100개의 구멍이 있다고 했을 때, 총 400번으로 1바퀴를 나눠볼 수가 있을 겁니다.그렇기 때문에, Optical encoder를 통해서, 이러한 방식으로 모터가 돌아감으로서, 얼마만큼 돌아가는지를 측정할 수 있게 되는 것이죠. Optical Encoder의 종류alt 가장 많이 쓰는 것은, 증분령, 즉 Incremental Rotary Encoder 입니다. Incremental Rotary Encoder는 전기가 입력되는 순간, 즉 모터와 Encoder의 외부에서의 어떤 전원이 인가되는 순간부터 측정을 시작하게 됩니다. 즉 전원이 인가되는 순간부터, 내가 어떤 방향으로, 앞인지 뒤인지, 몇번의 펄스가 뛰었는지를 측정해서,내가 앞으로 얼마만큼, 뒤로 얼마만큼 돌았다는 것을 측정하는 방식이, 바로 Incremental Rotary Encoder 방식이 되겠습니다 alt Absolute 방식의 경우 전원을 인가하는 순간, 그 순간은 일반적인 Incremental Rotary Encoder 방식의 경우에는 2개의 구멍만이 파여 있는데, Absolute 방식의 경우에는, 필요에 따라서, 보통은 5개 이상, 5개~6개의 층으로 나누어진, 구멍이 뚫린 판으로 이루어져 있습니다. 위의 그림처럼, 이렇게 뚫려 있을 경우에는, 내가 어느 위치에 딱 존재하는지를 바로 알아낼 수 있는, 즉 이 구멍의 조합으로서, 5개 이상, 또는 많게는 10개까지, 이러한 구멍의 조합이 나타나게 되면, 그 조합에 의해서, 내 위치를 정확하게 알아낼 수 있다, 라는 그런 장점을 가지고 있습니다. Absolute Encoder, Incremental Rotary Encoder의 장단점alt Absolute Encoder가 훨씬 더 사용하기가 편하다는 생각을 할 수 있지만 Incremental Rotary Encoder를 더 많이 사용한다. alt Absolute Encoder 같은 경우에는, 그 편리성, 즉 순간 내가 어느 위치에 있는지를 정확하게 알 수 있다는 장점을 가지고 있지만, 반면에 10개의 구멍, 11개의 구멍, 12개의 구멍을 가지고 정확하게 알아내기 위해서, 즉 정밀하게 하기 위해서는, 그 구멍들의 개수가 늘어나야 할 텐데, 늘어나는 순간, 그 구멍들의 정보들을 전부 다 전달받아야 합니다.그렇기 때문에, 10개, 11개, 12개, 13개, 늘어나는 만큼, 신호 시그널을 받아내기 위한 선들의 숫자가 증가하게 되는 것이죠.즉 선이 굉장히 많이 필요하게 됩니다. 13개, 14개의 선의 경우, 보통 로봇을 만들기 위해 6개 이상의 모터를 사용하는데, 그 로봇의 6개의 모터를, 전부 다 이런 식으로 12개 이상의 선을 사용한다면, 로봇이 선으로 뒤범벅이 될 수밖에 없습니다. alt 두번째로는, 정밀도를 높이는 데에 있어서, 구멍의 수를, 10개, 12개, 13개, 14개 이렇게 증가시켜나가는 것인데, 이렇게 계속해서 무한정 늘려나가는 것은 어려움이 많습니다. alt Incremental Rotary Encoder는 전기가 딱 가해지는 순간부터 측정이 가능하다는 것 외에는, 전기가 가해지는 순간부터, 보통 몇 바퀴 돌아가는지에 대한 것들을 구멍의 개수만큼으로 측정할 수 있습니다. alt 구멍을 조금 작게 뚫는 것들은, 요즘에 많은 정밀 가공 기술들로 인해서 그렇게 비교적, 구멍의 종류를 늘리는 것들에 비해서, 조금 더 쉬운 일입니다. 그래서 정밀도를 증가시키기 위해서는, 이 구멍의 개수를 늘리는 것이 그렇게 어렵지 않기 때문에, 관절 각도 측정을 하는 데에 있어서는, Incremental Rotary Encoder의 정밀도를 높이면서 활용하기에는 좋다는 장점을 갖고 있습니다. alt 그렇지만, 단점은 아까 말씀드렸던 것처럼, 초기에, 내가 어느 위치에 있는지를 정확하게 알기가 어렵다는 점입니다. alt 그래서 보통은 외부에서 측정할 수 있는, 리미트 스위치, 또는 근접 스위치 같은 것들을 통해서 초기화 시켜준다. 말단 장치의 오차 수정 방법alt 모터들의 오차가 쌓이게 되면, 그리고 센서들의 오차도 쌓이게 되면 각각의 하나하나의 오차들이 마지막 끝단에서는 크게 나타날 수도 있습니다. 그렇기 때문에, 그 말단 장치, 맨 마지막 장치에는 오차가 크게 나타날 수가 있어서, 각각에 달려있는 Encoder, 즉 관절 측정장치와는 별도로, 말단 장치에서의 위치와 각도를 측정할 필요가 있을 수도 있습니다. alt alt alt]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 2주차 2-5 특이값 변환]]></title>
    <url>%2F2020%2F10%2F22%2F2019-04-01-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-2-Weak-2-5%2F</url>
    <content type="text"><![CDATA[특이값 분해(Singular Value Decomposition, SVD)고유벡터와 고유값alt 선형 변환의 개념일 때, 어떠한 벡터를 집어넣었는데, 그 벡터의 차원이 바뀌지 않고, 방향도 그대로 유지하면서, 크기만 바뀌는, 특수한 어떤 방향이 존재하게 됩니다. 이러한 특수한 방향이 존재하고, 그 특수한 방향에 대해서 얼마만큼의 배율로 늘어나거나 줄어드는지를 설명하는 것이, 고유값과 고유벡터라고 말합니다. alt|특수한 벡터 K와 연결되어서, 특수한 벡터 K를 곱했을 때, 그 K를 그쪽 방향으로는 람다만큼 증가시키는, 0이 아닌 람다만큼 증가시키는 효과를 나타내는 것을 고유값이라고 한다.| alt $\lambda$를, 이 $(A-\lambda I)$라는 매트릭스를, singular 값이 되게 하는 $\lambda$를 찾아서, 그 $\lambda$ 값을 통해서, 하나씩 하나씩 대입함으로서, $K$값을 구할 수 있게 됩니다. alt 이렇게 구해진 고유값과 고유벡터에서, 어떤 경우에는 고유값은 항상 존재하지만, 고유값에 대한 고유벡터가 존재하지 않는 경우도 생깁니다. alt Singular Value Decomposition에서는, 만약에 어떤 매트릭스 $A$가 주어졌을 떄, 그 $A$는 $A=U \Sigma V^T$라는 식처럼, $U$와 $\Sigma$와, $V$라는 3개의 매트릭스로 항상 분해가 가능합니다. 앞에서 설명했던 이 Range space와 Null space의 개념처럼, Range space의 크기를 갖는, $A$의 Row space, Null space의 개념들을 활용해서, $U$라고 하는, 서로 orthogonal basis를 가진 이런 매트릭스와 만약에 A라는 매트릭스가 아래로 긴 행렬이라면 아래로 대각선이 일정 부분까지만 연결되는, 대각선으로 된 항목들은 값을 갖지만, 나머지는 0이 되는 $\Sigma$ 매트릭스, 그리고 역시 orthogonal basis를 갖고 있고, $A$의 row space를 구성할 수 있는 각각의 벡터들로 이루어진 이 $V$라는 매트릭스, 이렇게 $U$,$\Sigma$,$V$라는 3개의 매트릭스로, $A$를 분해할 수 있습니다. alt 먼저 일단 각각의 $U$는 Column space $A$에 의해서 $U$를 먼저 Coulmn에, 이 $A$의 Rank만큼 Coulmn space를 결정하고, 그 나머지 공간들은 이 앞에서 구해진 column space의 수직하도록 맞추어 구성하면 됩니다. 임의로 구성해주되, 앞의 column space와 직교하는 항들로만 구성해서, 전체 basis를 구성하면 됩니다. alt 마찬가지로, $V$도 $A$의 Row space만큼을 구성하고, 나머지는 이 앞에 있는 Row space, $A$의 Row space에 직교되는 공간으로 채워주면 됩니다. 그래서 특징들은 orthogonal basis를, 전체를 채울 수 있도록만 만들어주면 됩니다. alt 앞서서, $A$라고 하는 선형변환, 매트릭스는 선형변환의 하나의 개념으로 설명할 수 있다고 했는데요. 만약에 2개의 차원이 똑같은 2차원이라고 생각해본다면, 위 그림처럼, 원으로 구성되어 있는 전체 요소, 즉 각각의 X방향과 Y방향의 크기가 1로 전체 구성되어있는, 크기가 1인 공간을, $A$라는 매트릭스를 거치게 되면, 새로운 공간으로 어떻게 변화되는가 하면, 옆으로 이렇게 타원형을 형성하면서 좀 돌아간, 즉 축이 바뀌고, 그 축에 대한 길이의 비율이 바뀌는 이러한 변환으로 $A$라는 매트릭스를 설명할 수 있습니다. 그렇다면, 주어진 원이라는 공간이 $A$라는 매트릭스를 통해서 변환되는 과정을, Singular Value Decomposition $A$를 통해서 다시 설명하자면, 먼저 앞에서 Singular Value Decomposition, 즉 $A=U \Sigma V^T$라고 했을 때, 결과적으로는 $V^T$가 먼저 변환에 활용됩니다. 그렇다면, 어떤, 이 원이라는 공간을, $V^T$로서, 변환을 시킨다면, 이 $V^T$는 일종의 회전 매트릭스입니다. 어떤 방향으로 회전하게 되냐면, 이 변환을, $A$라는 매트릭스를 통해서 변환하게 되면, $A$를 회전시키고, 특수한 방향으로 찌그러뜨리는 그러한 역할을 한다고 말했는데요, 즉 어떤 방향으로 증폭시키고, 어떤 방향으로는 조금 축소시키는, 그러한 방식으로 변환이 이루어지는데 즉 $V^T$를 통해서, 이 공간, 주어진 공간을 회전시키고, 회전시키는 것은 찌그러뜨리기 좋은, 어떤 프레스에 집어넣어 빵을 찍어내듯이, 찍기 좋은 방향으로 돌려주고, $\Sigma$ 매트릭스만큼, 어떤 방향으로 늘리고, 어떤 방향으로는 찌그러뜨리는 역할을 $\Sigma$ 매트릭스가 한다. $\Sigma_1$과 $\Sigma_2$에 의해서, 그 비율만큼, 타원형의 형태로 바뀌게 되는 것입니다. 원래 우리가 바라보는 관점에서 어떻게 돌아갔는지 다시 돌려놓기 위해서 $U$라는 매트릭스를 통해서 다시 돌려주는 역할,즉 $A$라는 변환은 언제 돌려주고, 눌러주고, 다시 돌려서 원래대로 복구시키는 과정의 일환으로서 이루어지는 것을 Singular Value Decomposition, 즉 SVD를 통해서 물리적인 설명이 가능합니다. 이렇게 이루어진 것을 SVD의 그래픽컬한 설명이라고 말할 수 있습니다. alt 그렇다면, 이러한 Singular Value Decomposition을 어떻게 구할 수 있느냐, 어떻게 정의할 수 있느냐 하는 것은, $A^TA$를 취하게 되면, $A^T$ 같은 경우는 아까 앞서서 트렌스포즈, 여러 개의 매트릭스들이 곱해져 있을 경우에 트렌스포즈를 취하게 되면 순서를 바꾼다고 했습니다. 그래서 위에 보이는 식처럼, $V \Sigma^T U \Sigma V^T$의 형태로 쓰여집니다.여기에서, $U^TU$는 이 자체가 orthogonal basis로 구성되어 이루어져 있기 때문에 이 $U^TU$는, Identity matrix이다.그래서 실용적으로는 $V \Sigma^2$은 m by m 매트릭스의 형태로 구성되면서 $V \Sigma^2 V^T$가 됩니다. alt $V$라는 매트릭스를 구해내기 위해서는 $A^TA$라는 이렇게 연산을 취하는 것들 이렇게 취해진 매트릭스의 고유값을 그리고 고유벡터를 구하는 방법으로 바로 $V$를 정의할 수 있게 됩니다. 이와 마찬가지로 $AA^T$를 하게 되면 동일하게, $U \Sigma^2 U^T$ 형태를 얻게 됩니다. 역시 마찬가지로, $AA^T$를 새로운 매트릭스로 치환하면 $U$는 그 새로운 매트릭스의 eigenvector가 되기 때문에, 이렇게 구해진 eigenvector로서 $U$를 구성하게 되면, SVD에서 $U$를 찾을 수 있게 됩니다. 마지막으로, $\Sigma^2$ 같은 경우에는 이 $\Sigma^2$은, $A^TA$와 $AA^T$에서 모두 나오게 되는데 차원이 조금 다릅니다. 다르지만 결국에는 전체가 가지고 있는 $A$가 가지고 있는 Range, 즉 전체의 Rank 숫자만큼 0이 아닌 원소로 채워질 것이기 때문에, 그 원소만 빼게 되면, $\Sigma$도 손쉽게 eigenvalue를 통해서 구할 수 있게 됩니다. 밑의 식처럼, $\sigma_i(A)= \sqrt{ \lambda(A^TA)}$ 식처럼 마지막으로 $\Sigma$ 매트릭스를 구해낼 수 있게 됩니다 alt 그렇다면 Singular Value Decomposition을 어디에서 사용하는가 하면 앞서서 이 선형변환의 역변환으로 설명했던 Pseudo-inverse를 설명했었는데 이 Pseudo-inverse에서, $A^+$를 구할 때 그 Pseudo-inverse를 Moore-Penrose pseudo-inverse의 경우에는 $JJ^T$, $AA^T$ 같은 식으로 썼는데 그보다 좀 더 편리한 방법으로 만약에 $A$라는 매트릭스를, Singular Value Decomposition을 할 수 있다면 $A^+= V \Sigma^+ U^T$ 형태로 쓸 수 있습니다. $V$와 $U$는 앞의 식을 그대로 사용하면 되고, $\Sigma^+$는 앞에서 pseudo-inverse를 구할 때 말했던 것처럼 $(\Sigma \Sigma^T)^-1$ 이런 식으로 해야 하지만 그럴 필요 없이 pseudo-inverse를 구할 때, $\Sigma$의 경우에는 대각선의 성분만 남아있기 때문에 대각선의 성분에 역수를 취해주면 아래로 긴 행렬이면 옆으로 긴 행렬로 옆으로 긴 행렬이면 아래로 긴 행렬로 정방행렬이면 그대로 즉 각각의 대각선 요소들만 역수를 취해주게 되면, 바로 $\Sigma$의 pseudo-inverse를 구할 수 있게 됩니다. 이러한 방식으로 Singular Value Decomposition을 활용해서 Pseudo-inverse를 구하는 데에도 사용이 가능하다고 말할 수 있습니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 2주차 2-4 선형 변환]]></title>
    <url>%2F2020%2F10%2F22%2F2019-03-31-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-2-Weak-2-4%2F</url>
    <content type="text"><![CDATA[서울과학기술대학교 김진현교수님의 K-MOOC강좌 Robot Manipulator and Underwater Robot Application을 공부하고 내용을 정리한 것 입니다. K-MOOC 강좌 주소: http://www.kmooc.kr/courses/course-v1:SEOULTECHk+SMOOC03k+2019_T3/about 선형 변환(Linear Transformation)선형 변환(Linear Transformation)alt|$x$라고 되어 있는 어떠한 공간에서, $A$라는 행렬을 통해서 $y$라는 공간으로 변환된다, 라는 개념으로 어떤 매트릭스 연산을 설명할 수 있습니다.| 이런 개념이 필요한 이유는, 이어서 기구학에서 살펴볼, 조인트 스페이스와 태스크 스페이스, 즉 관절 공간과 작업 공간에서의 서로간의 속도 변환, 그리고 서로간의 위치 변환 같은 것들을 설명하기 위해서는, 이런 선형변환이라는 것에 대해서 여러분들이 익숙해질 필요가 있습니다. range space of $A$: 어떠한 매트릭스 $A$가 존재하고, 그 매트릭스 $A$에, Column으로 이루어져 있는, 이 Column들을 가지고 만들 수 있는, Span할 수 있는 공간 전체를 range space라고 합니다.(Column space) 이 레인지 스페이스 이외의 공간, 어떻게 보면 이렇게 만들어진 2차원 공간이라면, 이 2차원 공간과 수직하는 공간을, 레인지 스페이스의 직교공간이다, 라고 말할 수 있습니다. Range of space를 벗어나 있는 직교공간으로는 어떠한 수를 쓰더라도 이 $A$라는 매트릭스를 통해서는 갈 수가 없게 됩니다. Null space of $A$: 선형 방정식 $Ax=b$에서 $b$가 zero vector(=Null vector, =0벡터)일때 식을 만족시키는 모든 가능한 해 $x$에 대한 집합이다. 다시 말하면 선형방정식 $Ax=0$의 해들이 이루는 공간, Null Space를 의미한다. Matrix Normsalt p-norm: $Ax=y$임을 이용해 매트릭스가 아닌 벡터로 놈을 정의함 $sup$(상한): 가장 큰 값을 찾는 연산 F-norm(frobenius norm): 각각의 요소들을 전부 제곱을 한후 square root를 취한 norm(L2 norm형태) 의사역행렬(Pseudo-inverse)alt|두개의 벡터가 차원이 다를 경우라도 변환은 존재할 수 있는데, 이런 정방행렬이 아닐 경우에는 역행렬이 존재하지 않기 때문에 어떤 역으로 오는 변환하는 설명이 조금 어려울 것입니다.| alt 의사역행렬(Pseudo-inverse): $A$가 정방행렬이 아닌 경우에 대해서 정의할 수 있는 역행렬의 개념으로 실제 역행렬은 아니지만, 이것은 가상의 역행렬이다. alt 만약에 m이 n보다 작은 경우, 즉 그런 경우에 full row rank, 즉 row 방향으로 Rank가 전부 다 꽉 차 있을 경우에는, 오른쪽의 pseudo-inverse가 존재하고 오른쪽에만 곱했을 때, identity matrix를 만들 수 있는 성질을 가지고 있다.(right pseudo-inverse) 그런데, 그 중에서 Moore-Penrose pseudo-inverse라는 방법으로 정의되어 있는 역행렬이 가장 널리 사용되고, 가장 많이 사용되고 있는 역행렬, 이 pseudo-inverse에 계산하는 방법이 되겠습니다. 각각 $A$,$A^T$는 각각의 $A$의 Rank와 $A^T$의 Rank는 똑같이 같아지고 2개를 곱했기 떄문에 역시 Rank는 같습니다.그리고 full row rank이기 때문에, 당연히 역행렬이 존재할 수가 있게된다. 반대로, m이 n보다 큰 경우에는 즉 full column rank를 가진 경우에는, 이번에는 왼쪽에 곱했을 경우에만, $I$를 만들 수 있는 left Pseudo-inverse가 존재합니다. Pseudo-inverse의 경우 표기를 A^# $A^+$ A^십자가모양등 다양하게 사용된다. alt 의사역행렬을 이용한 역변환: 어떤 $y=Ax$라고 하는 변환을 생각을 할 수가 있는데, 이런 $y=Ax$에서, 그렇다면 이번에는 $x$라는 것을 거꾸로, $A$라는 어떤 Pseudo-inverse를 통해서, $y$라는 것을 통해서 역변환을 구할 경우 단순하게 $x=A^+y$라고만 하면 되는 게 아니라, 거기에 추가적으로 이와 같은 Null space로 프로젝션시키는 여분의 텀이 존재해야만 정확하게 표현이 가능해집니다. alt $A^+=B$라고하면 이 B라는 행렬에 대한 널 스페이스, 그리고 널 스페이스와 직교하는 스페이스, 그 다음에 이 $B$라는 행렬에 대한 Range space, 그 다음에 $B$라는 행렬에 대한 Range space에 직교하는 행렬, 직교하는 스페이스, 이렇게 4가지의 공간이 존재하게 되겠고요, 그래서 결과적으로는 이 Range space, $A^+$의 Null space에 존재하는 놈, 또는 앞서서 연산했던 것과 마찬가지로, 결국에는 Null space에 존재하기 때문에, 전부 다 0으로 수렴하게 됩니다. 그리고 이, $A^+$의 Null space에 존재하는, 이 Null space에 존재하지 않는,즉 이 Null space와 직교하는 공간에 있는 이러한 인자들은 전부 다 이 $A$라는 $A^+$의 Range space로 전부 다 매칭이 되는 거죠.그렇다면, 역시 마찬가지로 이 공간을 만들어낼 수 없다는 문제가 생기게 됩니다.원하는 것은 이 $x$라는 것이 $A$라는 것을 타고, $y$로 갔다가, 다시 $y$를 통해서, $x$로 복구가 되는 과정을 원하는 것이고, $x$ 전체를 만들어내는 것이 우리의 목표인데,이러한 역변환을 통해서 이 공간, 즉 앞서서 Null space, $A$의 Null space와 직교하는 공간에 있는 점들을 전부 다 만들어 낼 수가 있는데,Null space 상의, 즉 $A$의 Null space에 존재하는 것들을, 공간을 만들어낼 수가 없게 됩니다. alt 그래서, 이러한 공간을 만들어내기 위해서, 이와 같이 $I-A^+A$라고 하는, 가상의 새로운 프로젝션(projection)시키는, 변환을 만들어내게 됩니다.이 변환은, 여기에서는 유의하실 게, $x$에서 $x$로 가는 변환입니다. $y$에서 $x$로 가는 변환이 아닌, $x$에서 $x$로 가는 변환인데, 이 $x$에서 $x$로 가는 변환은, 즉 모든 $x$를 전부 다 Null space로 보내는 변환이 되겠습니다.그래서 $x$상에 있는 그 모든 원소들을 전부 다 이 Null space로 보내주고 직교하는 것들은 이 Null space에 직교하는 공간들은 전부 0으로 보내는, 변환이 바로 $I-A^+A$라고 하는, 변환이 되겠고요,이러한 변환을 통해서 앞서 페이지에서 만들어내고 싶었던, $A$라고 하는 것의 Null space에 직교하는 공간을, 만들어낼 수가 없었는데, 이 공간을 이러한 변환을 통해서 만들어낼 수가 있게 되는 겁니다.그래서 조금 전에 말씀드렸던 것처럼, 선형변환을 통해서, $x$라는 것을 만들어내기 위해서 $A^+$에 $y$를 곱해주고,그 다음에 $(I-A^+A)k$, 이때 $k$는 $x$하고 같은 크기의 임의의 어떤 벡터가 되겠고요, 이러한 변환을 통해서 온전하게 $x$를 다시 변환을 통해서 만들어낼 수가 있게 되는 겁니다. 이 개념이 나중에 기구학에서, 여러분들이 관절공간과, 그리고 작업 공간을 왔다갔다 하는 데에 있어서 꼭 필요한 개념이 되겠습니다.]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS285 Fa19 Introduction to Reinforcement Learning]]></title>
    <url>%2F2020%2F03%2F08%2F2020-02-25-cs285-4%2F</url>
    <content type="text"><![CDATA[CS 285 at UC Berkeley을 보고 정리한 글입니다. Definitions 강화학습의 목적은 $\theta$를 찾는 것이 목적이다. state는 markov property를 만족하고 observation은 만족하지 않는다. $\pi_{\theta} \mathbf (a_t|o_t)$는 partially observed라고 하고 $\pi_{\theta} \mathbf (a_t|s_t)$ fully observed라 한다. POMDP(Partially observable Markov decision process)와 Fully observed MDP가 있는데 일반적으로 Fully observed MDP로 가정한다. behavior cloning 전문가로부터 data(observation,action)을 수집해서 supervised learning을 사용한다. 일반적으로 behavior cloning은 잘 안되지만 DAgger를 사용하면 성능이 향상된다. reward function $\mathbf r(s,a)$(reward function): 어떤 행동이 좋고, 어떤 행동이 나쁜가?에 대한 기준 $s,{\mathbf a},r(s,a),p(s’|s,a)$ (transition probability)로 이뤄진 것을 Markov decision process라 한다. Markov chain $\mathcal{M} = {\mathcal{S,T}}$ 아직 RL은 아니다.(no agency,no action) $\mathcal{S}$ - state space states $s \in \mathcal{S}$(discrete or continuous) $\mathcal{S}$: set of valid state $s$:state $\mathcal{T}$ - Transition operator $p (s_{t+1}|s_t)$ why “operator”? Discrete space에서 $\mathcal{T}$ linear operator처럼 쓰이기 때문 let $\mu_{t,i} = p(s_t = i) \vec{\mu_t}$ is a vector of probabilites $\mu_{t,i}$: time step $t$에서 주어진 state $i$일 확률 let $\mathcal{T}_ {i,j} = p(s_{t+1} = i|s_t = j)$ then $\vec{\mu_{t+1}} = \mathcal{T}\vec{\mu_t}$ $\mathcal{T}_{i,j}$ -&gt; $N\times N$ matrix($N$:가능한 state 개수) 다음 step의 state 확률을 $\mathcal{T}$와 현재 step의 state 확률의 곱으로 구할 수 있다. $\vec{\mu_{t+1}} = \mathcal{T}\vec{\mu_t}$이 linear operator 처럼 행동 stationary distribuition 때 중요 Markov decision process $\mathcal{M} = {\mathcal{S,A,T,r}}$ $\mathcal{S}$ - state space states $s \in \mathcal{S}$(discrete or continuous) $\mathcal{A}$ - action space actions $a \in \mathcal{A}$(discrete or continuous) $\mathcal{O}$ - observation space observations $o \in \mathcal{O}$(discrete or continuous) $\mathcal{T}$ - Transition operator(Tensor!) action과 state를 가지고 있기에 Tensor이다.($p (s_{t+1}|s_t,a_t)$) let $\mu_{t,j} = p(s_t = j)$ let $\xi_{t,k} = p(a_t = k)$ let $\mathcal{T}_ {i,j,k} = p(s_{t+1} = i|s_t=j,a_t=k)$ $\mu_{t+1,i} = \sum_{j,k} \mathcal{T}_ {i,j,k}\mu_{t,j}\xi_{t,k}$(linear operator) $r$ - reward function $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$(scalar field) $r(s_t,a_t)$ - reward Partially observed Markov decision process $\mathcal{M} = {\mathcal{S,A,O,T,\varepsilon,r}}$ $\mathcal{S}$ - state space states $s \in \mathcal{S}$(discrete or continuous) $\mathcal{A}$ - action space actions $a \in \mathcal{A}$(discrete or continuous) $\mathcal{O}$ - observation space observations $o \in \mathcal{O}$(discrete or continuous) $\varepsilon$ - emission probability(operator) $p(o_t|s_t)$ $r$ -reward function $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ The goal of reinforcement learning Goal: reward를 최대로하는 policy 찾기(learn $\theta$) 항상 policy가 explicit 하지 않지만 지금은 explicit $p_\theta(s_1,{\mathbf a_1},\dots,s_T,{\mathbf a_T}) = p(s_1) \prod_{t=1}^T\pi_\theta({\mathbf a_t}|s_t)p(s_{t+1}|s_t,{\mathbf a_t})$ finite horizon으로 가정 $p_\theta(s_1,{\mathbf a_1},\dots,s_T,{\mathbf a_T}) = p_\theta(\mathcal{T})$ (joint probability distribution, $\mathcal{T}$는 trajectory) $s_{t+1}$는 보통 모른다. $\theta^\star = \arg \max_{\theta}E_{\mathcal{T}\sim p_\theta(\mathcal{T})}[\sum_t r(s_t,{\mathbf a_t})]$ 우리는 모든 time step마다 reward가 최대가 되길 원한다. 그래서 모든 time step에 대해 reward를 더한다. $s_t,{\mathbf a_t}$는 $\mathcal{T}\sim p_\theta(\mathcal{T})$에 대해 uncertain(randomly distributed)하기에 기댓값을 사용($\sum_{x-p(x)}[f(x)]=\int p(x)f(x)dx)$) 이 식은 policy의 목적이기에 action을 알려주지 않아서 기댓값은 가능한 모든 state와 action의 sequence를 고려해야한다. $\theta = \arg \max_{\theta}E_{\mathcal{T}\sim p_\theta(\mathcal{T})}[\sum_t r(s_t,{\mathbf a_t})]$ distribution이 MDP를 만족하지 않아도 $\pi_\theta({\mathbf a}|s)$가 정해지면 $s$와 $\mathbf a$의 합쳐짐으로 얻어진 augmented state space에서 markov chain으로 볼수있다. $p((s_{t+1},\mathbf{a_{t+1}})|(s_t,\mathbf{a_t}))) = p(s_{t+1}|s_t,\mathbf{a_t})\pi_\theta(\mathbf{a_{t+1}}|s_{t+1})$ Finite horizon case: state-action marginal $\theta^\star = \arg \max_{\theta}E_{\mathcal{T}\sim p_\theta(\mathcal{T})}[\sum_t r(s_t,{\mathbf a_t})]$ $\mathcal{T}$를 $s$와 $a$의 pair로 변경 $p_\theta(s_t,\mathbf{a}_t)$(state-action marginal) time step $t$에서의 state와 action의 marginal distribution Infinite horizon case: stationary distribution what if $T = \infty$? $p(s_t,\mathbf{a_t})$가 stationary distribution으로 수렴 한다면(항상은 아니지만 일반적으로 수렴) $\begin{pmatrix} s_{t+1} \ a_{t+1} \end{pmatrix} = \mathcal{T}^k \begin{pmatrix} s_{t} \ a_{t} \end{pmatrix}$(state-action transition operator)을 이용해서 $\mu = \mathcal{T}\mu$로 나타낼 수 있다. $\mu$: vector of probabilities state action pair $\mu = \begin{bmatrix} p(s_1) \cr p(s_2) \cr \vdots \cr p(s_N) \end{bmatrix} \ \ \text{(regular markov chain) } \mu = \begin{bmatrix} p(s_1,a_1) \cr p(s_1,a_2) \cr p(s_1,a_M) \cr p(s_2,a_1) \cr \vdots \end{bmatrix} \ \ ( \text{markov chain over} s \ \ \text{and} \ \ a)$ not change distribution $\ne$ not change state $(\mathcal{T} - I)\mu = 0$ $\mu$는 $\mathcal{T}$의 eigenvalue가 1인 eigenvector이다. 특정 조건(ergodicity - 모든 state action pair가 reachable)을 만족하면 수렴한다. $\theta^\star = \arg \max_{\theta} \frac{1}{T} \sum^T_{t=1}E_(s_t,\mathbf{a_t})\sim p_\theta(s_t,\mathbf{a_t})[r(s_t,\mathbf{a_t})] \rightarrow E_{(s,\mathbf{a})\sim p_\theta(s,\mathbf{a})}[r(s,\mathbf{a})]$ $T$가 무한대으로 가면 이전 공식이 무한대로 가기 때문에 $\frac{1}{T}$을 추가함(undiscounted average return) $\sum$이 $E$에 압도되어 결국 $E_{(s,\mathbf{a})\sim p_\theta(s,\mathbf{a})}[r(s,\mathbf{a})]$이 된다. RL에서 목적은 주로 expectation만을 고려한다. 그림처럼 도로위에 있으면 reward +1 이고 그외에는 -1이라 하자 reward function은 not smooth, not differentiable하다. 하지만 expectation을 추가하면 $\theta$ ($pi_\theta(\mathbf{a}=fall)=\theta$, distribution) 안에서는 smooth 해진다.($\theta * (-1) + (1-\theta)*1$이므로) AlgorithmsThe anatomy of RL algorithm ( sample 생성(즉 policy대로 agent 행동, collect data) ) ( Fit a model/ estimate the return(evaluation step, policy를 바꾸지 않고 평가한다.) ) ( improve the policy(policy를 업데이트한다.) ) ( 위 그림에서 trajectories ) ( evaluate reward function(나쁜결과는 probability 낮춘다)) ( $\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta)$(gradient descent) ) Model based RL ( $s_t$와 $\mathbf a_t$가 input으로 받고 output으로 $s_{t+1}$를 출력하는 $f_{\phi}$를 학습한다.(fitting neural net) ) ( $\pi_\theta(s_t)$를 학습 시키기 위해 $f_{\phi}$와 $r$을 통해서 backprop한다. ) Which parts are expensive? ( 실제 로봇이나 차등은 현실 시간과 동일하므로 오래 걸리지만 MuJoCo같은 시뮬레이션은 빠르다. ) ( reward를 계산하는 것은 빠르지만 neural net을 사용하면 느리다.(알고리즘에 따라 다르다.) ) ( gradient step같은경우 빠르지만 backprop은 느리다.(알고리즘에 따라 다르다.) ) How do we deal with all these expectations? $E_{\mathcal{T}\sim p_\theta(\mathcal{T})}[\sum_t r(s_t,\mathbf a_t)]$ $ E_{s_1 \sim p(s_1)} [E_{\mathbf{a}_1\sim \pi (\mathbf {a} _1|s_1)} [r(s_1,{\mathbf a}_1)+\ |s_1]] $ state $s_1$에서 action $a_1$을 한다. $ E_{s_{1} \sim p\left(s_{1}\right)}\left[E_ {\mathbf{a}_ {1} \sim \pi\left(\mathbf{a}_ {1} | \mathbf{s}_ {1}\right)}\left[r\left(\mathbf{s}_ {1}, \mathbf{a}_ {1}\right)+E_{\mathbf{s}_ {2} \sim p\left(\mathbf{s}_ {2} | \mathbf{s}_ {1}, \mathbf{a}_ {1}\right)}[ | \mathbf{s}_ {1}, \mathbf{a}_ {1}\right] | \mathbf{s}_{1}]\right] $ transition probability distribution $p(s_2|s_1,\mathbf{a_1})$에 따라 state $s_2$로 가고 action $a_2$를 한다. $ E_{s_{1} \sim p\left(s_{1}\right)}\left[E_ {\mathbf{a}_ {1} \sim \pi\left(\mathbf{a}_ {1} | \mathbf{s}_ {1}\right)}\left[r\left(\mathbf{s}_ {1}, \mathbf{a}_ {1}\right)+E_{\mathbf{s}_ {2} \sim p\left(\mathbf{s}_ {2} | \mathbf{s}_ {1}, \mathbf{a}_ {1}\right)}\left[E_{\mathbf{a}_ {2} \sim \pi\left(\mathbf{a}_ {2} | \mathbf{s}_ {2}\right)}\left[r\left(\mathbf{s}_ {2}, \mathbf{a}_ {2}\right)+\ldots | \mathbf{s}_ {2}\right] | \mathbf{s}_ {1}, \mathbf{a}_ {1}\right] | \mathbf{s}_{1}\right]\right] $ expectation이 많지만 markov property때문에 각각 1,2개의 condition만을 가진다.(convenient 해진다.) ex) $E_ {\mathbf{a}_ 2 \sim \pi (\mathbf{a}_ {2} | \mathbf{s}_{2})}$는 $s_2$만을 condition으로 받음 $ r (\mathbf{s}_ {1}, \mathbf{a}_ {1} )+E_{\mathbf{s}_ {2} \sim p (\mathbf{s}_ {2} | \mathbf{s}_ {1}, \mathbf{a}_ {1} )} [E_{\mathbf{a}_ {2} \sim \pi (\mathbf{a}_ {2} | \mathbf{s}_ {2} )} [r (\mathbf{s}_ {2}, \mathbf{a}_ {2} )+\ldots | \mathbf{s}_ {2} ] | \mathbf{s}_ {1}, \mathbf{a}_ {1} ] | \mathbf{s}_{1} ]]$를 안다면? time step $T$까지 기다릴 필요 없이 바로 policy를 앞으로의 보상에 맞게 학습할 수 있을 것이다.(drastically simplify) $Q(s_1,\mathbf{a_1}) = r(\mathbf{s}_ {1}, \mathbf{a}_ {1})+E_{\mathbf{s}_ {2} \sim p(\mathbf{s}_ {2} | \mathbf{s}_ {1}, \mathbf{a}_ {1})}[E_{\mathbf{a}_ {2} \sim \pi(\mathbf{a}_ {2} | \mathbf{s}_ {2})}[r(\mathbf{s}_ {2}, \mathbf{a}_ {2})+\ldots | \mathbf{s}_ {2}] | \mathbf{s}_ {1}, \mathbf{a}_ {1}]$ $E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^{T} r\left(\mathbf{s}_ {t}, \mathbf{a}_ {t}\right)\right]=E_{\mathbf{s}_ {1} \sim p\left(\mathbf{s}_ {1}\right)}\left[E_{\mathbf{a}_ {1} \sim \pi\left(\mathbf{a}_ {1} | \mathbf{s}_ {1}\right)}\left[Q\left(\mathbf{s}_ {1}, \mathbf{a}_ {1}\right) | \mathbf{s}_{1}\right]\right]$ 만약 모든 $s$, $a$에 대해서 $Q(s,a)$를 안다면 $\pi$를 쉽게 바꿀 수 있다. ex) 만약 ${\mathbf a_1} = \arg \max_{\mathbf a_1} Q(s_1,{\mathbf a_1})이면 $\pi\theta (\mathbf{a} _1|s_1) = 1$ 이다.(나머지 확률이 0이라) $Q$도 $\pi$에 영향을 받지만 지금은 고려하지 않는다.(나중에 트릭으로 해결) Q-function and value function Q-function(quality function): $s_t$에서 $\mathbf a_t$를 했을때 앞으로 받을 전체 reward의 합 $Q^{\pi}(s_t,{\mathbf a_t}) = \sum\nolimits^T_{t’=t} E_{\pi_\theta}[r(s_{t’},{\mathbf a}_{t’})|s_t,{\mathbf a_t}]$ value function: $s_t$로부터 앞으로 받을 전체 reward의 합 $V^{\pi}(s_t) = \sum \nolimits_{t’=t}^{T} {E}_ {\pi_{\theta}} [ r(s_{t’},{\mathbf a}_{t’}) | s_t]$ $V^{\pi}\left(\mathbf{s}_ {t}\right)=E_{\mathbf{a}_ {t} \sim \pi\left(\mathbf{a}_ {t} | \mathbf{s}_ {t}\right)}\left[Q^{\pi}\left(\mathbf{s}_ {t}, \mathbf{a}_{t}\right)\right]$ Idea1: $\pi$와 $Q^\pi(s,{\mathbf a})$를 안다면 $\pi$를 improve 할수있다. 만약 ${\mathbf a} = \arg \max_{\mathbf a} Q^\pi(s,{\mathbf a})$라면 ${\pi}’({\mathbf a}|s) = 1$으로 맞춘다. 그러면 ${\pi}’$는 적어도 $\pi$보다 좋거나 같아진다. Idea2: good action $\mathbf a$의 확률 증가시키기 위해 gradient 계산 만약 $Q^\pi(s,{\mathbf a}) &gt; V^{\pi}(s)$면 $\mathbf a$는 평균보다 더 좋다.($V^{\pi}(s_t) = \sum\nolimits_{t’=t}^{T} {E}_ {\pi_{\theta}} [ r(s_{t’},{\mathbf a}_{t’}) | s_t]$이기 때문) $Q^\pi(s,{\mathbf a}) &gt; V^{\pi}(s)$인 $\mathbf a$의 확률을 크게하게 $\pi({\mathbf a}|s)$로 바꾼다.(actor critic) Types of RL algorithms Objective: $\theta^\star = \arg \max_{\theta}E_{\mathcal{T}\sim p_\theta(\mathcal{T})}[\underset{t}\sum r(s_t,\mathbf a_t)]$ Policy gradients: 위 objective를 집접적으로 미분 Value-based: optimal policy의 Q-function or value function을 추정(no explicit policy) Actor-critic: 현재 policy의 Q-function or value function을 추정, policy를 향상시키기 위해 사용(combination Policy gradients and Value-based) Model-based RL: transition model 추정, 그리고 planning을 위해 모델사용(no explicit policy) policy 향상을 위해 사용 Something else plan하기 위해 모델 사용(no explicit policy) trajectory 최적화/ 최적 제어(우선적으로 continuous space) - 필수적으로 action을 최적화하기 위해 backpropagation discrete action space에서 discrete planning - e.g., Monte Carlo tree search Backpropagate gradient into the policy 동작하기 위해 몇가지 트릭 필요 Value function을 배우기 위해 모델 사용 Dynamic programming Generate simulated experience for model-free learner(Dyna) TradeoffsWhy so many RL algorithms Different tradeoffs Sample efficiency(sample collection time) stability &amp; ease of use(reliable) Different assumptions Stochastic or deterministic? Continuous or discrete? Episodic or infinite horizon? Different things are easy or hard in different settings Easier to represent the policy?(enviornment의 물리법칙이 복잡하지만 optimal behavior의 패턴은 단순) Easier to represent the model?(ex. chess) sample efficiency Sample efficiency = 좋은 정책을 얻기 위해서 얼마나 많은 샘플이 필요한가? 가장 중요한 질문: Off policy 알고리즘 인가? Off policy: policy에서부터 새로운 샘플 생성없이 policy를 향상할 수 있다. On policy: policy가 조금이라도 변할 때마다 새로운 sample을 생성해야 한다. 왜 덜 efficient한 알고리즘을 사용하는가? simulation power가 적게 들면, less efficient하더라도 더 나은 성능을 위해 많은 sample을 얻어서 학습하는 것이 효율적일 수 있다. stability and ease of use 수렴하는지, 어디에 수렴하는지, 항상 수렴하는지에 대한 이슈다. supervised learning에서는 gradient descent를 사용하기 때문에 대개 수렴하지만, RL에서는 대체로 gradient descent를 사용하지 않는다. Q-learning: fixed point iteration. converge가 보장되지 않는다. Model-based RL: model이 reward에 대해 최적화되는 것이 아니다. Policy gradient: gradient descent 보통 least efficient .(but stability) Value function fitting 가장 좋을때, error of fit을 최소화한다.(Bellman error) expected reward와 같지 않다.(better fit for value function $\ne$ better policy) 최악의 경우, 최적화 되지않는다. 많은 유명한 deep RL value fitting 알고리즘은 비선형의 경우 converge한다고 장담할 수 없다. 직접적으로 expected reward를 최적화 하지않는다. Model-based RL model이 error of fit를 최소화한다. 수렴한다. better mode = better policy라 보장할 수 없다. Policy gradient 유일하게 실제로 true objective를 gradient descent(ascent)를 사용한다. assumptions full observability 일방적으로 value function fitting에서 가정된다. recurrence를 추가하여 완화할 수 있다. episodic learning 주로 pure policy gradient에서 가정된다. 몇가지 model-based RL에서 가정된다. continuity or smoothness 몇가지 continuous value function learning 방법에서 가정된다. 몇가지 model-based RL에서 가정된다.]]></content>
      <categories>
        <category>lecture</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Double DQN]]></title>
    <url>%2F2020%2F03%2F08%2F2020-02-25-DQN%2F</url>
    <content type="text"><![CDATA[Double DQN기존의 Q-Learning Algorithm은 특정 조건에서 Q-value를 Overestimate(과평가)되어 좋은 성능을 보이지 못했다. 보상받을 값에 따라서 다음 행동이 결정되는데, 잘못된 행동에 대한 Q-value가 과평가되면 잘못된 방향으로 학습이 진행될 수 있다. Q-Learning은 estimated된 Q-value에 대한 maximization step이 존재하여 비현실적인 Q-value를 통해 학습하게 된다. 기존의 Q-Learning은 어떤 상태($s’$)에서 취할 수 있는 행동들 중에서 maximum Q-value를 구하는 것을 목표로 학습을 진행한다. 물론 학습을 하다보면 부정확한 value로 estimate되는 것은 당연하다. 만약 모든 값들이 균등하게 over-estimate된다면 이것은 문제가 되지 않으며, 심지어 원하는 값에 대해 overestimate되는 것은 때로 좋은 결과를 불러일으킨다. 그러나 overestimate가 일정하게 되지 않고, 우리가 학습하기를 원하는 값에서 발생되지 않는다면 Policy에 안좋은 영향을 미치게 된다. Double Q-learning Q-Learning은 Action Select와 Action Evaluate을 위해 같은 $\theta$를 사용 -&gt; Overestimation 발생$$ Y_t^Q = R_{t+1} + \gamma max_{a} Q (s_{t+1}, a; \theta_{t}) $$ 위의 식을 select와 evaluation을 명확히 하기 위해 아래와 같이 변형할 수 있다.$$ Y_t^Q = R_{t+1} + \gamma Q(S_{t+1}, argmax_{a} Q (s_{t+1}, a; \theta_{t}); \theta_t) $$ 그리고 Double Q-Learning에서는 아래와 같이 selection과 evaluation을 분리하여 표현한다.$$ Y_t^Q = R_{t+1} + \gamma Q(S_{t+1}, argmax_{a} Q (s_{t+1}, a; \theta_{t}); \theta’_t) $$ $\theta_t$는 argmax에서 action을 선택하고,$\theta’_t$는 policy를 evaluate하는데 2개의 weights는 서로 switching하며 update된다. Double DQN DQN에서도 Overestimation이 존재하기에 식을 위와같이 변경 Result $Q_*$: true value(보라선), $Q_t$: single action에 대한 approximation(초록선), sampled states(초록 점들) 첫번째와 두번째 그래프는 sampled states도 맞추지 못했는데 이는 estimation function의 차수가 낮아 충분히 flexible 하지 않기 때문이다. 세번쨰 그래프는 충분히 flexible 하지만 unsampled states에서의 정확성은 떨어진다. 특히 왼쪽에 sampled states 거리가 다른 states보다 멀어서 더 큰 estimation error를 만들었다. 이렇게 특정 순간에 우리가 제한적인 데이터만을 지니는 것이 실제 학습할 때의 환경과 같고 Estimation Error는 실제 학습을 진행할 때 자주 발생하게 된다. 초록색 그래프: 모든 행동에 대한 estimated action-value, 검은색 그래프: 초록색 그래프중 maximum값 max를 취했을 때 Overestimation이 발생한는 것을 볼 수 있다. 주황색 그래프: Maximum Estimation Function – True Function파란색 그래프: Double Q Estimation Function – True Function 주황색 그래프의 경우 upward bias을 지니고 있어 항상 양수이다. 파란색 그래프의 경우 평균적으로 거의 0에 근접하다. 첫번째 열과 두번째 열은 True Value Function만을 다르게 한 것 -&gt; Overestimation이 특정 구조에 종속된 것이 아니라는 것 두번째 열과 세번째 열은 Function approximation의 flexibility가 다름 row2는 flexibility가 낮아서 True Value에서도 정확한 값을 갖지 않음 row3는 flexibility가 높지만 주어진 True Value거리가 먼 경우에서 정확한 값을 갖지 않음 이렇게 시작되는 Overestimation은 계속해서 propagate되고, 상황은 계속해서 악화됨 DQN의 경우 Overestimation이 발생하면서 Score가 하락함 Dueling DQN 위 stream은 State-value function으로서 scalar 값을 받게 됩니다. 즉, State s가 얼마나 좋냐를 측정하게 되는데 이는 Environment내에서 상대적인 값으로 봐야한다. ex) 7.8이니까 좋다, 라고 말할 수 없고, 이전 상태에서는 5이고 지금 상태는 7이니 이전 상태보다 state value가 높다 라고 해석해야 한다. 아래 stream은 advantage function로 특정 action이 평균 action의 value보다 얼마나 좋은지, 나쁜지를 판단할 수 있는 함수이며 state에 dependent하다. 마지막 부분은 $V(s) + A(s, a)$로 계산이 됩니다. $V(s)$는 Scalar이고 $A(s, a)$의 action에 대한 vector인데, $V(s) + A(s, a)$를 계산할때에 $V(s)$ 를 action수 만큼 동일하게 복사하여 더해준다. 하지만 이렇게 하면 identifiability 문제가 발생한다.($Q(s,a)$가 주어질떄 $V(s)$와 $A(s,a)$를 찾지 못함, back propagation하는데 문제 발생) 이 문제의 해결방법은 $A$를 0으로 만들면 되서 위와 같이 식을 바꾸면 된다. agent가 앞으로의 reward를 얻기 위해 어디에 관심을 두는지를 표시한 그림 위 그림을 보면 지평선 끝에 차 한대가 작게 있는 것을 볼 수 있다. 먼저, value stream 관점에서는 앞으로의 reward를 최대화 하기 위해서 agent는 score와 멀리 있는 차(장애물) 그리고 차를 피하기위해 가야할 길에 집중한다. advantage stream 관점에서는 집중하지 않는다. 왜냐하면 당장에 action을 취하지 않아도 어차피 차는 나아가고 score는 쌓이고 있기 때문이다. 아래 그림을 보면 차와 다른 차들이 매우 가깝다. 그렇기 때문에 value stream 입장에서는 우리가 계속 score를 쌓기 위해 장애물들을 피해 나아가야할 곳에 집중한다. advantage stream을 보면 당장에 action을 취하지 않으면 reward를 얻는데 영향을 주기 때문에 이때는 앞에 있는 차에 신경을 써서 action을 취해야한다. normal DQN에서는 agent가 state에서 각 action에 대한 Q-value를 계산해야 하지만 value function이 좋지 않을 때, 즉, state가 dangerous하거나 reward를 얻기에 좋지 않은 state에서도 Q-value를 계산해봐야하고, state에서 할 수 있는 모든 action들이 나쁜결과를 초래하더라도 계산해봐야한다. 이렇게 Q-learning을 두 가지 stream으로 dualing하면, agent가 state마다 action을 해보면서 그 결과를 학습하지 않아도 어떤 state가 valuable한지 알 수 있다. 참고 자료 https://mangkyu.tistory.com/66 https://jsideas.net/dqn/ https://whereisend.tistory.com/137?category=783755 https://sumniya.tistory.com/19 https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/ https://towardsdatascience.com/dueling-deep-q-networks-81ffab672751]]></content>
      <categories>
        <category>theory</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS285 Fa19 Deep Reinforcement Learning, Decision Making, and Control]]></title>
    <url>%2F2020%2F02%2F25%2F2020-02-25-cs285-1%2F</url>
    <content type="text"><![CDATA[CS 285 at UC Berkeley을 보고 정리한 글입니다. Terminology &amp; notation image classification에서는 input으로 image로 받고 output으로 class이지만 순차적 의사결정 문제(sequential decision making problem)에서는 아래첨자에 time step $t$가 추가 되고 output이 $\bf a_t$(action)이 된다. 그리고 순차적 의사결정 문제는 action이 observation($\bf o_t$)에 영향을 끼친다. $\pi_{\theta} \bf (a_t|o_t)$(policy)은 $o_t$가 주어졌을 때 $a_t$의 확률분포이다. observation 과 state의 차이observation($o_t$)는 input 이미지로 볼 수 있고 state($\bf s_t$)는 치타의 상태와 가젤의 상태(environment의 모든 정보)라 할 수 있다. 그래서 위 그림같이 이미지에서 자동차가 치타를 가리면 observation에서는 치타의 정보를 알 수 없지만 state는 알 수 있다.($\pi_{\theta} \bf (a_t|o_t)$는 partially observed라고 하고 $\pi_{\theta} \bf (a_t|s_t)$ fully observed라 한다.) $\bf p(s_{t+1}|s_t,a_t)$: state $\bf s_t$에서 action $\bf a_t$를 취했을때 $\bf s_{t+1}$로 갈 확률 이러한 성질은 markov chain에서 보면 위 그림과 같다. 여기서 $\bf s_t$에서 $\bf s_{t+1}$로 갈때 $\bf s_{t-1}$은 이와 독립적이다.(future은 past에 독립) 이를 Markov property라고 한다. 위와 같은 그래프가 형성되기 위해서는 state가 필요한데 그 이유는 observation은 markov property를 만족하지 않기 때문이다. ex) 위 치타와 차 예시를 보면 $\bf o_{t-1}$를 차가 치타를 가리기 전 $\bf o_t$를 차가 치타를 가린 경우 $\bf o_{t+1}$를 차가 지나간 후라면 $\bf o_{t+1}$는 $o_{t-1}$의 정보를 알지 못하면 알 수 없기에 markov property를 따르지 않는다. 가끔 state를 $\scr{x_t}$로 action을 $\bf u_t$로 나타내기도 한다. Imitation Learning 사람(전문가)으로부터 observation $\bf o_t$와 action $\bf a_t$를 모아서 $\pi_\theta \bf (a_{t}|o_{t})$를 supervised learning 하는 것을 behavior cloning이라 한다. 검은선: training data의 trajectory 빨간선: 학습시킨 $\pi_{\theta}$의 trajectory 일반적으로 이러한 방법은 잘 안되는데 그 이유는 supervised learning이 정확하게 training data와 100% 똑같지 않기때문에 초반에 mistake가 일어나고 그 결과 모르는 trajectory로 가게 되고 계속해서 mistake가 커지기 때문이다. 문제 개선 End to End Learning for Self-Driving Cars 에서 위에서 나온 문제를 해결하기 위해 차에 3개의 카메라를 달아서 해결하였다. 왼쪽 카메라는 오른쪽 핸들로 bias를 오른쪽 카메라는 왼쪽 핸들로 bias를 주어 상호보완이 가능하게 하여 학습이 더 잘되게 하였다. 하지만 이 방법은 자율주행에서만 가능하다. trajectory의 distribution 안정적이게 되었다. $\bf p_{\text {data}}(o_t)$: training data(observation)의 distribution$\bf p_{\pi_\theta}(o_t)$: $\pi_\theta$를 따랐을때 나온 observation 의 distribution 실제 training data는 $\bf p_{\text {data}}(o_t)$에서 sampling된 것으로 볼 수 있다.여기서 $o_t$는 서로 독립적이지 않지만, supervised learning을 이용하기에 independent가 된다. 여기서 문제는 $\bf p_{\text {data}}(o_t)$와 $\bf p_{\pi_\theta}(o_t)$가 달라서 mistake가 생기게 된다는 것이다. 그렇다면 $\bf p_{\text {data}}(o_t) = p_{\pi_\theta}(o_t)$로 강제로 만들어 주면 어떻게 될까? DAgger 여기서 우리는 policy를 data만큼 바꾸지 않을 것이다. DAgger의 idea는 training datg를 $\bf p_{\text {data}}(o_t)$가 아닌 $\bf p_{\pi_\theta}(o_t)$에서 모으는 것이다. human data로 $\bf \pi_\theta (a_t|o_t)$를 학습 시킨다. $\bf \pi_\theta (a_t|o_t)$로 새로운 데이터($o_t$)를 얻는다. 사람이 새로운 데이터에 라벨($\bf a_t$)을 달아준다. 새로 얻은 데이터를 training data에 합친다. 1-4를 계속 반복한다. 하지만 사람이 label을 정의 해주기 힘들고 너무나 많은 노동력이 든다는 문제점이 생긴다. 만약 모델이 drift가 생기지 않을 정도로 학습이 잘되고 overfit 되지 않는다면 성능이 올라갈 것이다. 하지만 그렇게는 잘되지 않는다. 실패 원인Non-Markovian behavior $\bf \pi_\theta (a_t|o_t)$는 현재 observation에만 의존하여 행동하지만, 실제 사람은 과거의 observation도 고려하기에 문제가 발생하였다. 위 문제의 해결방안중 하나는 RNN이다. Multimodal behavior 파란색 바그래프는 discrete case를 나타낸 것이고 밑에는 continuous 정규분포(Maximum Likelihood)로 나타낸 것이고 이는 적용되지않는다. 위 나무 그림처럼 같은 상황에서도 다양한 행동(왼쪽, 오른쪽)이 가능하다. 이러한 문제는 Output Mixture of Gaussians, Latent variable model, Autoregressive discretization으로 해결할 수 있다. Output mixture of Gaussians action을 선택할 때 한 개의 Gaussian에서 뽑는 것이 아닌 여려 개의 Gaussian을 이용하는 방법이다.하지만 이 방법은 n개의 action만 사용 가능 하므로 action이 많을수록 사용하기 힘들다는 단점이 있다. latent variable models 이 방법은 input에 노이즈를 추가해서 딥러닝 모델에 학습시킨다. 원하는 대로 표현이 가능하지만, 훈련이 어렵다는 단점이 있다. autoregressive discretization softmax를 통해 discrete한 확률분포를 뽑아내고 분포에서 sampling하여 또 다른 네트워크에 입력으로 넣어주고 다시 확률분포를 뽑아준다. 이 과정을 n번 반복하여 n개의 dimension을 출력으로 뽑아내게 된다.이 방법은 만약 작은 차원의 action space를 가지고 있고 action space가 continuous이라도 discrete하게 바꿀 수 있다. 하지만 high dimential에서는 practical 하지 않다. Imitation learning: recap Behavior cloning은 종종 이 방법만으로 불충분하다. 왜냐하면, distribution mismatch problem이 발생하기 때문이다. 하지만 다음과 같은 방법을 쓰면 잘 작동한다. Hacks (e.g 자율주행에서 카메라 3개를 쓴 방법) 안정적인 trajectory distribution으로부터 sampling on-policy data를 추가한다. (e.g Dagger 사용) 더 정확하게 모델링 한다. Imitation learning의 한계 사람이 data를 제공해야 한다. 딥러닝은 data가 많을 때 잘된다. 사람이 특정 action을 정해주기 힘든 경우도 있다. 사람은 자동으로 배우는데 기계는 그렇지 못한다. 스스로 경험으로부터 무제한 데이터가 필요 연속적으로 스스로 improvement 해야 한다. learning without imitation $\delta$: 사자한테 먹히면 1 아니면 0 사자예시로 다시 돌아가보면 우리는 사자에게 먹히지 않는 것이 목표이다.(위 식의 기댓값을 minimize) 우리는 오늘도 내일도 모레도 먹히는 것을 원하지 않기에 위와 같이 state와 action의 sequence로 식을 바꿀 수 있다. 이 식은 강화학습 문제와 비슷하게 된다. cost function: 얼마나 나쁜 결과를 했는지를 나타낸다.(minimize) reward function: 얼마나 잘했는지를 나타낸다(maximize) cost function for imitation learning imitation learning에서 reward function은 $\bf r(s,a) = \log p(\bf a = \pi ^\star (s)|s)$($\bf p(\bf a = \pi ^\star (s)|s)$은 action과 optimal policy가 같을 확률)으로 나타내고 cost function은 위 식과 같이 $\bf c(s,a)$로 나타내고 이는 틀린 횟수로 볼 수 있다. Distribution mismatch analysis distribution mismatch 문제에서 time 축을 time step $\bf T$로 정의하고, cost function을 $\bf r(s,a) = \log p(a=\pi^\star (s) | s)$로 log likelyhood loss를 사용해도 되지만 간단하게 분석하기 위해서 zero-one loss를 사용한다. $$\bf c(s,a) = \begin{cases}0 &amp; \ \ \text{ if } \ a = \pi ^\star (s) \ \cr1 &amp; \ \ \ \text{otherwise.}\end{cases} - (8) $$ tightrope로 예시를 들면, 위 grid 그림처럼 나타낼 수 있다.(빨간색이 낭떠러지) 학습이 어느정도 되서 모든$\bf s \in \mathit{D_{train}}$에 대해 $\pi_{\theta}(a \ne \pi^\star (s) | s) \leq \epsilon$이라 가정하자. 학습데이터의 모든 state에서 사람과 다른 행동(줄에서 떨어짐)을 할 확률이 $\epsilon$ 보다 작다는 것을 말한다($\epsilon$을 mistake할 확률으로 볼 수 있고 worst case이다.). $\epsilon$은 training method에 따라 값이 달라지고 method가 좋을수록 값이 작아진다. 이때 total cost 기댓값의 upper bound는 다음과 같다. $$\bf \mathbb{E} \left[\ \mathsf{\underset{t}{\sum}}\ c(s_{t},a_{t})\right] \leq \epsilon T + (1-\epsilon)(\epsilon(T-1) + (1-\epsilon)(…))$$ 첫번째 step에서 mistake했을때 cost의 기댓값은 $\bf \epsilon \bf T$(mistake를 하면 그 이후 step에서는 계속 mistake하므로)이고 첫번째는 잘하고 두번째 step에서 mistake했을때 cost의 기댓값은 $\bf (1-\epsilon)\epsilon(T-1)$이고 $\bf T$ step만큼의 항이 나온다. 그리고 각가의 항이 $\bf O(\epsilon T)$(Order of T)이기 때문에 $\bf \mathbb{E} [\ \mathsf{\underset{t}{\Sigma}}\ c(s_{t},a_{t})]$의 bound는 $O(\epsilon T^2)$가 된다. 하지만 $O(\epsilon T^2)$는 좋지 않다. 왜냐하면 $\epsilon$이 아무리 작아도 $T$가 길어질수록 cost가 커지기 때문이다. 위에서 했던 가정 “모든 $\bf s \in \mathit{D_{train}}$”은 trained state만 고려하기에 적절한 가정은아니다(image classification에서 train과 test를 똑같은 사진을 쓰는 것과 같음, generalization 문제). 그래서 가정을 $\bf s \sim p_{train}(s)$으로 바꾸자. 먼저 DAgger를 적용했을 때 $p_{train}(s) \rightarrow p_\theta(s)$($p_{train}(s) = p_\theta(s)$)이 된다. 이때 total cost 기댓값의 upper bound는 $$\bf \mathbb{E} \left[\ \mathsf{\underset{t}{\sum}}\ c(s_{t},a_{t})\right] \leq \epsilon T$$이된다.(매 step마다 mistake할 확률이 $\epsilon$으로 동일하기 때문) 이제 DAgger를 쓰지않고 $\bf p_{train}(s) \ne p_{\theta}(s)$(train data의 상태분포와 trained된 상태분포가 다름)라 가정하자. $$ \bf p_{\theta}(s_{t}) = (1-\epsilon)^t p_{train}(s_{t}) + (1-(1-\epsilon)^t)p_{mistake}(s_t)$$ 여기서 $(1-\epsilon)^t$는 mistake를 하지 않을 확률이고 $\bf p_{mistake}(s_{t})$는 mistake했을때 확률분포이다. $$ \bf | p_{\theta}(s_{t}) - p_{train}(s_{t})| = \underset{s_{t}}{\sum} | p_{\theta}(s_{t}) -p_{train}(s_{t}) $$ total variation divergence between $p_\theta(s_t)$ and $p_{train}(s_t)$ $$\bf p_{\theta}(s_{t}) = (1-\epsilon)^t p_{train}(s_{t}) + (1-(1-\epsilon)^t)p_{mistake}(s_t)$$ $$\bf p_{train}(s_t) = (1-\epsilon)^t p_{train} + (1-(1-\epsilon)^t)p_{train}$$ 이므로 $\bf \left| p_{\theta}(s_{t}) - p_{train}(s_{t}) \right| = (1-(1-\epsilon)^t) \left| p_{mistake}(s_{t})-p_{train}(s_{t}) \right|$이된다. $$\bf \left| p_{\theta}(s_{t}) - p_{train}(s_{t}) \right| = (1-(1-\epsilon)^t) \left| p_{mistake}(s_{t})-p_{train}(s_{t}) \right| \leq 2(1-(1-\epsilon)^t) \leq 2\epsilon t$$ 두 확률분포의 차이의 절댓값($\bf \left| p_{mistake}(s_{t})-p_{train}(s_{t}) \right|$)의 최댓값은 2이고 $\bf -(1-\epsilon)^t \leq -(1-\epsilon t)$ for $\epsilon \in \left[0,1\right]$이므로 $\bf \left| p_{mistake}(s_{t})-p_{train}(s_{t}) \right|$의 upper bound는 $2\epsilon t$이 된다. cost의 기댓값은 아래와 같이 전개된다. $$\bf \underset{t}{\sum} \mathbb{E_{p_{\theta}(s_{t})}} \left[ c_{t} \right] = \underset{t}{\sum}\underset{s_{t}}{\sum} p_{\theta}(s_{t})c_{t}(s_{t})$$ $$\bf p_\theta c = p_{train}c + (p_\theta - p_{train})c \ \ \ \because \bf p_\theta = p_{train} + p_\theta - p_{train}$$ $$\bf \leq p_{train}c + |p_\theta-p_{train}|c \leq p_{train}c + |p_\theta-p_{train}|c_{max}$$ $$\bf p_\theta = p_{train} + p_\theta - p_{train}$$ $$\bf \underset{t}{\sum} \mathbb{E_{p_{\theta}(s_{t})}} \left[ c_{t} \right] = \underset{t}{\sum}\underset{s_{t}}{\sum} p_{\theta}(s_{t})c_{t}(s_{t}) \leq \underset{t}{\sum}\underset{s_{t}}{\sum} p_{train}(s_{t})c_{t}(s_{t}) + \left| p_{\theta}(s_{t}) -p_{train}(s_{t})\right|c_{max}$$ 여기서 $\bf c_{max}$(worst possible cost)는 cost definition에 의해 1이고 $p_{train}$의 cost 기댓값 $\bf \underset{s_{t}}{\sum}p_{train} (s_{t})c_{t}(s_{t})$은 $\epsilon$이기때문에 아래와 같이 전개된다. $$\bf \underset{t}{\sum} \mathbb{E_{p_{\theta}(s_{t})}} \left[ c_{t} \right] \leq \underset{t}{\sum}\epsilon + 2 \epsilon t \leq \epsilon T + 2 \epsilon T^{2}$$ 결국 $\bf O(\epsilon T^{2})$이 된다.]]></content>
      <categories>
        <category>lecture</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRML 1 Introduction]]></title>
    <url>%2F2019%2F08%2F31%2F2019-08-31-PRML-1%2F</url>
    <content type="text"><![CDATA[0.Introduction패턴인식(Pattern Recognition) &amp; 머신러닝(Machine Learning) Pattern Recognition이란 computer 알고리즘을 사용하여 데이터에 내재된 패턴을 자동으로 찾는 것을 의미한다. 아래와 그림과 같이 필기체를 분류(classification)하는 문제가 있다고 하자 $\bf x$:숫자의 이미지 $\bf y$: 숫자의 class 다음과 같은 문제는 Rule-based(heuristics)한 방법으로 해결할 수 있지만 실제로 이러한 방법은 예외사항을 계속해서 만들어야 하므로 안 좋은 결과를 만들게 된다. 더 좋은 방법은 training set을 사용하여 model의 parameters를 조절하는 machine learning을 사용하는 것이다. machine learning의 결과는 $\bf x$를 input으로 받고 output으로 $\bf y$를 내보내는 $y({\bf x})$로 표현할 수 있다. 이 함수는 training data를 기반으로 training(learing)을 거치며 원하는 함수에 가까워 진다. training된 모델로 test set 이라 부르는 새로운 데이터의 결과를 알 수 있습니다. 일반화(Generalization) training 동안 사용되지 않은 data를 올바르게 분류하는 능력을 generalization이라 한다. generalization은 pattern recognition의 주된 목적이다. 전처리(Pre-Process) 대부분의 상황에서 입력데이터를 문제를 더 쉽게 해결하기 위해서 전처리(pre-process)라 한다. 이러한 전처리과정은 test data도 train data에서 한 것과 똑같이 적용해야 한다. 전처리 과정은 data의 목적은 연산 속도를 높이고 데이터를 더 쉽게 다루기 위해서 이다. 전처리 과정에서 정보를 잃게되는데 그로인해 전체적인 성능이 저하되지 않도록 조심해야한다. Machine Learning의 종류지도학습(Supervised Learning) Problem input vectors와 상응하는 target vectors가 있는 문제 분류(classification) target vectors가 유한개의 discrete categories인 문제 예) 필기체 숫자 분류 회귀(regression) target vectors가 continuous 변수인 문제 예) 주식 가격 예측 비지도학습(Unsupervised Learning) Problem target vectors없이 input vectors만 존재하는 문제 클러스터링(clustering) 데이터내에서 비슷한 속성을 가지는 그룹을 찾는 문제 밀도 추청(density estimation) 입력 데이터로 변수의 분포를 도출하는 문제 (변수 $\neq$ 입력데이터) 시각화(visualization) 고차원 공간의 데이터를 2 또는 3 차원 공간에 투영하는 보여주는 것 강화학습(reinforcement learning) 주어진 상황에서 reward를 최대화 하기위한 적절한 actions를 찾는 문제 강화학습은 지도학습처럼 optimal한 outputs이 주어지지 않고 trial and error를 통해서 학습한다. 대부분의 경우, 현재의 action은 즉각적인 reward 뿐만 아니라 이후의 reward에도 영향을 끼칩니다. 어떤 action은 좋은 결과를 어떤 action은 덜 좋은 결과를 불러오는데 이러한 것을 정확히 알지 못하는 문제를 신뢰 활당 문제(credit assignment problem)이라 한다. 강화학습의 특징 중 하나는 탐색(exploration, 효율적인 새로운 행동을 찾는 것)과 활용(exploitation, 높은 보상을 주는 쪽으로 행동하는 것)의 trade-off 문제를 해결하는 것이다. 이 책에서는 다루지 않는다. 1. Example: Polynomial Curve FittingPolynomial Curve Fitting 0과 1사이에서 10개의 샘플을 뽑아 입력 변수를 $x$라고 하자. 초록색 선은 $\sin (2\pi x)$ 이다. 우리의 목표는 새로운 입렵 변수의 타겟 값 $t$를 올바르게 예측하는 것이다. 위 그림에서 주어진 데이터들은 랜덤하게 noise가 더해져 있다.(푸른색 동그라미가 초록색선위에 있지 않다.) 이 문제를 간단한 curve fitting 방식으로 접근해보자 $$ y(x,{\bf w})=w_0+w_1x+w_2x^2+…+w_Mx^M=\sum_{j=0}^{M}w_jx^{j} \qquad{(1.1)} $$ $M$: 다항식의 차수 $y(x,{\bf w})$가 $x$에 대하여 비선형이지만 계수 ${\bf w}$에 대해서는 선형 함수이다. 다항 함수와 같이 알려지지 않는 변수 ${\bf w}$에 대해 선형인 함수를 선형 모델(linear models)라 불린다. (Chapters 3와 4에 자세히 다룸) error function 계수 ${\bf w}$는 training data를 통해 다항 함수를 fitting함으로써 결정된다. 이러한 방법은 error function(함수 $y(x,{\bf w})$와 target 변수$t$의 차이)를 최소화 함으로써 가능하다. 간단한 error function의 예로 오차의 제곱합이 있다. $$ E({\bf w})=\dfrac{1}{2}\sum_{n=1}^{N}{y(x_n,{\bf w})-t_n}^2 \qquad{(1.2)} $$ $\frac{1}{2}$는 계산의 편의를 위해서 사용됨 주어진 error function이 이차형식(quadratic)의 함수 꼴이므로 최소화 하는 값은 유일 해를 가지게 됨을 보장 받는다. 결과적으로 얻어지는 함수 값은 $y(x, {\bf w}^*)$이다. ${\bf w}^*$는 유일 해를 가질때 ${\bf w}$이다. Model Comparison(Model Selection) 다항식의 차수 $M$를 고르는 문제가 있는데 이를 model comparison 또는 model selection이라 한다. $M=0$,$M=1$일때 training data와 목표인 $\sin (2\pi x)$모두 fit하지 않는데 이런 경우를 under-fitting이라 한다. $M = 9$일때 training data와 가장 잘 fit하지만 $\sin (2\pi x)$와는 fit 하지 않는 것을 확인할 수 있다. 이런 경우를 over-fitting이라 한다. 우리의 목표는 새로운 데이터에 대해서 정확한 예측하는 좋은 일반화(generalization) 모델을 찾는 것이다.]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[공업 수학 7강]]></title>
    <url>%2F2019%2F08%2F15%2F2019-08-15-engineering-mathematics%2F</url>
    <content type="text"><![CDATA[7.1 행렬, 벡터: 합과 스칼라곱행렬(matrix) 수(혹은 함수)를 직사각형 모양으로 괄호 안에 배열한 것(사각행렬) 원소(Entry) 또는 요소(Element): 행렬에 배열되는 수(혹은 함수) 행(Row) : 수평선 열(Column) : 수직선 일반적인 표기법과 개념$$ {\bf A} = [a_{jk}] =\begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \cra_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \cr\vdots &amp; \vdots &amp; \ddots &amp; \vdots \cra_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\end{bmatrix} : m \times n 행렬 $$ 행렬은 볼드체의 대문자를 사용한다. 첫 번째 아래 첨자 $j$는 행(Row) 두 번째 아래 첨자 $k$는 열(Column) $a_{jk}$: $j$ 행, $k$ 열의 원소(Element) $m \times n$은 행렬의 크기(size)를 나태는 것이다. 정방행렬(square matrix) $m=n$ 이라면 A는 정사각형 모양이다 정방행렬에서 원소 $a_{11}, a_{22}, \cdots , a_{nn}$을 포함하는 대각선을 행렬 $A$ 의 주대각선(Principal Diagonal)이라고 한다 $ {\bf A} = [a_{jk}] =\begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \cra_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \cr\cdot &amp; \cdot &amp; \cdots &amp; \cdot \cra_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}\end{bmatrix}$ 계수행렬(coefficient matrix) 선형연립방정식의 계수 만으로 이루어진 행렬 *통상, 선형 연립방정식을 $\bf Ax = b$ 로 나타낼 때, 행렬 A를 말한다.ex) $ \bf \widetilde A =\begin{bmatrix}4 &amp; 6 &amp; 9 \cr6 &amp; 0 &amp; -2 \cr5 &amp; -8 &amp; 1\end{bmatrix}$ 첨가행렬(augmented matrix) 계수행렬 및 우변 상수항을 모두 포함한 행렬 계수행렬 및 우변 상수항을 모두 포함한 행렬 각 행이 선형연립방정식의 하나의 식과 대응되는 행렬 ex) $ \bf \widetilde A =\begin{bmatrix}4 &amp; 6 &amp; 9 &amp; 6 \cr6 &amp; 0 &amp; -2 &amp; 20 \cr5 &amp; -8 &amp; 1 &amp; 10\end{bmatrix}$ 벡터(Vector) 단 하나의 행, 또는 하나의 열만으로 이루어진 행렬 벡터의 성분을 성분(component)라 한다. 벡터는 소문자 볼드체 문자 $\bf a, b, c,\cdots,$ 또는 $\bf a = [a_j]$로 표현한다. 행벡터(Row Vector) 하나의 행으로 구성된 행렬 $$ {\bf a} = \begin{bmatrix} a_1, a_2, \cdots, a_n \end{bmatrix}$$ 열벡터(Column Vector) 하나의 열로 구성된 행렬 $$ {\bf b} = \begin{bmatrix} b_1\cr b_2\cr \vdots \cr b_n \end{bmatrix} $$ 행렬의 상등(Equality of Matrices) 두 행렬 A와 B의 크기가 같고 대응하는 성분들이 모두 같을 경우 ($A = B$) 행렬의 합(Matrix Addition) 같은 크기의 행렬에 대해서만 정의되고 그 합은 대응하는 원소를 각각 합함으로 얻어진다. 스칼라곱(Scalar Multiplication) 행렬의 각 원소에 상수(Scalar)를 곱(Product)하여 얻어진다. 행렬의 가법과 스칼라곱에 대한 연산법칙 7.2 행렬의 곱행렬과 행렬의 곲(Matrix Multiplication) $r \times p$행렬 $B = [b_{jk}]$의 행수 $r$와 $m \times n$행렬 $A = [a_{jk}]$의 열수 $n$가 서로 같아야 정의되며 $$c_{jk} = \sum_{j=1}^n a_{jl}b_{lk} = a_{j1}b_{1k} +a_{j2}b_{2k} + \cdots + a_{jn}b_{nk}$$를 원소로하는 $m\times p$ 행렬로 정의된다.($AB$는 정의되지만 BA는 정의되지 않을 수 있다.) 행렬의 곱은 비가환적(Not Commutative)이다. 즉 $AB \neq BA$ 행렬의 곱에 대한 연산 법칙$$ (kA)B = k(AB) = A(kB)$$$$ A(BC) = (AB)C\ \ \ \ \ \text{(결합법칙(Associative Law))}$$$$ (A+B)C = AC + BC\ \ \text{(분배법칙(Distrivutive Law))}$$$$ C(A+B) = CA + CB\ \ \text{(분배법칙(Distrivutive Law)} $$ 행렬과 벡터의 전치(Transposition of Matrices) 열과 행이 서로 바뀌어 얻어진 행렬 $$ {\bf A^T} = [a_{kj}] =\begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{m1} \cra_{21} &amp; a_{22} &amp; \cdots &amp; a_{m2} \cr\vdots &amp; \vdots &amp; \ddots &amp; \vdots \cra_{1n} &amp; a_{2n} &amp; \cdots &amp; a_{mn}\end{bmatrix}$$ 정방행렬에 대한 전치는 주대각선에 관하여 대칭으로 위치된 원소들을 서로 바꾼 것이다. 전치 연산에 대한 법칙$$ (A^T)^T =A $$$$ (A + B)^T = A^T +B^T $$$$ (cA)^T = cA^T $$$$ (AB)^T = B^TA^T $$ 특수한 행렬(Special Matrices)대칭행렬(Symmetric Matrix) 전치가 본래의 행렬과 같은 정방행렬 ($A^T = A$) 반대칭행렬(Skew- symmetric Matrix) 전치가 본래의 행렬의 음이 되는 정방행렬 ($A^T = -A$) 삼각행렬(Triangular Matrix) 위삼각행렬(Upper Triangular Matrix) 주대각선을 포함하여 그 위쪽으로만 0이 아닌 원소를 갖는 정방행렬 아래삼각행렬(Lower Triangular Matrix) 주대각선을 포함하여 그 아래쪽으로만 0이 아닌 원소를 갖는 정방행렬 대각행렬(Diagonal Matrix) 주대각선 상에서만 0이 아닌 원소를 가질 수 있는 정방행렬 스칼라 행렬(Scalar Matrix) 주대각선 원소들이 모두 같은 대각행렬 단위행렬(Unit 또는 Identity Matrix) 주대각선 원소들이 모두 1은 대각행렬 7.3 선형연립방정식, Gauss 소거법선형연립방정식(=선형계(linear systems)) 제차연립방정식(Homogeneous Simultaneous System) $b_j$가 모두 0인 경우 비제차연립방정식(Nonhomogeneous Simultaneous System) $b_j$중 적어도 하나는 0이 아닌 경우 선형연립방정식의 행렬표현 : $\bf Ax = b$ 계수행렬(Coefficient Matrix) : $\bf A$ 해벡터(Solution Vector) :$\bf x$ 첨가행렬(Augmented matrix) : 계수행렬 $\bf A$에 열벡터 $\bf b$를 첨가한 행렬 가우스 소거법과 후치환(Gauss Elimination and Back Substitution) $x_1$을 소거 : 첫 번째 식에 두 배 한 후, 이를 두 번째 식에 더한다. 후치환(Back Substitution) :$x_2, x_1$ 순으로 해를 구한다. 마지막 방정식에서 해를 구한 후, 그 결과를 역순으로 첫째 방정식에 대입하여 정리한다. 기본행연산. 행동치 연립방정식(Elementary Row Operations. Row-Equivalent Systems) 기본 행연산을 이용하여 미지수를 하나씩 소거하여 대각선 아래의 계수를 0으로 만든다 행동치(Row-Equivalent) 선형시스템 $S_1$이 선형시스템 $S_2$에 유한번의 기본행연산을 가하여 얻어질 수 있다면 $S_1$을 $S_2$의 행돋치라 한다. 행동치 연립방정식(Row-Equivalent Systems) 행동치 연립방정식들은 같은 해집합을 갖는다. 지금까지 행연산에 국하여 다루었고 열연산의 경우 해집합에 영향을 미칠 수 있다. 미지수보다 더 많은 방정식을 가진다면 과잉한정(overdetermined) 미지수보다 적은 방정식을 가지면 과소한정(underdetermined) 행사다리꼴(Row Echelon Form)과 행 사다리꼴로부터의 정보행사다리꼴 Gauss 소거법의 마지막 단계에서 보는 계수행렬과 첨가행렬의 형태와 이에 대응하는 연립방정식각 행에서 가장 왼편에 있는 0아닌 성분을 모두 1로 만든 것을 축소된 사다리꼴(reduced echelon form)이라 한다. 3가지 가능한 경우 정확하게 하나의 해가 존재한다. : $r=n$이고 $\widetilde{b_{r+1}}, \cdots, \widetilde{b_m}$이 모두 0이다. 무한히 많은 해가 존재한다. : $r &lt; n$이고 $\widetilde{b_{r+1}}, \cdots, \widetilde{b_m}$이 모두 0이다. 해가 없다. : $r &lt; m$이고 $\widetilde{b_{r+1}}, \cdots, \widetilde{b_m}$중 하나라도 0이 아니다.(모순이 없는 경우, $r = m$이거나$, $r &lt; m$이더라도 $\widetilde{b_{r+1}}, \cdots, \widetilde{b_m}$가 모두 0이라면 해가 존재한다.) 7.4 일차 독립. 행렬의 계수. 벡터공간벡터의 일차 독립과 종속성 일차 독립(Linearly Independent) : 모든 $c_j = 0$ 일 때만 위 식이 만족 일차 종속(Linearly Dependent) : 어떤 $c_j \neq 0$ 이어도 위 식이 만족 행렬의 계수(Rank) 행렬에서 1차독립인 행벡터의 최대수이며 rank($\bf A$)라 표시 행동치인 행렬 행동치인 행렬들은 같은 계수를 갖는다. 일차종속성과 일차독립성 각각 $\bf n$개의 성분을 갖는 $\bf p$개의 벡터들은 이 벡터들을 행벡터로 취하여 구성된 행렬의 계수가 $\bf p$이면 일차독립이고, 그 계수가 $\bf p$보다 작으면 일차종속이다. 열벡터에 의한 계수 행렬의 계수는 행렬의 일차독립인 열벡터의 최대수와 같다.=&gt; 행렬과 행렬의 전치는 같은 계수를 갖는다 벡터의 일차종속 $\bf n$개의 성분을 갖는 벡터가 $\bf p$개 있고 $\bf n &lt; p$ 라면 이들 벡터들은 항상 일차종속이다. 벡터공간 (Vector Space) 공집합이 아닌 벡터의 집합에 속해 있는 임의의 두 원소에 대하여, 이들의 일차결합이 다시 집합의 원소가 되며 다음 법칙을 만족하는 벡터들의 집합 차원(Dimension) 벡터공간내의 일차독립인 벡터들의 최대수이며 $\bf \dim(V)$로 표기 기저(Basis) 벡터공간내의 최대로 가능한 수의 일차독립인 벡터로 구성되는 부분집합이며 기저가 되는 벡터의 수는 차원과 같다. 생성공간(Span) 성분의 수가 같은 벡터들에 관한 일차결합으로 표환되는 모든 벡터들의 집합 부분공간(Subspace 벡터공간에서 정의된 벡터합과 스칼라곱에 관하여 닫혀있는 부분집합 $\bf R^n$ 벡터공간 $\bf n$개의 성분을 갖는 모든 벡터들로 이루어진 벡터공간 $\bf R^n$의 차원 $n$이다. 행공간(Row Space) 행벡터들의 생성공간 열공간(Column Space) 열벡터들의 생성공간 행공간과 열공간 행렬의 행공간과 열공간은 차원이 같고, 행렬의 계수와도 동일하다. 영공간(Null Space) $\bf Ax = 0$의 해집합 퇴화차수(Nullity) 영공간의 차원 $\bf A$의 계수 + $\bf A$의 퇴화차수 = 행렬 $\bf A$의 행 갯수 7.5 선형연립방정식의 해 : 존재성, 유일성선형연립방정식에 대한 기본정리존재성(Existence) 선형연립방정식이 모순이 없기 위한(Consistent), 다시 말해서 해를 갖기 위한, 필요충분조건은 계수행렬과 첨가행렬이 같은 계수를 갖는 것이다. 유일성(Uniqueness) 선형연립방정식이 유일한 해를 갖기 위한 필요충분조건은 계수행렬과 첨가행렬이 같은 계수를 갖는 것이다 무수히 많은 해(Infinitely Many Solutions) 계수행렬과 첨가행렬이 같은 계수$\bf r$을 갖고 $\bf r &lt; n$이면, 무수히 많은 해가 존재한다. Gauss 소거법(Gauss Elimination) 해가 존재하면 Gauss 소거법에 의해 모두 구해질 수 있다. 제차연립방정식 제차연립방정식은 항상 자명한 해(Trivial Solution)을 갖는다. 자명하지 않은 해가 존재할 필요충분조건 : $\bf r &lt; n$ (계수행렬의 계수= $\bf r = \text{rank} A$, 미지수의 갯수 = $ \bf n$) $\bf r &lt; n$이면 해공간은 $\bf n-r$차원 벡터공간이다. 제차연립방정식의 두 해벡터의 일차결합도 제차연립방정식의 해이다. 미지수보다 방정식의 수가 적은 제차 선형연립방정식 방정식의 수가 미지수의 수보다 적은 제차연립방정식은 항상 자명하지 않은 해(Nontrivial Solution)를 갖는다. 비제차연립방정식 만약 비제차 연립방정식이 해를 갖는다면 모든 해는$$\bf x = x_0 + x_h$$와 같은 형태가 된다.$\bf x_0$은 고정된 임의의 해이고 $\bf x_h$는 대응하는 제차연립방정식의 모든 해를 대표한다. 7.6 참고사항 : 2차 및 3차 행렬식2차 행렬식(Determinant of Second Order) 선형연립방정식 3차 행렬식(Determinant of Third Order) 선형연립방정식 7.7 행렬식. Cramer의 법칙$\bf n$차 행렬식(Determinant of Third Order) 소행렬식(Minor): $M_{jk}$ 여인수(Cofactor): $C_{jk}$ 기본행연산항(Elementary Row Operation)에서의 $\bf n$차 행렬식의 변화 (a) 두 행을 바꾸는 것은 행렬식의 값에 -1을 곱하는 것이다 (b) 한 행의 상수배를 다른 행에 더하는 것은 행렬식의 값에 변화를 주지 않는다. (c) 한 행에 0이 아닌 $c$를 곱하면 행렬식의 값이 $c$배가 된다. $\bf n$차 행렬식의 추가적인 성질 (a)-(c)는 열에 대해서도 성립한다. (d) 전치(Transposition)는 행렬식의 값에 변화를 주지 않는다. (e) 0행 또는 0열은 행렬식의 값을 0으로 만든다. (f) 두 행이나 두 열이 비례관계에 있으면 행렬식의 값은 0이다. 특히 같은 두 행이나 두 열을 가진 행렬식의 값은 0이다. 행렬식과 계수 $m \times n$ 행렬 $A=[a_{jk}]$가 계수 $r(\geq 1)$을 갖기 위한 필요충분조건은(1) $\bf A$가, 0이 아닌 행렬식을 갖는 $r \times r$ 부분행렬을 가지고,(2) $\bf A$의 $(r+1) \times (r+1)$ 또는 그보다 큰 크기의 모든 정방 부분행렬(그런 부분행렬이 존재할 경우)의 행렬식이 0이 되는 것이다.특히, $\bf A$가 정방행렬 $ n \times n$ 정방행렬일 때, 계수가 $n$일 필요충분조건은(3) $\text{det}A \neq 0$이다. Cramer의 정리(행렬식에 의한 선형연립방정식의 해) $D \neq 0이면 그 연립방정식은 오직 하나의 해를 갖는다. $D_k$는 $D$의 $k$번째열을 $b_1, \cdots, b_n$을 성분으로 하는 열벡터로 대치하여 얻은 행렬식이다. 따라서 위 식이 제차이고 $D \neq 0$이면, 그것은 오직 자명한 해 $x_1 = 0, x_2 = 0, \cdots, x_n = 0$만을 갖는다. 만약 $D = 0$이면, 제차연립방정식은 자명한 해가 아닌 해도 갖는다. 7.8 역행렬. Gauss-Jordan 소거법역행렬(Inverse Matrix) 정칙행렬(Nonsingular Matrix) : 역행렬을 갖는 경우 특이행렬(Singular Matrix) : 역행렬을 갖지 않는 경우 역행렬을 가지면 그 역행렬은 유일하다 역행렬의 존재성 $\bf A$가 $\bf n \times n$행렬일 때, 역행렬$A^{-1}$이 존재할 필요 충분조건은 $\text{rank}A = n$이다.($\bf \text{det}A \neq 0$도 같은 조건이다) Gauss-Jordan 소거법에 의한 역행렬의 결정 행렬식에 의한 역행렬 공식 여인수 $C_{jk}$가 놓인 위치는, 행렬 $\bf A$의 성분 $a_{jk}$가 놓인 자리가 아니라, 성분 $a_{kj}$가 있는 자리이다. 대각행렬의 역행렬 대각행렬 ${\bf A} =[a_{jk}](즉, $j \neq k$일 때 $a_{jk} = 0$)가 역행렬을 가질 필요충분조건은 주대각선 상의 모든 성분 $a_{jj}$가 0이 아니어야 한다. $$ 이 경우 \bf A^{-1}은 \frac 1 a_{11}, \cdots, \frac 1 a_{nn}들이 대각원소인 대각행렬이 된다.$$ 두 행렬의 곱 $\bf AC$의 역행렬$$\bf(AC)^{-1} = C^{-1}A^{-1}$$$$\bf (AC\cdotsPQ)^{-1} = Q^{-1}P^{-1}\cdots C^{-1}A^{-1}$$ 역행렬의 역행렬$$\bf (A^{-1})^{-1} = A$$ 행렬의 곱에 대한 특이 성질, 소거법 행렬의 곱은 교환법칙이 성립하지 않는다.(일반적으로 성립하지 않는다.) $$ \bf AB \neq BA$$ $\bf AB = 0$일 때 $\bf A = 0$ 또는 $\bf B = 0$이 아닐 수도 있다. $$\begin{bmatrix}1 &amp; 1 \cr2 &amp; 2\end{bmatrix}\begin{bmatrix}-1 &amp; 1 \cr1 &amp; -1\end{bmatrix} =\begin{bmatrix}0 &amp; 0 \cr0 &amp; 0\end{bmatrix}$$ $\bf AC = AD$일 떄 $C \neq D$일 수도 있다.(심지어 $A \neq 0$ 일 때에도)) 소거법칙 $\bf A,B,C$를 $n \times n$ 행렬이라 하자. (a) $\text{rank} A = n$이고 $\bf AB = AC$이면, $B = C$이다. (b) $\text{rank} A = n$이면 $\bf AB = 0$은 $\bf B = 0$을 의미한다. 그러므로 $\bf AB = 0$이면서 $\bf A \neq 0$이고 동시에 $\bf B \neq 0$이면, $\text{rank} {\bf A} &lt; n$이고 $\text{rank} {\bf B} &lt; n$이다. $\bf A$가 특이행렬이면, $\bf BA$와 $\bf AB$도 특이행렬이다. 행렬곱의 행렬식 $\bf \det (AB) = \det (BA) = \det A \det B 7.9 벡터공간, 내적공간, 일차변환실벡터공간(Real Vector Space) 성분 $\bf a,b,\cdots$을 갖는 공집합이 아닌 집합 $V$에 대하여 “벡터의 덧셈”과 “스칼라 곱”이라고 하는 두 가지 대수학적 연산법칙이 다음과 같이 정의되어 있으면, 이 집합 $V$를 실벡터공간(real vector space, 또는 실선형 공간(real linear space))이라 부르고 $V$의 성분을 벡터라 부른다. 벡터의 덧셈: $\bf a + b$ 가환성(Commutativity) $\bf a+b = b+a$ 결합성(Associativity) $\bf (u+v) + w = u + (v+w)$ 영벡터(Zero Vector) $\bf a+0=a, a+(-a)=0$ 스칼라곱: $k{\bf a}$ 분배성(Distributivity) $c({\bf a+b}) = c{\bf a} + c{\bf b}$ 분배성(Distributivity) $(c+k){\bf a} = c{\bf a} + k{\bf a}$ 결합성(Associativity) $c(k{\bf a}) = (ck){\bf a}, 1{\bf a} = {\bf a}$ 실내적공간(Real Inner Product Space) 실벡터공간 $V$에 속한 임의의 한 쌍의 벡터 $\bf a,b$에 대하여 하나의 실수를 대응시키는 규칙이 존재하여 다음 공리를 만족한다면, $V$를 실내적공간(real inner product space)이라 부른다. 내적(Inner Product)$$ (a,b) = a \cdot b $$ 선형선 $q_1{\bf a} + q_2{\bf b,c} = q_1({\bf a,c}) + q_2({\bf b,c})$ 대칭성 $\bf (a,b) = (b,a)$ 양의정치성(Positive-definiteness) $\begin{cases}(a,a) \geq 0, \cr(a,a) = 0 \text{일 필요충분조건은} \ \ a = 0\end{cases}$ 직교(Orthogonal) 내적이 영인 두 벡터 벡터의 길이 또는 노름(norm)$$\bf \lVert a \rVert = \sqrt{(a,a)} (\geq 0)$$ $n$차원 Euclid 공간$$\bf (a,b) = a^Tb = a_1b_1 + \cdots + a_nb_n$$ 위 식과 같이 약속된 내적이 정의되었다고 하면, 이 때 이 공간을 $n$차원 Euclid 공간이라 부르고 $E^n$(또는 $R^n$)이라 표기한다. Euclid 노름(Euclidean norm) 단위벡터(Unit Vector) 길이가 1인 벡터 Cauchy-Schwarz 부등식$$\bf \lvert (a,b) \rVert \geq \lVert a \rVert \lVert b \rVert $$ 삼각부등식$$\bf \lVert a+b \rVert \geq \lVert a \rVert + \lVert b \rVert $$ 평행사변형 등식$$\bf {\lVert a+b \rVert}^2 + {\lVert a-b \rVert}^2 = 2({\lVert a \rVert}^2 + {\lVert b \rVert}^2) $$ 일차변환(Linear Transformations) $\bf X$에서 $\bf Y$로의 사상(mapping) 또는 변환(transformation), 연산자(operator) 공간 $\bf X$의 벡터 $\bf x$에 대하여 공간 $\bf Y$의 유일한 벡터 $\bf y$를 대응(이와 같은 사상을 $\bf F$와 같은 대문자로 표기하자) 공간 $\bf X$의 벡터 $\bf x$에 대응하는 $\bf Y$의 벡터 $\bf y$를 $\bf F$에 의한 $\bf x$의 상(image)이라 하고 $\bf F(x)(또는 괄호 없는 Fx)$로 표시한다. $\bf F$를 선형사상(linear mapping) 또는 일차변환(linear transformation) $\bf X$의 임의의 벡터 $\bf v, x$와 임의의 스칼라 $c$에 대하여 다음의 식을 만족$$\bf F(v + x) = F(v) + F(x)$$$${\bf F}(c {\bf x}) = c{\bf F}({\bf x}) $$ $R^b$ 공간에서 $R^m$ 공간으로서의 선형변환 $\bf X = R^n$, $\bf Y = R^m$이라 하자 $m \times n$ 행렬 $\bf A$가 주어지면 $R^n$에서 $R^m$으로 의 변환$$\bf y = Ax $$$\bf A(u + x) = Au + Ax$와 ${bf A}(c{bf x}) = c\bf{ A x}$이므로 이 변환은 선형이다.$\bf F$는, $\bf R^n$과 $\bf R^m$ 공간에 각각 주어진 기저를 이용하여, 적절한 $m \times n$행렬 $\bf A$로 나타낼 수 있다. http://contents2.kocw.or.kr/KOCW/document/2016/hanbat/kimdongsoo/7.pdf https://latexbase.com/d/e2c4eeb2-68f6-4efa-a305-112097ad9e8b https://forknwork.wordpress.com/2018/02/14/openpose3d-pose-baseline/ https://github.com/wangzheallen/awesome-human-pose-estimation#3d-pose-estimation]]></content>
      <categories>
        <category>mathematics</category>
      </categories>
      <tags>
        <tag>linear algebra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CVPR 2017] Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields 리뷰]]></title>
    <url>%2F2019%2F08%2F02%2F2019-08-02-Openpose-paper-review%2F</url>
    <content type="text"><![CDATA[Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields을 읽고 정리한 글입니다. IntroductionHuman pose estimation 문제는 다음과 같은 어려움 점이 존재합니다. Challenges Unknown number of people that can occur in a frame. (한프레임에 사람에 몇명이 있는지 모른다.) Complex Spatial Interference - Contact, Occlusion between people. (사람들간의 접촉, 맞물림등 복잡한 공간적 간섭이 존재) Variance in person scales (사람들의 크기가 다양함) Run time Complexity. (실행시간이 길다) Human pose estimation 문제를 접근하는 방법을 크게 Top-down Approach, Bottom-up Approach 두가지 방법이 있습니다. Top-down Approach Fig1. Top-down Appraoch방식은 이미지에서 사람을 먼저 찾고 관절을 추정하는 방식입니다. Problem Human detector가 사람을 잘못찾으면 이를 해결할 방법이 없다. 사람의 수가 많아지면 computational cost가 증가한다. ( Challenges 4 ) Bottom-up Approach Fig2. Bottom-up Appraoch방식은 관절을 먼저 다찾고 이를 알맞게 이어주는 방식입니다. Problem 찾은 관절을 매칭할 수 있는 조합이 매우 많고 이를 적절하게 매칭하는데 시간이 많이 걸리고 Accuracy도 높이는 것이 힘듭니다. Architecture Fig3 Fig4 Input &amp; 2 BranchInput: RGB color image 병렬적으로 구성된 2개의 Branch Branch 1에서는 Confidnce maps를 찾아낸다.$$S^1 = \rho^1(F), S^t = \rho(F,S^(t-1),L^(t-1), \forall t \ge 2) - (1)$$ Branch 2에서는 PAF를 찾아낸다.$$S^1 = \rho^1(F), S^t = \rho(F,S^(t-1),L^(t-1), \forall t \ge 2) - (2)$$ $S^t$:$\ $ Stage $t$에서 만들어진 Confidence maps $F$:$\ $ VGG-19에서 추출된 Feature $L^t$:$\ $ Stage $t$에서 만들어진 PAF Fig5.stage가 지날수록 성능이 향상된다.(위에 주황색원: False negative, 빨간원: False positive이 해결되고 있음 Loss functiongroundtruths와 추정된 값의 $L_2$ loss를 사용하였다. $$f_{S}^t = \sum\limits_{j=1}^J\sum_{p}W(p) \dot \lVert S_{j}^t(p) - S_{j}^{\ast}(p)\rVert_{2}^2 - (3)$$ $$f_{L}^t = \sum\limits_{c=1}^C\sum_{p}W(p) \dot \lVert L_{c}^t(p) - L_{c}^{\ast}(p)\rVert_{2}^2 - (4)$$ $J$,$C$: Confidence map의 개수와 PAF의 개수 $f_{S}^t$: Stage $t$ 에서의 Confidence map loss $f_{L}^t$: Stage $t$ 에서의 PAF loss $p$: 이미지의 좌표 $W(p)$: binary mask, true-positive를 처벌하는것을 피하기위해서 사용됨, annotation이 없을때 $W(p) = 0$ Stage 마다 loss를 계산하여 Vanishing gradient를 해결( paper) overall objective$$f = \sum\limits_{t=1}^T(f_{S}^t + f_{L}^t)- (5)$$ Generation of confidence mapdataset은 keypoint만 주어지기에 groundtruths에 될 confidence maps를 만들어 줘야한다. $$S_{j,k}^{\ast}(p) = \exp (-\dfrac{\lVert p-x_{j,k}\rVert _{2}^2}{\sigma^2}) - (6)$$ $S_{j,k}^{\ast}(p)$:$ \ \ k$번째 사람의 $j$번째 관절의 confidence map $x_{j,k}$:$\ \ k$번째 사람의 $j$번째 관절의 keypoint $\sigma$:$ \ $ peak의 범위를 조절함 Fig6. $$S_{j}^{\ast}(p) = \max_{\rm k}S_{j,k}^{\ast}(p) - (7)$$ 이렇게 만든 가우시안 분포에 max를 취해준다.(average보다 peaks가 distinct하게 남아서 근접에 대한 정밀도를 가진다.) PAFs(Part Affinity Fields) Fig7. (a):$ \ $ keypoint의 모든 연결 후보, (b):$ \ $ 각 연결 쌍의 중간점을 추가하여 중간 점의 발생률을 가지고 그리기(초록색선:틀린선,검은선:맞는선),사람들이 몰려있으면 오판될 수 있다.(이러한 방법은 위치만 고려하고 방향은 고려 하지 않고 region of support of limb를 한점으로 줄여버린다.) (c):$ \ $PAF를 이용한 결과(위치와 방향문제 해결 및 region of support of limb의 문제를 해결) Fig8. 식(4)를 풀기위해서 train때 PAF의 groundtruth를 다음과 같이 정의합니다.$$L_{c,k}^{\ast}(p) = \begin{cases}v &amp; \ \ \text{ if } \ p \ \text{on limb} \ c,k \cr0 &amp; \ \ \ \text{otherwise.}\end{cases} - (8) $$ $c$:$\ $limb의 종류 $v$:$\ \ v = \frac{(x_{j_2,k}-x_{j_1,k})}{\lVert x_{j_2,k}-x_{j_1,k}\rVert_2}$, limb의 방향의 unit vector(위 그림의 초록색선) limb위에 있는 points는 distance threshold내로 정의합니다. $$0 \le v \dot (p-x_{j_1,k}) \le l_{c,k} , \text{and} , |v_{\perp} \dot (p-x_{j_1,k})| \le \sigma_l - (9)$$ $l_{c,k}$: $\ \ l_{c,k} = \lVert x_{j_2,k} - x_{j_1,k}\rVert_2$, limb의 길이 $\sigma_l$:$ \ $ limb의 폭 마지막으로 이미지에 모든 사람으로 평균화 합니다. $$L_{c}^{\ast}(p) = \dfrac 1 n_{c}(p) \sum_{k}L_{c,k}^*(p) - (10)$$ $n_c(p)$:$\ $p에서의 non-zero vectors의 수(서로다른 사람의 limbs의 overlap되는 픽셀을 평균화시킴) test때 에는 $d_{j_1}$으로부터 $d_{j_2}$로 일정간격으로 선적분을 수행하여 affinity field의 세기(E)를 구합니다. $$E = \int_{u=0}^{u=1} L_c(p(u)) \cdot \dfrac {d_{j_2}-d_{j_1}} {\lVert d_{j_2}-d_{j_1} \rVert_2} du - (11)$$ $L_c$: $ \ $예측된 PAF $d_{j_1}$: $ \ $ 시작part 위치 $d_{j_2}$: $ \ $ 끝 part 위치 $p(u)$: $ \ $ $p(u) = (1-u)d_{j_1} + ud_{j_2}$, 2 part의 위치 사이를 채웁니다. Multi-Person parsing using PAFs Fig9 (a): Original image (b): 가능한 모든 연결 (c): 사람의 관절구조(spanning tree형태) (d): 이웃한 관절끼리 두개씩의 매칭 non-maximum suppression(NMS)을 사용해서 모든 part의 confidence maps를 구하였습니다. 이로인해 다양한 후보가 발생하고 여러 사람이 있기에 false positive 문제가 발생할 수 있습니다. optimal한 parse를 찾는 문제는 K-dimensional matching problem을 가지고있다.(NP-Hard 이라고도 함) 이 논문에서는 계속해서 high-quality한 match를 찾는 greedy relaxation을 말하고 있습니다.(optimal association 문제를 maximum weight bipartite graphmatching problem으로 줄입니다.) 어떠한 2개의 edge도 1개의 node를 공유하지 않기에 목표는 주어진 edges에서 maximum weight를 찾는 것입니다.(weight는 식(10)을 이용하여 구합니다) $$\max_{Z_c}E_c = \max_{Z_c} \sum_{m \in D_{j_1}}\sum_{n \in D_{j_2}}E_{mn}\dot z_{j_1j_2}^{mn} - (12)$$ $z_{j_1j_2}^{mn}$:$\ \ z_{j_1j_2}^{mn} \in \lbrace 0,1\rbrace$, detection candidates $d_{j_1]}^m$와 $d_{j_2}^n$이 연결되 있는지 아닌지를 나타내는 변수 $Z$:$\ \ Z = \lbrace z_{j_1j_2}^{mn} \text{for} j_1,j_2 \in \lbrace 1…J\rbrace,m\in \lbrace 1…N_{j_1}\rbrace,n \in \lbrace 1…N_{j_2}\rbrace\rbrace$, $z_{j_1j_2}^{mn}$ 가능한 모든 연결의 집합 $D_J$:$\ \ D_J = \lbrace d_j^m : \text{for} j \in \lbrace 1…J\rbrace,m\in \lbrace 1…N_j\rbrace\rbrace$,body part detection candidates의 집합 결론적으로 optimization은 다음과 같이 표현된다 $$\max_{Z} E = \sum_{c=1}^C\max_{Z_c}E_c - (13)$$ ( ? adjacent tree nodes는 PAFs에의해서 explicitly 하게 모델되었고 nonadjacent tree nodes는 CNN에의해 implicitly 모델되었습니다. ) ( ? 이러한 특성은 CNN이 large receptive field를 가지고 train 되고 non-adjacent tree nodes의 PAFs 또한 predicted PAF에 영향 끼쳤기에 발생하였습니다. ) ResultsMPII Multi-person Dataset COCO Dataset Referencehttps://ml.starall.kr/1 https://cloudup.com/i_gPL3kASQg]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>deep_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Robot Manipulator and Underwater Robot Application 6주차 6-2 Relative Velocities of Moving Frames(1)]]></title>
    <url>%2F2019%2F04%2F19%2F2019-04-19-KMOOC-Robot-Manipulator-and-Underwater-Robot-Application-6-Weak-6-2%2F</url>
    <content type="text"><![CDATA[Relative Velocities of Moving Frames 세 개의 프레임을 한번 생각해보면 위 그림처럼 A B C라고 하는 세개의 프레임이 존재하고 이랬을 때 A는 고정되어 있습니다.A는 고정되어 있고 A에 대해서 B는 움직이고 B에 대해서 역시 C는 움직이고 있습니다. 자 이랬을 때 우리는 A라는 고정된 프레임에서 C라는 프레임의 원점의 좌표를 원점의 운동을 기술해보고 싶은 것입니다. 그렇다면 간단한 벡터관계식에 의해서 우리는 A프레임에 대한 C프레임의 원점을 $^AP_C$라고 이렇게 옆에 식처럼 표현할 수가 있고이 좌표는 직접적으로 구하는 것이 아니라 간접적으로 B프레임까지 가서 다시 B프레임에서 C프레임까지 가는 그 과정으로 이 값들을 정의할 수 있겠습니다. 그랬을 때 B프레임 즉 A프레임에 대한 B프레임의 원점에 대한 좌표를 APB라고 하고 그 플러스 B프레임에 대해서 C프레임의 원점에 대한 좌표가 결국에는 BPC로 그렇게 정의가 됩니다.이렇게 정의된 것을 그대로 쓰는 것이 아니라 A프레임으로 가지고 와야 됩니다. 그 A프레임으로 가지고 오는 매트릭스의 변화를 로테이션 매트릭스라고 합니다.최종적으로는 C라는 프레임의 원점을 A프레임으로 가지고 오기 위해서는 B프레임을 A프레임으로 기술한 포지션과 C프레임을 A로 가지고 오기 위한 B프레임까지 갔다가 rotation되서 오는 벡터 두 개의 합으로써 표현할 수가 있을 것입니다. 이렇게 표현되어 있는 벡터, 위치관계식을 가지고 속도관계식을 얻으려고 하는 것인데 속도관계식을 얻기 위해서 미분을 양변에 해보도록 하겠습니다.그랬을 경우에 앞에 C프레임의 원점을 A프레임으로 가져오는 포지션 벡터 P를 미분한 $\dot P$은 결과적으로는 B프레임으로부터 A프레임으로 오는 사이의 translation vector인 $\dot P$ + matrix 형태로 곱해져있는 rotation matrix와 P가 곱해져 있는 것을 미분한 형태로써 주어지게 되겠고 여기에서 A프레임은 고정되어 있기 때문에 B프레임까지 가는 그 속도관계식은 그대로 그냥 사용을 하면 되겠는데 moving frame일 경우에는 그 rotation하는 rate, 그 내용들 때문에 속도관계식이 조금 복잡해집니다. 그래서 일단은 그러한 관계식을 수식적으로 analytic 하게 계산을 해 보도록 하겠습니다. 그래서 여러분들 잘 아시는 것 처럼 두 개의 어떤 항이 있을 때의 미분은 앞에 것 미분 뒤에 것 미분 그런 방식으로 하는 것을 잘 알고 계실 것 입니다.그래서 위에 보이는 식처럼 파랗게 표현한 부분은 일단은 로테이션 매트릭스를 미분한 것과, 그 다음에 뒤의 것은 그대로 놔둔 즉, C에서부터 B까지 오는 C프레임의 원점을 B로 가져오는 위치, 그 다음에 플러스 rotation matrix 에다가 B프레임에 대한 C프레임의 이동속도 $\dot P$, 이렇게 표현이 되겠습니다.여기에서 각각의 Linear속도, 즉 translation 하는 속도는 큰 문제가 없습니다. 그대로 사용하면 되겠는데 여기에서 $d \above 1pt dt$ 그 다음에 R 로테이션 매트릭스에 대한 미분, 이 부분은 조금 다루기가 까다로울 수가 있습니다.rotation matrix를 가지고 있습니다. 그럼 미분을 하면 그대로 사용을 하면 됩니다. 하지만 그랬을 때 이 로테이션 매트릭스에 대한 미분이 가지고 있는 정확한 물리적인 의미를 설명하는 것은 조금 어려울 수가 있겠습니다.그래서 이러한 부분들을 잘 설명하기 위해서 이 rotation matrix를 미분하는 그 과정을 한번 차근차근 살펴보도록 하겠습니다. 그에 앞서서 로테이션 rate 을 보통 $\omega$라고 정의하는데 이 $\omega$는 사실 어떠한 프레임이 회전하는속도, 즉 각속도를 의미하는데 이 프레임이 회전하는 각속도라는 것이 사실상 그 프레임에서의 어떤 벡터로써 나타낼수 있습니다. 이제 이것을 더 이상 각속도라 생각하지 않고 이 로테이션 하는 이 벡터, 로테이션 벡터, $\omega$라고 하는 하나의 벡터, 로테이션이라고 하는 것은 잊어버리고 그냥 벡터다라고 가정을 한다면 이 벡터 자체로는 앞에서 위치관계식과 비슷하게 $\omega$는 A에서 표현되는 $\omega$ C프레임의 회전 각속도 $\omega$는 A프레임에서 표현되는 B프레임의 각속도에다가로테이션 매트릭스를 포함하는 C프레임의 회전 각속도를 B프레임에서 기술한 이 방식, 즉 위치관계식과 그대로 일치하는 이런 수식을 보실 수 있겠습니다. 이렇게 표현이 되었고 그렇다면 앞서서 보여드렸던 rotation matrix를 미분하는 관계식 rotation matrix에 대한 미분이 물리적 의미, 수학적 의미를 가지는지를 살펴보도록 하겠습니다. 먼저 또다른 그래프를 가지고 왔는데요. 0번째 프레임이 위에서 보이는 그림에서 0번째 프레임은 고정된 프레임 입니다. 그리고 이 고정된 프레임에 대해서 첫번째 프레임, 1번 프레임이 존재한다고 가정을 하고 이 첫번째 프레임은 0번째 프레임에 대해서 $\omega$라는 velocity로, $\omega$라는 각속도를 가지고 회전을 하고 있습니다. 첫번째 프레임에 고정되어 있는 벡터 s라는 것이 존재한다 s라는 그러한 벡터가 있었을 경우에 회전하는 각속도 $\omega$와 벡터에 대한 미분과 이런것들이 어떻게 표현되는지 살펴보도록 하겠습니다.옆에 그림에서 s가 $\omega$에 의해서 회전을 통해서 새로운 s로 바뀌게 됐을 때, 그 둘 사이에 $\Delta s$ 즉 s의 변량, 변화량을 어떻게 정의할 수 있냐면 위에 그림에서 만약에 이 $\omega$에 의해서 아주 빠르게 아주 작은 시간동안 돌았다라고 한다면$\Delta s$는 결과적으로 위에 보이는 식에서 A에서부터 B까지의 그 변화량이 결국에는 $\Delta s$ 즉 s의 변화량이 되겠습니다.그러면 위에 그림에서 $\overline{AB}$는 $\overline{AC}$라고 불리는 회전축, 회전축까지의 거리에다가 $\Delta \theta$, 이 끼인각인 $\Delta \theta$로 표현을 할 수가 있겠습니다.이게 만약에 굉장히 긴 변이라면 이렇게 표현이 안되겠지만 짧은 변이 굉장히 짧은시간 동안에 회전이 이뤄지는 경우에는 이 미분관계식에 의해서 $\Delta \theta$ 즉 $\overline{AC} |\Delta \theta|$는 AB하고 같다라고 표현할 수 있습니다.이렇게 표현했을 때 여기에서 이 $\overline{AC}$라고 하는 것은 다시 s전체, 고정되어 있는 벡터 s에다가 $sin \phi$를 곱해주면 결과적으로 이 $\omega$축 방향으로의 내려가는 그 길이만큼즉 $\overline{AC}$만큼의 길이를 구할 수 있겠죠. 즉 $\overline{AC}$는 s의 절대값에다가 $sin \phi$ 즉 s의 크기에다가 $sin \phi$로써 $\overline{AC}$를 정의할 수가 있겠고요. 위에 그림에서의 $\angle AOC = \phi$라고 얘기합니다.그리고 $\Delta \theta$라고 하는 것은 $\omega$의 속도로 현재 돌고 있다고 말씀드렸습니다.그래서 $\omega$ 곱하기 $\Delta t$만큼 즉, $\Delta t$만큼 $\omega$의 속도로 돌면 그게 결국에는 $\Delta \theta$라고 표현이 됩니다.따라서 결국 $\Delta s$는 $\overline{AB}$로 표현되고, $\overline{AB}$는 $\overline{AC}$의 $\Delta \theta$로 표현되고, 다시 이것들은 금방말씀드린 것 처럼 s의 크기와 $\omega$의 크기와 $sin \phi$의 $\Delta t$만큼으로 표현이 될 것 입니다.이렇게 s하고 $\omega$하고 $sin \phi$하고 $\Delta t$로 표현이 되는데 이때 $\Delta t$를 양변에 나눠주게 되면 보이는 식처럼 $|\Delta s| \above 1pt \Delta t = |s||\omega| sin \phi$라고 표현이 됩니다.즉, s하고 $\omega$ 라는 두 개의 벡터에 의한 cross product, 즉 $\omega X s$라고 하는 cross product의 절대량이라는 것을 알 수가 있습니다.이렇게 $\omega X s$에 대한 크기로써 $\Delta s \above 1pt \Delta t$를 설명할 수가 있겠고요. 자 이랬을 때 그렇다면 이 $\omega$ 크로스 s가 나타내는 방향은 어디일까요.$\omega X s$를 한번 여러분이 생각해보시면 $\omega$하고 s하고 이루는 각도에 대해서 $\omega X s$에 대해서 오른손법칙을 적용하게 되면결과적으로는 $\overline{AB}$ 방향하고 수평되는 같은 방향을 나타낸다는 것을 알 수 있습니다. 자 그러면 어떤 벡터가 존재하고요 그 벡터의 크기는 $|\omega X s|$고그 벡터의 방향은 $\omega X s$방향이다라고 한다면 이 벡터는 뭐가 될까요.바로 그냥 $\omega X s$가 되겠죠. 즉, s를 시간에 대해서 미분한 것은 $\omega X s$로써 표현될 수 있기 때문에 옆에 보이는 식처럼 $\dot s$ 이라고 하는 것은 $\omega X s$로 표현이 되겠습니다.그렇게 표현이 된 것을 이용해서 rotation matrix에 대한 미분을 한번 다른 방법으로 조금씩 조금씩 찾아보도록 하겠습니다. 일단은 rotation matrix가 존재할 때 이 rotation matrix는 앞서서 rotation matrix를 설명하는 방법이 세가지 정도 있다고 말씀을 드렸는데 그 중에서 한가지는 어떠한 A프레임에 대해서 B프레임에 X Y Z축에 대한 좌표계를 A프레임에 대해서 표현을 한 것으로 나타낼 수 있다라고 해서옆에 보이는 수식처럼 로테이션 매트릭스는 각각 X벡터 Y벡터 Z벡터로 이뤄진 이러한 벡터로서 표현이 가능해 지고 C라는 점은 B프레임에서 기술되는 C라는 점은 X Y Z형태로 이렇게 표현을 풀어서 나눠서 써볼 수 있습니다. 그렇다면 지금 구하고 싶은 $d /above 1pt dt$ 로테이션 매트릭스의 그 C에서부터 B로 오는 포지션, translation 벡터, 이렇게 표현되는 것들을 옆의 식처럼 하나씩 풀어서 설명을 해 보겠습니다.이 두 식을 rotation matrix를 각각 $d /above 1pt dt$ X, 그 다음에 $P_c$의 X 역시 마찬가지로 Y Z에 대해서도 똑같이 각각 로테이션 매트릭스를 벡터단위로 나누어 위에 식처럼 표현을 할 수 있습니다.이건 계산상으로 수식적으로 풀어쓰면 똑같이 이렇게 나오게 되는 것이고요. 이 때 X Y Z라고 하는 각각의 좌표축들은 보시면 B라는 프레임에서의 A에 대한 각각의 좌표축입니다. 하나의 벡터들입니다.조금 전에 회전하는 좌표프레임에 대해서 그 회전하는 각속도에 대해서 고정된 벡터에 대한 회전관계식을 어떻게 유도했나요?바로 s가 주어졌을 때 그 $\dot s = \omega X s$로 주어진다고 말씀드렸습니다.그래서 이 위에 식은 바로 아래처럼 $d /above 1pt dt$. 이 미분항들이 바로 $\omega X x$, $\omega X y$, $\omega X z$로써 이렇게 바꿔서 표현이 가능해 지고분배법칙에 의해서 $\omega$를 전부다 묶으면 $XP_X YP_Y ZP_Z$ 이런식으로 옆에 식처럼 이렇게 묶어서 설명할 수 있겠고이걸 다시 원래의 식으로 돌리면 원래의 로테이션 매트릭스 하고 그 다음에 포지션 벡터로서 표현을 한다면 마지막 식에서 보이는 것처럼 $\omega X R P$형태로서 이렇게 정리가 되겠습니다.즉 $d/above 1pt dt (R) P$라고 하는 이러한 연산은 최종적으로는 $\omega X R P$ 의 형태로 주어지게 되는 것이죠. 여러분들이 앞에서 아까 제가 말씀드린 것처럼 직접적으로 $\omega$라고 하는 것은 rotation matrix에서 rotation matrix 사이에정해진 축에 대해서 회전하는 그러한 방법으로 표현도 가능하다고 말씀드렸는데요. 그 회전하는 축과 관계식을 가지는지에 대한 설명이 되겠습니다.일단 이러한 식을 전개하기 앞서 몇가지 트릭을 쓸텐데요. rotation matrix가 주어지고그 rotation matrix는 이 자체로는 orthogonal한 형태를 띄게 됩니다.그랬을 때 $R(t)$는 결과적으로 identity matrix가 되죠. 즉 transpose가 자기 자신의 역행렬이 된다는 것을 앞에서 여러 번 말씀드린적이 있습니다.그래서 $R^T(t)$는 identity matrix가 된다는 것을 알 수가 있습니다. 이 식을 바탕으로 양 변을 그대로 미분해보겠습니다.양변을 그대로 미분하면 identity matrix는 111로 구성되어있는 상수항이기 때문에, 즉 상수 매트릭스이기 때문에 미분하게 되면 0으로 사라지겠죠. 0 매트릭스가 만들어 질 것 입니다.거기에다가 앞에 부분에서 $\dot R$ 미분한거 그 다음에 뒤에쪽 $\dot R , \dot R^T$을 미분한 것을 쓰게되면 이렇게 $\dot R(t) R^T(t) + R(t) \dot R^T(t) = 0$라고 하는 새로운 식이 만들어지게 되는 것입니다.여기에서 조금 유념해서 보실 부분이 $\dot R R^T$ 하고 뒤에 $R \dot R^T$ 두 개의 식들은 가만히 보시면 트랜스포즈 관계식을 가지고 있다라는 것을 확인하실 수가 있겠습니다.그렇다면 앞에 부분을 $S^T(t)$라고 정의를 해 보고요 $R(t) \dot R^T(t)$를 $S(t)$라고 정의를 하면, $S^T(t) + S(t) = 0$라고 하는 수식이 만들어지는 것입니다. 바로 이런 관계식을 가지고 있는 것을 skew-symmetric 매트릭스라고 얘기했었습니다. 즉 $S^T$가 자기자신에 -를 붙인것과 같다고 하는 것입니다.그래서 트랜스포즈하고 자기자신을 더했을 때 0이 되는 그러한 매트릭스를 skew-symmetric 매트릭스라고 얘기 했습니다.즉 S라고 하는 것은 $\dot R R^T$의 형태를 띄게 되고 이렇게 어떤 S가 아직 뭔지 모르겠지만 S를 $\dot R R^T$로 이렇게 만들어줄 수 있다라는 것입니다.그랬을 때 $\dot R$은 다시 이 $S = \dot R R^T$ 에서 양변에다가 $R$을 곱해주게되면 결국에는 우측변에 있는 항에는 $\dot R$만 남게되겠고좌측변에 있는 항이 $SR$의 형태로 주어지게 될 것 입니다. 이렇게 주어졌을 때 만약에 이 $\omega$라고 하는 두 좌표계상에서의 두 좌표계의 속도 관계식, 속도가 $\omega$로 회전을 하고 있다라고 했을 때$\omega$는 $\omega$는 $\omega_x$ $\omega_y$ $\omega_z$ 라고 이렇게 표현을 할 수 있다고 말씀드렸는데 이 S하고 $\omega$하고 관계식이 분명히 있을 것 같다라는 추정으로 일단은 S를 $\omega$ S에 cross product 형태로 만들어보겠습니다.과연 이 추정과 이런것들이 어떻게 뒤에서 연결되는지는 이어지는 설명에서 계속 말씀드릴텐데이렇게 S를 표현하는데, 앞서서 cross product 를 설명할 때 S로 표현을 하기도 했었는데 이걸 조금 달리 $\omega X$ 라고 하는 이러한 rate으로 표현되는 이 형태로도 표현이 가능합니다.그렇기 때문에 $a = Sb = [\omega X]b = \omega X b$이런 형태가 다 같은 것을 의미한다고 여러분들은 생각하면 되겠습니다. 이어서, 앞서 Rodrigues parameter, Rodrigues equation을 Euler’s theorem을 가지고 설명한 적이 있었는데 만약에 미소변이에 대해서 미소한 어떤 각도표현에 대해서 두 개의 로테이션 매트릭스는 곱하기 형태로 바꿔 쓸 수 있다.즉 $t + \Delta t$에서 $\Delta t$가 굉장히 작은값이다라고 했을 경우에는 $R(\Delta t) R(t)$의 형태로 표현이 가능하다는 것을 말씀드린적이 있었고이것을 Rodrigues equation을 가지고 설명을 쭉, 수식을 쭉 전개를 하면 옆에 보이는 식처럼위 2번째 식의 형태로 표현이 되겠고이 때 굉장히 작은 미소한 회전량을 설명하는 것이기 때문에 $sin(\Delta \delta)$를 결국에는 싸인값은 $\Delta \delta$로 바뀌게 되겠고그다음에 $cos(\Delta \delta)$ 같은 경우에는 1의 값으로 바뀌어서 뒤의 항들이 사라지는 것을 앞에서 다룬 적이 있었습니다.그래서 결과적으로는 $R (t + \Delta t)$는 밑에 보이는 식처럼 I의 identity matrix + $\Delta \delta$에 그 다음에 $[\lambda X]$에 $R(t)$의 형태로 이러한 수식으로 전개가 되겠습니다.이때 $\Delta \delta$ 같은 경우에는 굉장히 작은 어떠한 미세한 미소 회전량이 되겠죠. 이런 미소 회전량에 대한 설명이 되겠습니다. 이렇게 주어졌을 때 로테이션 매트릭스를 정의를 할 때 로테이션 매트릭스에 dot, 즉 시간에 대한 미분은 수식적으로 어떻게 설명이 가능하냐하면$R(t+\Delta t) - R(t) \above 1pt \Delta t$. 일반적인 함수에 대한 미분관계식과 똑같이 이렇게 표현한다면, 앞서서 $R(t + \Delta t)$라고 정의된 그 내용을 바로 대입을 해 보겠습니다.그러면 아이덴티티 매트릭스가 있으니까 R에 대해서 소거가 되고 결과적으로 옆에 보이시는 것 처럼 limit, t를 $\Delta t$를 0으로 보낼 때 $\Delta \delta[\lambda X] R(t) \above 1pt \Delta t$고,이랬을 때 여기에서의 유일한 작은 스몰 변량에 대한 미분관계식을 만드는 것은 바로 $\Delta \delta[\lambda X] R(t) \above 1pt \Delta t$ 그래서 이것들은 그렇게 미분해주게 되면 $\dot \delta [\lambda X]R(t)$ 람다하고 R같은 경우에는 각각 현재의 상수형태를 표현하고 있기 때문에 이렇게 $\dot \delta [\lambda X]R(t)$ 의 형태가 되겠고요이 때 $\lambda$라고 하는 것은 로드리게스 파라미터에서 회전의 중심이 되는 축이되겠습니다. 회전의 중심이 되는 축이 되겠고요. 그 축에 대해서 $\dot \delta$만큼 즉 그 축에 대한 회전량이 되겠죠.회전량만큼은 결과적으로 이게 어떤 축이 회전하는 그 양, 즉 $\omega$ 형태로 표현이 가능해지는 것이죠.그래서 결국 이 식들은 옆에 보이는 식처럼 $\omega X$ 매트릭스 형태에다가 $R(t)$ 형태로 바꿔쓸수가 있게 되는 것이고이것은 앞서서 직전에 $\omega X$를 S로 전개한 것과 마찬가지로 $SR$의 형태로 성립할 수가 있다는 것을 말씀드릴 수 있겠습니다.최종적으로 $\dot R$이라고 하는 것을 어떻게 회전에 대한 식으로써 표현하는지에 대한 것들을 이제까지 쭉 정리를 해보았습니다. [[cos{\theta_1},0,-sin{\theta_1},-sin{\theta_1}d_3],[sin{\theta_1},0,cos{\theta_1},cos{\theta_1}d_3]]$\left\lceil\matrix{[cos{\theta_1} &amp; 0 &amp; -sin{\theta_1} &amp; -sin{\theta_1}d_3 \cr sin{\theta_1} &amp; 0 &amp; cos{\theta_1} &amp;cos{\theta_1}d_3 \cr 0 &amp; -1 &amp; 0 &amp; d_1+d_2 \cr 0 &amp; 0 &amp; 0 &amp; 1}\right\rceil $^0A_1 = Rot(Z_{0},\theta_1)Trans(Z_{0},d_1)Trans(X_1,a_1)Rot(X_1,\alpha_1) =$ $^1A_2 = Rot(Z_{1},\theta_2)Trans(Z_{1},d_2)Trans(X_2,a_2)Rot(X_2,\alpha_2) =$ $^2A_3 = Rot(Z_{2},\theta_3)Trans(Z_{2},d_3)Trans(X_3,a_3)Rot(X_3,\alpha_3) =$]]></content>
      <categories>
        <category>Robotics</category>
      </categories>
      <tags>
        <tag>K-MOOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openpose 설치 tutorial]]></title>
    <url>%2F2019%2F04%2F08%2F2019-04-08-Openpose-tutorial%2F</url>
    <content type="text"><![CDATA[Openpose installrequirement NVIDIA CUDA_(그래픽카드 메모리가 1.6GB이상이여한다.)NVIDIA cuDNN Clone OpenPose$ git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose openpose/3rdparty 폴더에 가보면 이렇게 되어있을텐데 openpose github페이지에 가서 3rdparty에 들어간다. 여기서 caffe,pybind11에 들어가 깔아둔 openpose/3rdparty에 복사한다. 12345# openpose/3rdparty에서 터미널 실행하고$ git clone https://github.com/CMU-Perceptual-Computing-Lab/caffe.git$ git clone https://github.com/pybind/pybind11.git install library12345678910111213$ sudo apt-get install wget vim cmake cmake-qt-gui$ sudo apt-get install python-dev python-pip python-numpy$ pip install --upgrade pip$ sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev$ sudo apt-get install libhdf5-serial-dev protobuf-compiler$ sudo apt-get install libboost-all-dev libgoogle-glog-dev$ sudo apt-get install liblmdb-dev libopenblas-dev libatlas-base-dev cmake터미널 창에 cmake-gui 입력 다음과같이 where is the source code: Openpose 주소 where to build the binaries: openpose/build 로 바꿔준다. 그리고 configure를 눌러준다. 이런창이 나오면 generate 버튼을 클릭하고 완료되면 build 폴더로 간다. cd openpose/build/ 그리고 make 해준다 make -j 8 Demo 실행make가 완료되면 ./build/examples/openpose/openpose.bin --video examples/media/video.avi 을 시키고 잘작동하는지 확인한다 TODOtf버전 Openpsoe 추가]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep learning 유용한 사이트]]></title>
    <url>%2F2019%2F04%2F03%2F2019-04-04-Deep-learning-site%2F</url>
    <content type="text"><![CDATA[공부하면서 도움이 되었던 사이트들을 모아두는 곳 입니다. deep learning 기법Weight Initializationhttps://flonelin.wordpress.com/2018/01/28/weight-initalizer-%EC%A2%85%EB%A5%98/ https://gomguard.tistory.com/184 Regularizationhttp://www.hellot.net/new_hellot/magazine/magazine_read.html?code=202&amp;idx=41074&amp;public_date=2018-06 논문 분석http://openresearch.ai/ Pose estimationhttps://github.com/wangzheallen/awesome-human-pose-estimation#3d-pose-estimation 영상처리https://laonple.blog.me/220463627091 Codinghttps://github.com/Hvass-Labs/TensorFlow-Tutorials/ https://github.com/aymericdamien/TensorFlow-Examples https://github.com/tensorflow/docs/blob/master/site/en/tutorials/estimators/cnn.ipynb]]></content>
      <categories>
        <category>Information</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker,XML,JSON,CSV 간단 공부]]></title>
    <url>%2F2019%2F04%2F02%2F2019-04-02-docker-DATA%2F</url>
    <content type="text"><![CDATA[머신러닝, 딥러닝 실전 개발 입문 강의를 보고 공부한 내용입니다. https://www.youtube.com/watch?v=l_XFlB1Wwz8&amp;list=PLBXuLgInP-5m_vn9ycXHRl7hlsd1huqmS&amp;index=1 Docker먼저 docker 이미지를 가져온다(miniconda: anaconda 패키지중 가장 기본적인것만 설치되어있음) 1docker pull continuumio/miniconda3 설치가 다되었으면 아래 명령어로 이미지를 실행한다. 1docker run -i -t continuumio/miniconda3 /bin/bash |![alt](/images/Deep-learningetc/docker.png)| 위 이미지처럼 환경이 바뀌었으면 잘 작동한 것이다. 1exit 를통해서 환경을 종료 할 수 있다. 아래 명령어를 입력하면 1docker ps -a |![alt](/images/Deep-learningetc/docker (2).png)| 위 그림과 같이 컨테이너 실행 기록을 확인할 수 있다. 123docker commit 컨테이너 ID 이름:태그ex) docker commit &lt;a4997a3ede6e&gt; mlearn:init 위 명령어로 통해 컨테이너 이미지를 저장할 수 있다. 1docker run -i -t mlearn:init 위 명령어로 위에서 사용했던 환경과 똑같은 환경을 이용할 수 있다. 123docker run -i -t -v 자신이 가진폴더:컨테이너의 폴더 이미지 이름:태그 이름ex) docker run -i -t -v /home/kist-student/docker_sample:/sample mlearn:init 위 명령어로 폴더 마운트해서 이미지를 실행시킨다. XML(Extensible Markup Language)XML 형태여는 태그와 닫는 태그 &lt;태그&gt;&lt;/태그&gt; #요소(element) &lt;태그 /&gt; 콘텐츠&lt;태그&gt;콘텐츠&lt;/태그&gt;&lt;태그&gt; &lt;태그&gt;콘텐츠&lt;/태그&gt; &lt;태그&gt;콘텐츠&lt;/태그&gt;&lt;/태그&gt; 속성: “” =&gt; 문자열&lt;태그 속성=”값” 속성=”값” 속성=”값” 속성=”값”&gt;콘텐츠&lt;/태그&gt;&lt;태그 속성=”값” 속성=”값” 속성=”값” 속성=”값” /&gt; |![alt](/images/Deep-learningetc/XML.png)| Root tag 항상 하나,CDATA 내부 글자가 클때 데이터 보호용,rss는 태그이름 참고: https://sjh836.tistory.com/118 JSON(JavaScript Object Notation)JSON 구조가능한 자료형 숫자: 10, 253, 52.3 문자열: “안녕하세요” bool: true false null: null 배열:[10, 273, “안녕하세요”, true] 객체: 1234567&#123; &quot;키A&quot;: &quot;값&quot;, &quot;키B&quot;: 273, &quot;키C&quot;: true, &quot;키D&quot;: [12, 52] &quot;키E&quot;: &#123; &quot;name&quot;: 52 &#125;&#125; |![alt](https://www.w3resource.com/w3r_images/json-introduction.png)| 처음에는 배열이나 객체가 먼저오는게 일반적 CSV(Comma-Seperated Values)CSV 특징 한 줄에 데이터 하나 첫 번쨰 줄은 헤더로 사용 가능 1234ID, 이름, 가격1000,비누,300 # 1번 데이터1001,장갑,150 # 2번 데이터1002,마스크,230 # 3번 데이터 SSV: 뛰어쓰기TSV: tabCSV &gt; TSV, SSV xml 글자 많음(데이터 많음) &gt; json &gt; csv 표현력 많음: xml &gt; json &gt; csv xml은 잘 쓰이지 않고 있다고함 참고https://www.youtube.com/watch?v=dmwBi_JiYMs&amp;list=PLBXuLgInP-5m_vn9ycXHRl7hlsd1huqmS&amp;index=12 https://sjh836.tistory.com/118 https://stophyun.tistory.com/162 ##TODO데이터형 parsing 코드추가 및 docker 설명 추가]]></content>
      <categories>
        <category>theory</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNN Family 간단 정리]]></title>
    <url>%2F2019%2F04%2F01%2F2019-04-01-RCNN-Family%2F</url>
    <content type="text"><![CDATA[R-CNNRegion Proposals 입력 영상에서 ‘물체가 있을 법한’ 영역을 빠른 속도로 찾아내는 알고리즘을 region proposal 알고리즘이라 합니다. ex) Selective search, Edge boxes Transfer Learning기존의 만들어진 모델을 사용하여 새로운 모델을 만들시 학습을 빠르게 하며, 예측을 더 높이는 방법입니다. 사용되는 이유: 실질적으로 Convolution network을 처음부터 학습시키는 일은 많지 않습니다. 대부분의 문제는 이미 학습된 모델을 사용해서 문제를 해결할 수 있습니다. 복잡한 모델일수록 학습시키기 어렵습니다. 어떤 모델은 2주정도 걸릴수 있으며, 비싼 GPU 여러대를 사용하기도 합니다. layers의 갯수, activation, hyper parameters등등 고려해야 할 사항들이 많으며, 실질적으로 처음부터 학습시키려면 많은 시도가 필요합니다. 결론적으로 이미 잘 훈련된 모델이 있고, 특히 해당 모델과 유사한 문제를 해결시 transfer learining을 사용합니다. 새로 훈련할 데이터가 적지만 original 데이터와 유사할 경우데이터의 양이 적어 fine-tune (전체 모델에 대해서 backpropagation을 진행하는 것) 은 over-fitting의 위험이 있기에 하지 않습니다.새로 학습할 데이터는 original 데이터와 유사하기 때문에 이 경우 최종 linear classfier 레이어만 학습을 합니다. 새로 훈련할 데이터가 매우 많으며 original 데이터와 유사할 경우새로 학습할 데이터의 양이 많다는 것은 over-fitting의 위험이 낮다는 뜻이므로, 전체 레이어에 대해서 fine-tune을 합니다. 새로 훈련할 데이터가 적으며 original 데이터와 다른 경우데이터의 양이 적기 때문에 최종 단계의 linear classifier 레이어를 학습하는 것이 좋을 것입니다. 반면서 데이터가 서로 다르기 때문에 거의 마지막부분 (the top of the network)만 학습하는 것은 좋지 않습니다. 서로 상충이 되는데.. 이 경우에는 네트워크 초기 부분 어딘가 activation 이후에 특정 레이어를 학습시키는게 좋습니다. 새로 훈련할 데이터가 많지만 original 데이터와와 다른 경우데이터가 많기 때문에 아예 새로운 ConvNet을 만들수도 있지만, 실적적으로 transfer learning이 더 효율이 좋습니다. 전체 네트워크에 대해서 fine-tune을 해도 됩니다. https://fabj.tistory.com/57 R-CNN의 구조 이미지를 입력으로 받음 Selective search를 이용해 이미지로부터 약 2000개 가량의 region proposal을 추출함 각 region proposal 영역을 이미지로부터 잘라내고(cropping) 동일한 크기로 만든 후(warping), CNN을 활용해 feature 추출 각 region proposal feature에 대한 classification을 수행 ImageNet classification 데이터로 ConvNet을 pre-train 시켜 모델 $M$을 얻습니다. $M$을 기반으로, object detection 데이터로 ConvNet을 fine-tune 시킨 모델 $M^’$을 얻습니다. object detection 데이터 각각의 이미지에 존재하는 모든 region proposal들에 대해 모델 $M^’$으로 feature vector $F$를 추출하여 저장합니다. a. 추출된 $F$를 기반으로 classifier (SVM)을 학습합니다.b. 추출된 $F$를 기반으로 linear bounding-box regressor를 학습합니다. 여기서 CNN은 Transfer Learning을 사용 :모델의 마지막 층에 SVM을 두어 간단하게 이 결과가 객체인지 아니지, 객체가 맞다면 어떤 객체인지를 분류하도록 하였다. 문제점 localization에 취약함 -&gt; 개선: linear regression model을 통해 tight하게 맞추도록함 참고 모델이 Image feature를 생성하는 것(CNN), classifier가 class를 예측하는 것(SVM), regression model이 bouding box를 찾아낸 것 (linear regression) 총 3개가 필요합니다. R-CNN의 단점 Test 속도가 느림(CNN을 2000번 돌리기 때문) 학습과정이 복잡함(3단계 pipeline) Input image 크기를 강제로 224 x 224로 warp, crop Fast R-CNNSPP(Spatial Pyramid Pooling) 다양한 크기의 입력으로부터 일정한 크기의 feature를 추출해 낼 수 있는 방법 중 Bag-of-words (BoW)라는 방법이 있습니다. 하지만 BoW는 이미지가 지닌 특징들의 위치 정보를 모두 잃어버린다는 단점이 존재합니다. 이러한 단점을 보완하기 위한 Spatial Pyramid Pooling 은 이미지를 여러개의 일정 개수의 지역으로 나눈 뒤, 각 지역에 BoW를 적용하여 지역적인 정보를 어느정도 유지할 수 있게 됩니다. ROI Pooling(Single-level SPP) SPP layer는 feature map 상의 특정 영역에 대해 일정한 고정된 개수의 bin으로 영역을 나눈 뒤, 각 bin에 대해 max pooling 또는 average pooling을 취함으로써 고정된 길이의 feature vector를 가져올 수 있습니다. Fast R-CNN에서는 이러한 SPP layer의 single level pyramid만을 사용하며, 이를 RoI Pooling layer라고 명칭하였습니다. RoIPool의 핵심은 한 이미지의 subregion에 대한 forward pass값을 서로 공유하는 것이다. 위의 그림을 통해 어떻게 각 region에 대한 CNN feature가 feature map의 동일한 영역으로 부터 선택되어 값을 얻어내는지 확인할 수 있습니다. Fast R-CNN의 구조 pretrained된 모델에 이미지를 1개만 입력으로 받음 CNN을 통과한 feature map을 selective search로 2000개 가량의 region proposal을 추출함 region proposal(feature map)을 Roi pooling을 한다. Softmax classifier와 linear bounding-box regressor의 loss를 더하여 학습시킨다. 특징: CNN을 2000번 돌리는 것이 아닌 1번만 돌리면 된다.(속도 향상), 모델이 3개에서 1개의 네트워크로 통일됨 Fast R-CNN의 장단점장점 R-CNN에 비해 detection/localization 정확성 및 속도 개선 단점 region proposal 시간을 포함 시 real-time X (region proposal 에서 병목 현상) Faster R-CNNRPN(Region Proposal Network)Fast R-CNN 중 Selective Search(Region proposal)부분을 딥러닝으로 바꾼 것을 RPN이라 한다. 즉, 이 CNN기반의 미니 CNN인 RPN이 이미지 -&gt; CNN -&gt; output feature(feature map)를 잘라준다. RPN의 Covolution NN이 output feature를 sliding window방식으로 돌면서 연산후 classification 과 Regression 연산까지 한다. forward/ backward propagation -&gt; weight 업데이트 과정을 거치면 -&gt; Selective search를 대체하여 이미지를 2000개로 조각낸다. 즉, CNN기반의 RPN이 sliding window방식으로 box를 찾는 역활을 한다. RPN 구현 방법 이미지를 CNN으로 연산한다. 연산 결과를 n x n(보통 3x3) Convolutional Layer로 연산하고, 이 연산 결과가 맞는지를 보유하고 있는 bounding box 데이터와 Loss Function으로 비교한다. Loss function 결과로 backpropagation을 시키면 Region Proposal Network가 학습된다. 이 때, box를 찾는 과정에서, 어떤 object는 가로가 길고, 어떤 object는 세로가 길어서, sliding window가 꼭 정사각형이 아니라 직사각형 형태로 도는 것이 유리할 수 있다. 이러한 여러 형태의 sliding window를 anchor box라 한다. 그래서 RPN에서는 output feature인 feature map을 도는 여러개의 anchor box를 운영한다. 즉 CNN의 필터 대신, RPN은 anchor box를 사용하여, 따로 foward/backward하면서 training하여, 2000조각 낼 부분을 predict한다. Faster R-CNN구조 image를 CNN에 집어넣는다. CNN에서 나온 output feature(feature map)을 RPN에 집어넣어 classification 과 box를 얼마나 쳐야하는지를 따로 return받는다. Roi pooling을 이용하여 box크기를 fully-connected에 넣을 수 있게 resizing해준다. fast R-CNN과 동일하게 해준다. 1개의 모델에 끝에만 classification / regression을 따로 만들어 loss2개, weight업데이트도 2개로 따로하여 classification / regression(box위치)를 predict한다. ##TODO Mask R-CNN정리 참고http://incredible.ai/artificial-intelligence/2017/05/13/Transfer-Learning/ https://blog.lunit.io/2017/06/01/r-cnns-tutorial/ https://junn.in/archives/2517 https://nittaku.tistory.com/273]]></content>
      <categories>
        <category>theory</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mask RCNN-tensorflow tutorial]]></title>
    <url>%2F2019%2F03%2F27%2F2019-03-27-Mask-RCNN-tensorflow-tutorial%2F</url>
    <content type="text"><![CDATA[이 내용은 유튜버 Augmented Startups와 Mark Jay을 공부한 내용을 정리한 글입니다. Augmented Startups: https://www.youtube.com/watch?v=GSDbfGsxruA&amp;t=561s Mark Jay: https://www.youtube.com/watch?v=lLM8oAsi32g 1. Mask RCNN-tensorflow버전 설치Mask-RCNN-Tensorflow을 git clone 해준다1$ git clone https://github.com/matterport/Mask_RCNN.git Dependencies을 설치해주자먼저 필요한 python package들은 설치한다.12$ cd Mask_RCNN$ pip install -r requirements.txt 그리고 pretrained model을 다운받고 Mask-RCNN 폴더에 옮겨주자https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5 이제 cocoapi를 설치하자1234$ git clone https://github.com/philferriere/cocoapi.git$ cd cocoapi$ cd PythonAPI$ python setup.py build_ext install 설치가 잘되었는지를 확인하기터미널 창에 jupyter-notebook 입력해 쥬피터 노트북을 실행하고 samples 폴더안에 demo.jpynb를 실행시켜본다. 잘 설치된 경우 아래 그림과 같이 랜덤한 사진의 결과가 나온다. Failed to get convolution algorithm 문제 발생시: 12345import tensorflow as tfsess_config = tf.ConfigProto()sess_config.gpu_options.allow_growth = Truesess = tf.Session(config=sess_config) 와 같이 텐서플로우를 import하고 sess를 적절한위치에 추가해주자 2. webcam과 video으로 테스트 해보기webcam과 video 코드는 Mark Jay가 만든 코드를 사용할 것이다.https://github.com/markjay4k/Mask-RCNN-series 에서 visualize_cv2.py process_video.py 이 두개의 파일을 다운받는다. 그리고 visualize_cv2.py을 열어 다음과 같이 수정해준다. 12345678910111213import utils -&gt; from mrcnn import utilsimport model as modellib -&gt; from mrcnn import model as modellibROOT_DIR = os.getcwd() -&gt; ROOT_DIR = os.path.abspath("./")import coco -&gt; sys.path.append(os.path.join(ROOT_DIR,"samples/coco/"))import coco (위에 있던 import coco를 sys.path.append(os.path.join(ROOT_DIR,"samples/coco/")) 추가하고 아래에 옮겨준다. ) Failed to get convolution algorithm 문제 발생하면 위에 해결법을 이용하면 된다. 비디오 테스트에 경우 process_video.py에서 capture = cv2.VideoCapture(&#39;비디오 주소&#39;) 만 자신의 비디오 주소로 변경하면 된다. 3. 간단한 코드 분석123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133import cv2import numpy as npimport osimport sysfrom mrcnn import utilsfrom mrcnn import model as modellibimport tensorflow as tfROOT_DIR = os.path.abspath("./")MODEL_DIR = os.path.join(ROOT_DIR, "logs")sys.path.append(os.path.join(ROOT_DIR,"samples/coco/"))import cocoCOCO_MODEL_PATH = os.path.join(ROOT_DIR, "mask_rcnn_coco.h5")if not os.path.exists(COCO_MODEL_PATH): utils.download_trained_weights(COCO_MODEL_PATH)############# 여기까지 필요한 module importclass InferenceConfig(coco.CocoConfig): GPU_COUNT = 1 IMAGES_PER_GPU = 1config = InferenceConfig()config.display()############## cudnn 문제 해결 부분sess_config = tf.ConfigProto()sess_config.gpu_options.allow_growth = Truesess = tf.Session(config=sess_config)##############model = modellib.MaskRCNN( mode="inference", model_dir=MODEL_DIR, config=config ############# 모델 불러오기)model.load_weights(COCO_MODEL_PATH, by_name=True) ############# 모델 weight 불러오기class_names = [ 'BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']############## class이미지 이름 지정 (자리순서마다 이미어떤 클래스가 정해져 있고 이를 text로 표현하는 부분이다)def random_colors(N): np.random.seed(1) colors = [tuple(255 * np.random.rand(3)) for _ in range(N)] return colors############## class마다 다른 색깔로 segmentation 하기위한 부분colors = random_colors(len(class_names))class_dict = &#123; name: color for name, color in zip(class_names, colors)&#125;def apply_mask(image, mask, color, alpha=0.5): """apply mask to image""" for n, c in enumerate(color): image[:, :, n] = np.where( mask == 1, image[:, :, n] * (1 - alpha) + alpha * c, image[:, :, n] ) return image############## segemetation mask를 이미지에 표시def display_instances(image, boxes, masks, ids, names, scores): """ take the image and results and apply the mask, box, and Label """ n_instances = boxes.shape[0] if not n_instances: print('NO INSTANCES TO DISPLAY') else: assert boxes.shape[0] == masks.shape[-1] == ids.shape[0] for i in range(n_instances): if not np.any(boxes[i]): continue y1, x1, y2, x2 = boxes[i] label = names[ids[i]] color = class_dict[label] score = scores[i] if scores is not None else None caption = '&#123;&#125; &#123;:.2f&#125;'.format(label, score) if score else label mask = masks[:, :, i] image = apply_mask(image, mask, color) image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2) image = cv2.putText( image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2 ) return image############## mask, box, class_name을 모두 이미지에 표시if __name__ == '__main__': test everything capture = cv2.VideoCapture(0) # these 2 lines can be removed if you dont have a 1080p camera. capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640) capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 640) while True: ret, frame = capture.read() results = model.detect([frame], verbose=0) r = results[0] ############## 여기서 roi,masks, class_id가 나온다. frame = display_instances( frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'] ) cv2.imshow('frame', frame) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break capture.release()cv2.destroyAllWindows() 여기서 이용할 수 있는 부분은 results에서 나온 roi, masks, class_id로 어떤물체의 위치와 크기 이다. Mask-RCNN Train먼저 https://supervise.ly/이 사이트에 가입하고 사이트에 로그인한다. 그 후 Import 탭에 들어간 후 Import Plugin을 Images로 바꾸고 자신의 데이터셋 폴더를 위 그림의 상자안에 드래그 드랍 합니다. 그러면 위 사진처럼 창이 바뀌면 칸에 Project이름을 작성한후 Start Import 버튼을 누르면 Import가 됩니다. Project 탭에 들어가면 그림과 같이 자신이 Import한 이미지의 Project가 생성된 것을 확인할 수 있습니다. 이제 프로젝트를 클릭하고 이미지 폴더에 들어가면 다음과 같은 창이 열린다. 여기서 빨간색 박스가 쳐진 버튼을 클릭합니다. 그러면 이런 창이 뜨는데 여기서 Title에 물체의 label을 지정합니다. 그 후 그림처럼 labeling을 진행 합니다. 그리고 class를 하나더 추가하고 싶으면 위 그림처럼 Create Class를 눌러 추가 해주면됩니다. labeling을 다 했으면 다시 프로젝트 탭으로 돌아온후 위 그림처럼 Instance segmentation 버튼을 클릭해준다. Cluster 탭에 들어간 후 Instructions을 클릭합니다. 그러면 위 같은 창이 뜨는데 먼저 nvidia docker를 설치합니다. Neural Networks 탭에 들어간 후 ADD 버튼을 클릭합니다. 그 후 아래로 내려보면 Mask-RCNN이 있고 ADD버튼을 눌러준다. Neural Networks 탭에 다시 들어간 후 Train 버튼을 누른다. 자신의 데이터셋을 input project에 입력해주고 결과의 project이름을 정해준다. Train이 끝나면 다음과 같이 새로운 Neural Network가 생기고 여기서 Download를 해주거나 test버튼을 눌러 test 데이터를 test할 수 있다. 만약 Download한다면 .tar파일 안에 model.h5라는 파일 있는데 이 파일을 Mask-RCNN 폴더에 옮겨준다. 그리고 위 코드처럼 바꿔준다. 여기서 NUM_CLASSES 에서 뒤에 있는 2는 자신이 train 시킨 클래스의 수를 적어준다. https://deepmi.me/linux/18791/ nvidia docker를 설치 했으면 그림에 있는 명령어를 터미널에 입력합니다. TODO Mask-RCNN을 자신만의 데이터로 training 하기]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VNect-tensorflow]]></title>
    <url>%2F2019%2F03%2F22%2F2019-03-22-vnect-tensorflow-tutorial%2F</url>
    <content type="text"><![CDATA[VNect Tensorflow버전 github: https://github.com/timctho/VNect-tensorflow 1. 먼저 VNect-Tensorflow를 git clone 해준다.1$ git clone https://github.com/timctho/VNect-tensorflow.git 2. caffe python버전과 opengl을 설치한다.caffe는 여러가지 설치법이 있지만 anaconda에서 Python3로 쉽게 설치하는 방법은 아래 사이트를 참고한다 https://yangcha.github.io/Caffe-Conda3/ opengl의 경우는 1$ pip install PyOpenGL PyOpenGL_accelerate 으로 설치해주면 된다. 3. Vnect-Tensorflow가 설치된 폴더로 이동하고 caffe_weights_to_pickle.py를 실행한다.먼저 Vnect-Tensorflow가 설치된 폴더로 이동한다. 1$ cd VNect-tensorflow 그다음 환경을 자신이 caffe를 설치한 환경으로 바꿔준다. 1$ source activate testcaffe 이제 caffe로 만들어진 모델을 pickle 형식으로 바꿔준다. 123$ python caffe_weights_to_pickle.py --prototxt=../Documents/VNECT/mpii_vnect_model_code/mpii_vnect_model_demo/models/vnect_net.prototxt --caffemodel=../Documents/VNECT/mpii_vnect_model_code/mpii_vnect_model_demo/models/vnect_model.caffemodel 주소가 복잡하면 VNect-Tensorflow에 있는 models 폴더안에 vnect_net.prototxt 와 vnect_model.caffemodel 을 복사한후 아래와 같이 실행시키면 1$ python caffe_weights_to_pickle.py vnect.pkl 이라는 파일이 만들어졌을것이다. 4. models 폴더안에 vnect_model.py 수정하고 실행에 필요한 모델파일 만들기이유는 모르겠지만 직접 실행할때 필요한 모델을 만드는 파일이 없기에 models 폴더안에 vnect_model.py를 조금 수정해야한다.코드를 보시면 맨아래 if __name__ == &#39;name&#39;: 아래를 1234567model_file = '../vnect.pkl'model = VNect(368)with tf.Session() as sess: saver = tf.train.Saver() model.load_weights(sess, model_file) save_path = saver.save(sess, "./vnect_tf") 으로 바꿔주고 실행시키면 123vnect_tf.data-00000-of-00001vnect_tf.indexvnect_tf.meta 세가지 파일이 만들어 졌을것이다.이제 이 파일을 models/weights 안에 복사한다. (weights폴더가 없으니 만들어주자) 5. demo_tf_gl.py로 테스트 해보자.cudnn 오류때문에 1sess_config = tf.ConfigProto(device_count=gpu_count) 아래에 1sess_config.gpu_options.allow_growth = True 을 추가해준다. 그리고--demo_type&#39;, default=&#39;image&#39;를 --demo_type&#39;, default=&#39;webcam&#39; 으로 바꿔주면 웹캠으로 테스트가 가능하다. TODOcaffe 설치 오류 확인해보기]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
</search>
